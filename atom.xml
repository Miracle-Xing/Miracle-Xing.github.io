<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>邢大强的blog</title>
  
  <subtitle>Stay hungry. Stay foolish.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://miracle-xing.github.io/"/>
  <updated>2019-07-23T18:47:27.198Z</updated>
  <id>https://miracle-xing.github.io/</id>
  
  <author>
    <name>Miracle</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark 编程核心抽象——RDD</title>
    <link href="https://miracle-xing.github.io/2019/07/24/Spark-%E7%BC%96%E7%A8%8B%E6%A0%B8%E5%BF%83%E6%8A%BD%E8%B1%A1%E2%80%94%E2%80%94RDD/"/>
    <id>https://miracle-xing.github.io/2019/07/24/Spark-编程核心抽象——RDD/</id>
    <published>2019-07-23T18:27:29.000Z</published>
    <updated>2019-07-23T18:47:27.198Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-什么是RDD？"><a href="#1-什么是RDD？" class="headerlink" title="1. 什么是RDD？"></a>1. 什么是RDD？</h1><p>RDD 是 Resilient Distributed Dataset（<strong>弹性分布式数据集</strong>） 的简称。它是 Apache Spark 的基本数据结构。它是一个不可变的对象集合，在集群的不同节点上进行计算。</p><ul><li>Resilient: 即在 RDD lineage(DAG) 的帮助下具有容错能力，能够重新计算由于节点故障而丢失或损坏的数据分区。</li><li>Distributed: 数据分布在多个节点上。</li><li>Dataset: 表示所操作的数据集。用户可以通过 JDBC 从外部加载数据集，数据集可以是 JSON 文件，CSV 文件，文本文件或数据库。<br><br><a id="more"></a></li></ul><h1 id="2-RDD-的特点"><a href="#2-RDD-的特点" class="headerlink" title="2. RDD 的特点"></a>2. RDD 的特点</h1><ol><li><p><strong>内存计算</strong>：它将中间计算结果存储在分布式内存（RAM）中，而不是磁盘中。</p></li><li><p><strong>延迟计算</strong>：Apache Spark 中的所有 transformation 都是惰性的，因为它们不会立即计算结果，它们会记住应用于数据集的那些 transformation。直到 action 出现时，才会真正开始计算。</p></li><li><p><strong>容错性</strong>：Spark RDDs 能够容错，因为它们跟踪数据<strong>沿袭</strong>（lineage）信息，以便在故障时自动重建丢失的数据。</p></li><li><p><strong>不可变性</strong>：跨进程共享数据是安全的。它也可以在任何时候创建或检索，这使得缓存、共享和复制变得容易。因此，它是一种在计算中达到一致性的方法。</p></li><li><p><strong>分区性</strong>：partition 是 Spark RDD 中并行性的基本单元，每个分区都是数据的逻辑分区。Partition—task 一 一对应。</p></li><li><p><strong>持久化</strong>：用户可以声明他们将重用哪些 RDDs，并为它们选择存储策略。</p></li><li><p><strong>数据本地性</strong>：RDDs 能够定义计算分区的位置首选项。位置首选项是关于 RDD 位置的信息。</p></li></ol><h1 id="3-Spark-RDD-的操作类型"><a href="#3-Spark-RDD-的操作类型" class="headerlink" title="3. Spark RDD 的操作类型"></a>3. Spark RDD 的操作类型</h1><p>Apache Spark 中的 RDD 支持两种操作：</p><ul><li><strong>Transformation</strong></li><li><strong>Action</strong></li></ul><h2 id="3-1-Transformation-操作："><a href="#3-1-Transformation-操作：" class="headerlink" title="3.1 Transformation 操作："></a>3.1 Transformation 操作：</h2><p>Spark RDD transformation 操作是一个从现有的 RDD 生成新 RDD 的函数（方法、算子）。如：map(), filter(), reduceByKey()。</p><p>Transformation 操作都是<strong>延迟计算</strong>的操作。</p><p>有两种类型：<strong>窄变换、宽变换</strong>（窄依赖、宽依赖）。</p><ol><li><strong>窄依赖</strong>：它是map、filter 这样数据来自一个单独分区的操作。即输出 RDD 分区中的数据，来自父 RDD 中的单个分区。不需要 shuffle 操作就能解决。<br><img src="/images/2019/07/24/75185340-ad78-11e9-bb89-83337f9b9a34.png" alt="窄依赖.png"></li></ol><p><strong>窄依赖算子</strong>：map(), flatMap(), mapPartition(), filter(), sample(), union()</p><ol start="2"><li><strong>宽依赖</strong>：在子 RDD 单个分区中计算结果所需的数据可能存在于父 RDD 的多个分区中。类似 groupByKey() 和 reduceBykey() 这样的 transformation。宽依赖也称为 shuffle transformation。<br><img src="/images/2019/07/24/7bda84f0-ad78-11e9-bb89-83337f9b9a34.png" alt="宽依赖.png"></li></ol><p><strong>宽依赖算子</strong>：intersection(), distinct(), reduceByKey(), groupByKey(), join(), cartesian(), repartition(), coalesce()。</p><h2 id="3-2-action-操作"><a href="#3-2-action-操作" class="headerlink" title="3.2 action 操作"></a>3.2 action 操作</h2><p>Spark 中的 action 操作 ，返回 RDD 计算 的最终结果，<strong>其结果是一个值，而不是一个 RDD</strong>。</p><p>Action 触发血缘关系中 RDD 上的 transformation  操作的真正计算，计算结果返回 Driver端或者写入数据库。</p><p>这种设计使 Spark 运行更加高效。例如：map操作返回的数据集用于 reduce 操作，返回到 driver 端的只是 reduce 的结果值，而不是 map操作的数据集。</p><p><strong>常见的 Action</strong>：first(), take(), reduce(), collect(), the count()。</p><h1 id="4-创建-RDD"><a href="#4-创建-RDD" class="headerlink" title="4. 创建 RDD"></a>4. 创建 RDD</h1><p>三种创建 RDD 的方法：</p><ol><li>使用集合创建 RDD（<strong>parallelize</strong>）</li><li>使用已有 RDD 创建 RDD（<strong>父生子</strong>）</li><li>从外部数据源创建 RDD（<strong>textFile</strong>）</li></ol><p>在我们学习 Spark 的初始阶段 ，RDD 通常由集合创建的，即在 Driver 程序中创建集合并将其传递给 SparkContext 的 paralize() 方法。这种方法很少在 正式环境中使用，因为这种方法的整个数据集位于一台主机上。</p><p>首先实例化 SparkContext 对象：</p><p><strong>Scala</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val sparkConf = new SparkConf().setAppName(&quot;sc_wordcount&quot;).setMaster(&quot;local[*]&quot;)</span><br><span class="line">val sc = new SparkContext(sparkConf)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);</span><br><span class="line">JavaSparkContext jsc = new JavaSparkContext(conf);</span><br></pre></td></tr></table></figure><p>其中 appName 用于显示在 Spark 集群的 webUI上面。master 是一个 spark、YARN、mesos 集群 URL，或者是一个 local 字符串。<strong>实际项目中，在集群上运行时，不会对 master 进行硬编码。而是用 spark-submit 启动应用程序，并传递 master 给应用程序。但是，对于本地测试和单元测试，可以使用 local 运行 Spark</strong>。<br><img src="/images/2019/07/24/938a83c0-ad78-11e9-bb89-83337f9b9a34.png" alt="sparksubmit示例.png"></p><h2 id="4-1-使用集合创建-RDD"><a href="#4-1-使用集合创建-RDD" class="headerlink" title="4.1 使用集合创建 RDD"></a>4.1 使用集合创建 RDD</h2><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val data = Array(1, 2, 3, 4, 5)</span><br><span class="line">val distData = sc.paralize(data)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; data = Arrays.asList(1, 2, 3, 4, 5);</span><br><span class="line">JavaRDD&lt;Integer&gt; distData = sc.paralize(data);</span><br></pre></td></tr></table></figure><h2 id="4-2-从外部数据源创建-RDD"><a href="#4-2-从外部数据源创建-RDD" class="headerlink" title="4.2 从外部数据源创建 RDD"></a>4.2 从外部数据源创建 RDD</h2><p>Spark 可以从 Hadoop 支持的任何存储源创建分布式数据集，包括本地文件系统、HDFS、Cassandra、HBase、Amazon S3等。<br>Spark 支持文本文件、SequenceFiles 和任何其他 Hadoop InputFormat。</p><h3 id="4-2-1-读取本地文件"><a href="#4-2-1-读取本地文件" class="headerlink" title="4.2.1 读取本地文件"></a>4.2.1 读取本地文件</h3><p>文本文件 RDDs 可以使用 SparkContext 的 textFile 方法创建。此方法接受文件的 URI（机器上的本地文件路径、hdfs://、s3a://等URI），并将其作为行集合读取。下面是一个示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val distFile = sc.textFile(&quot;data.txt&quot;)</span><br></pre></td></tr></table></figure><p>一旦创建，就可以对 distFile 进行相应操作。例如，我们 可以将所有行的长度相加，使用 map 和 reduce 操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distFile.map(line=&gt;line.length).reduce(_+_)</span><br></pre></td></tr></table></figure><p><strong>关于用 Spark 读取文件的一些注意事项</strong>：</p><ol><li><p>如果使用本地文件系统上的路径，则必须在 worker 节点上的同一路径上，此文件可访问。要么将文件复制到所有 worker 上，要么使用一个挂载网络的共享文件系统。</p></li><li><p>Spark 所有基于文件的输入方法（textFile 等），支持在目录、压缩文件和通配符上运行，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">textFile(&quot;/my/directory&quot;), textFile(&quot;/my/directory/*.txt&quot;), textFile(&quot;/my/directory/*.gz))</span><br></pre></td></tr></table></figure></li><li><p>textFile 方法还接受一个可选的第二个参数，用于控制文件的分区数量。默认情况下，<strong>Spark 为文件的每个块创建一个分区（HDFS 中的块默认 是 128MB）</strong>，但是 您也可以通过传递更大的值来要求更高数量的分区。注意，分区数不能少于块数。</p></li></ol><p><strong>除了文本文件，Spark 的 Scala API 还支持其他几种数据格式</strong>：</p><ol><li><p><strong>SparkContext.wholeTextFile</strong> 允许您读取包含多个小文本文件的目录，并将它们作为（filename, content）的键值对返回。这与 textFile 不同，textFile 将在每个文件中每行返回一条 记录。分区由数据本地性决定，在某些情况下，数据本地性可能导致分区太少。对于这些情况，wholeTextFile 提供了控制最小分区数量的第二个可选 参数。</p></li><li><p>对于** SequenceFiles**，使用 SparkContext 的 sequenceFile[K, V]方法，其中K和V是文件中的键和值的类型。这些应该是 Hadoop Writable 接口的子类，比如 IntWritable 和 Text。</p></li><li><p>对于其他 Hadoop inputformat，您可以使用  SparkContext.hadoopRDD方法，它接受任意的 JobConf 和 输入格式类、键类和值类。将这些设置为与使用输入源 Hadoop 作业相同的方式。还可以使用 SparkContext。基于“new”MapReduce API（org.apache.hadoop.mapreduce）的 inputformat 的 newAPIHadoopRDD。</p></li></ol><h3 id="4-2-2-读取-HDFS-上的数据"><a href="#4-2-2-读取-HDFS-上的数据" class="headerlink" title="4.2.2 读取 HDFS 上的数据"></a>4.2.2 读取 HDFS 上的数据</h3><ol><li>启动 HDFS</li><li>读取 HDFS 上的数据<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val textFileRDD = sc.textFile(&quot;hdfs://bigdata01:9000/textdata/order.txt&quot;)</span><br><span class="line">val count = textFileRDD.count()</span><br><span class="line">println(&quot;count:&quot; + count)</span><br></pre></td></tr></table></figure></li></ol><h3 id="4-2-3-提交应用程序到-Spark-集群"><a href="#4-2-3-提交应用程序到-Spark-集群" class="headerlink" title="4.2.3 提交应用程序到 Spark 集群"></a>4.2.3 提交应用程序到 Spark 集群</h3><ol><li><p>打包应用程序</p></li><li><p>上传 jar 包到服务器</p></li><li><p>运行 spark-submit 命令<br>./spark-submit –class sparkcore.learnTextFile –deploy-mode client /opt/sparkapp/learnTextFile.jar</p></li></ol><h3 id="4-2-4-配置并启动-Spark-History-Server"><a href="#4-2-4-配置并启动-Spark-History-Server" class="headerlink" title="4.2.4 配置并启动 Spark History Server"></a>4.2.4 配置并启动 Spark History Server</h3><ol><li><p>重命名 conf/spark-deaults.conf.template 为 conf/spark-defaults.conf</p></li><li><p>修改 spark-defaults.conf 配置文件，并同步到其他节点。<br>修改前：<br><img src="/images/2019/07/24/323ce350-ad79-11e9-bb89-83337f9b9a34.png" alt="spark historyserver 配置1.png"><br>修改后（注意：hdfs 目录要先创建）：<br><img src="/images/2019/07/24/36b43aa0-ad79-11e9-bb89-83337f9b9a34.png" alt="spark historyserver 配置2.png"></p></li><li><p>启动 ./start-history-server.sh<br><img src="/images/2019/07/24/3b44e650-ad79-11e9-bb89-83337f9b9a34.png" alt="spark historyserver 配置3.png"></p></li><li><p>访问 <a href="http://bigdata01:18080/" target="_blank" rel="noopener">http://bigdata01:18080/</a> webUI </p></li></ol><h1 id="5-向-Spark-算子传递函数"><a href="#5-向-Spark-算子传递函数" class="headerlink" title="5. 向 Spark 算子传递函数"></a>5. 向 Spark 算子传递函数</h1><p>Spark API 严重依赖于将 dirver 程序中的函数传递到集群上运行</p><p><strong>Scala</strong>:<br>推荐使用如下两种方式实现函数的传递：</p><ol><li>匿名函数语法，可用于短代码段。</li><li>全局单例对象中的静态方法。例如，您可以定义对象 MyFunctions，然后传递 MyFunctions.func1，如下所示：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">object MyFunctions&#123;</span><br><span class="line">    def func1(s:String): String = &#123;...&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">myRDD.map(MyFunctions.func1)</span><br></pre></td></tr></table></figure></li></ol><p><strong>Java</strong>:<br>在 Java 中，函数由实现 org.apacche.spark.api.java.function 包中的接口的类表示。<br><img src="/images/2019/07/24/a6005070-ad78-11e9-bb89-83337f9b9a34.png" alt="Java 函数编程1.png"></p><p><img src="/images/2019/07/24/ab566f00-ad78-11e9-bb89-83337f9b9a34.png" alt="Java 函数编程2.png"></p><p>有两种方法可以创建这样的函数：</p><ol><li>可以是匿名内部类</li><li>也可以是创建 类实现相应接口，并将其实例传递给 Spark</li></ol><p>使用<strong>lambda 表达式</strong>简洁的定义实现。<br>例如，可以这样编写代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;data.txt&quot;);</span><br><span class="line">JavaRDD&lt;Integer&gt; lineLengths = lines.map(new Function&lt;String, Integer&gt;()&#123;</span><br><span class="line">    public Integer call(String s) &#123;return s.length();&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">int totalLength = lineLengths.reduce(new Function2&lt;Integer,  Integer, Integer&gt;()&#123;</span><br><span class="line">    public Integer call(Integer a, Integer b)&#123;return a + b;&#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>或者 可以这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class GetLength implements Function&lt;String, Integer&gt;&#123;</span><br><span class="line">    public Integer call(String s)&#123; return s.length();&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Sum implements Function2&lt;Integer, Integer, Integer&gt; &#123;</span><br><span class="line"></span><br><span class="line">  public Integer call(Integer a, Integer b) &#123; return a + b; &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;data.txt&quot;);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; lineLengths = lines.map(new GetLength());</span><br><span class="line"></span><br><span class="line">int totalLength = lineLengths.reduce(new Sum());</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：Java 中的匿名内部类也可以访问闭包作用域中的变量，只要它们被标记为 final。 Spark 将把这些变量的副本发送到每个 worker 节点上。</p><h1 id="6-Spark-算子实战——transformation"><a href="#6-Spark-算子实战——transformation" class="headerlink" title="6. Spark 算子实战——transformation"></a>6. Spark 算子实战——transformation</h1><h2 id="6-1-map-和-flatMap-算子"><a href="#6-1-map-和-flatMap-算子" class="headerlink" title="6.1 map 和 flatMap 算子"></a>6.1 map 和 flatMap 算子</h2><p><strong>map()</strong>: 将传入的函数应用于RDD 中的每一条记录，返回由函数结果组成的新 RDD。函数的结果值是一个对象，不是一个集合。</p><p><strong>flatMap()</strong>: 与map() 操作类似。但是传入 flatMap() 的函数可以返回 0个、1个或多个结果值。即函数结果值是一个集合，而不是一个对象。</p><p><strong>map操作 Scala版本</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val textFileRDD = sc.textFile(&quot;in/README.md&quot;)</span><br><span class="line">val uppercaseRDD = textFileRDD.map(line=&gt;line.toUpperCase)</span><br><span class="line">for ( elem &lt;- uppercaseRDDD.take(3))&#123;</span><br><span class="line">    println(elem)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>map操作 Java版本</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; javaRDD = jsc.textFile(&quot;in/README.md&quot;);</span><br><span class="line">JavaRDD&lt;String&gt; map = javaRDD.map(line -&gt; line.toUpperCase());</span><br><span class="line">for (String line : map.take(3)) &#123;</span><br><span class="line">    System.out.println(line);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>flatMap操作 Scala版本</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val flatMapRDD = textFileRDD.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">flatMapRDD.take(3).foreach(println)</span><br></pre></td></tr></table></figure><p><strong>flatMap操作 Java版本</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; javaRDD = jsc.textFile(&quot;in/README.md&quot;);</span><br><span class="line">JavaRDD&lt;String&gt; wordsRDD = javaRDD.flatMap(line -&gt; (Arrays.asList(line.split(&quot; &quot;)).iterator()));</span><br><span class="line">wordsRDD.take(3).forEach(word-&gt; System.out.println(word));</span><br></pre></td></tr></table></figure><p><img src="/images/2019/07/24/b9d781e0-ad78-11e9-bb89-83337f9b9a34.png" alt="flatMap执行过程.png"><br>从Spark map() 和 flatMap() 的比较中可以看出，Spark map函数表达的是一对一的转换。它将集合的每个数据元素转换为结果集合的一个数据元素。而Spark flatMap 函数表示一对多的转换。它将每个元素转换为 0 或更多的元素。</p><h2 id="6-2-filter-算子"><a href="#6-2-filter-算子" class="headerlink" title="6.2 filter 算子"></a>6.2 filter 算子</h2><p>Spark RDD filter() 函数返回一个新的 RDD，<strong>只包含满足过滤条件的元素</strong>。这是一个<strong>窄依赖</strong>的操作，不会将数据从一个分区转移到其他分区。——不会发生shuffle。<br>例如，假设 RDD 包含5个整数（1, 2, 3, 4, 5），过滤条件是 判断是否偶数。过滤后得到的 RDD将只包含偶数，即 2 和 4。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val filterRDD = fileRDD.filter(line =&gt; line.contains(&quot;Spark&quot;))</span><br><span class="line">filterRDD.foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; filterRDD = javaRDD.filter(line -&gt; line.contains(&quot;Spark&quot;));</span><br><span class="line">filterRDD.foreach(line -&gt; System.out.println(line));</span><br></pre></td></tr></table></figure><h2 id="6-3-distinct-算子"><a href="#6-3-distinct-算子" class="headerlink" title="6.3 distinct 算子"></a>6.3 distinct 算子</h2><p>返回 RDD 中的非重复记录。注意：此操作是昂贵的，因为它需要对数据进行 shuffle。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val distintRDD = sc.parallelize(Seq(1,2,3,4,5,1,2,3,4)).distinct()</span><br><span class="line">distintRDD.foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; numsRDD = jsc.parallelize(Arrays.asList(1, 2, 3, 4, 5, 1, 2, 3, 4));</span><br><span class="line">JavaRDD&lt;Integer&gt; distinctRDD = numsRDD.distinct();</span><br><span class="line">distinctRDD.foreach(ele -&gt; System.out.println(ele));</span><br></pre></td></tr></table></figure><h2 id="6-4-mapPartitions"><a href="#6-4-mapPartitions" class="headerlink" title="6.4 mapPartitions"></a>6.4 mapPartitions</h2><p>在 mapPartition() 函数中，map() 函数同时应用于每个 partition 分区。对比学习 foreachPartition() 函数，foreachPartition() 是一个action 算子，操作方式与 mapPartition相同。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val mapPartitionRDD = fileRDD.mapPartitions(partition =&gt; &#123;</span><br><span class="line">  // map 每一个分区，然后再 map 分区中的每一个元素</span><br><span class="line">  partition.map(line =&gt; line.toUpperCase())</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">// foreach 是一个没有返回值的 action</span><br><span class="line">mapPartitionRDD.foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; mapPartitionsRDD = javaRDD.mapPartitions(stringIterator -&gt; &#123;</span><br><span class="line">    List&lt;String&gt; list = new ArrayList&lt;&gt;();</span><br><span class="line">    while (stringIterator.hasNext()) &#123;</span><br><span class="line">        list.add(stringIterator.next().toUpperCase());</span><br><span class="line">    &#125;</span><br><span class="line">    return list.iterator();</span><br><span class="line">&#125;);</span><br><span class="line">mapPartitionsRDD.foreach(line-&gt; System.out.println(line));</span><br></pre></td></tr></table></figure><h2 id="6-5-mapPartitionWithIndex"><a href="#6-5-mapPartitionWithIndex" class="headerlink" title="6.5 mapPartitionWithIndex()"></a>6.5 mapPartitionWithIndex()</h2><p>就像 mapPartition，除了 mapPartition外，它还为传入的函数提供了一个整数值，表示<strong>分区的索引</strong>，map()在分区索引上依次应用。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val mapPartitionRDD = fileRDD.mapPartitionsWithIndex((index, partition) =&gt; &#123;</span><br><span class="line">  partition.map(line =&gt; index + line.toUpperCase())</span><br><span class="line">&#125;)</span><br><span class="line">mapPartitionRDD.foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; mapPartitionsRDD = javaRDD.mapPartitionsWithIndex((index, stringIterator) -&gt; &#123;</span><br><span class="line">    List&lt;String&gt; list = new ArrayList&lt;&gt;();</span><br><span class="line">    while (stringIterator.hasNext()) &#123;</span><br><span class="line">        list.add(index + stringIterator.next().toUpperCase());</span><br><span class="line">    &#125;</span><br><span class="line">    return list.iterator();</span><br><span class="line">&#125;, false);</span><br><span class="line">mapPartitionsRDD.foreach(line -&gt; System.out.println(line));</span><br></pre></td></tr></table></figure><h2 id="6-6-union-并集"><a href="#6-6-union-并集" class="headerlink" title="6.6 union 并集"></a>6.6 union 并集</h2><p>使用 union() 函数，我们可以在新的 RDD 中获得两个 RDD 的元素。<strong>这个函数的关键规则是两个RDDs 属于同一类型</strong>。例如，RDD1 的元素是（Spark, Spark, Hadoop, Flink），而 RDD2 的元素是（Big data, Spark, Flink），所以结果 union(rdd1.union) 有元素（Spark, Spark, Spark, Hadoop, Flink, Flikn, Big data）</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val RDD1 = sc.parallelize(Seq(1,2,3,4))</span><br><span class="line">val RDD2 = sc.parallelize(Seq(1,2,3,4))</span><br><span class="line">val unionRDD = RDD1.union(RDD2)</span><br><span class="line">unionRDD.foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; RDD1 = jsc.parallelize(Arrays.asList(1, 2, 3, 4));</span><br><span class="line">JavaRDD&lt;Integer&gt; RDD2 = jsc.parallelize(Arrays.asList(1, 2, 3, 4));</span><br><span class="line">JavaRDD&lt;Integer&gt; unionRDD = RDD1.union(RDD2);</span><br><span class="line">unionRDD.foreach(num -&gt; System.out.println(num));</span><br></pre></td></tr></table></figure><h2 id="6-7-intersection（交集）"><a href="#6-7-intersection（交集）" class="headerlink" title="6.7 intersection（交集）"></a>6.7 intersection（交集）</h2><p>使用 intersection() 函数，我们只得到新 RDD 中的两个 RDD 的公共元素。<strong>这个函数的关键规则是这两个 RDDs 应该是同一类型的</strong>。</p><p>举个例子，RDD1 的元素是（Spark, Spark, Hadoop,  Flink)，RDD2的元素是（Big data, Spark, Flink) 交集（RDD1.intersection(RDD2))将包含元素（Spark）。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val RDD1 = sc.parallelize(Seq(1,2,3,4))</span><br><span class="line">val RDD2 = sc.parallelize(Seq(1,2,3,4))</span><br><span class="line">val intersectionRDD = RDD1.intersection(RDD2)</span><br><span class="line">intersectionRDD.foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; RDD1 = jsc.parallelize(Arrays.asList(1, 2, 3, 4));</span><br><span class="line">JavaRDD&lt;Integer&gt; RDD2 = jsc.parallelize(Arrays.asList(1, 2, 3, 4));</span><br><span class="line">JavaRDD&lt;Integer&gt; intersectionRDD = RDD1.intersection(RDD2);</span><br><span class="line">intersectionRDD.foreach(num -&gt; System.out.println(num));</span><br></pre></td></tr></table></figure><h2 id="6-8-PairRDD"><a href="#6-8-PairRDD" class="headerlink" title="6.8 PairRDD"></a>6.8 PairRDD</h2><p>现实生活中的许多数据集通常是<strong>键值对</strong>形式的。例如：包含课程名称和选修课程的学生名单的数据集。<br>这种数据集的典型模式是每一行都是一个key映射到一个或多个value。为此，Spark提供了一个名为 PairRDD 的数据结构，而不是常规的 RDD。这使得处理此类数据更加简单和高效。</p><p>PairRDD 是一种特殊类型的 RDD，可以存储 键-值对</p><p><strong>创建 PairRDD</strong>:</p><ol><li>通过键值数据结构列表构建 Pair RDD。键值数据结构称为 tuple2 元组。（Java 语言没有内置的 tuple类型，所以Spark 的 Java API 允许用户使用 scala.Tuple2 类创建元组）。</li></ol><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val tuple = List((&quot;张三&quot;, &quot;语文&quot;), (&quot;李四&quot;, &quot;数学&quot;), (&quot;王五&quot;, &quot;英语&quot;))</span><br><span class="line">val pairRDD = sc.parallelize(tuple)pairRDD.foreach(t =&gt; println(t._1 + &quot;: &quot; + t._2))</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Tuple2&lt;String, String&gt;&gt; tuple2s = Arrays.asList(new Tuple2&lt;&gt;(&quot;张三&quot;, &quot;语文&quot;),</span><br><span class="line">        new Tuple2&lt;&gt;(&quot;李四&quot;, &quot;数学&quot;),</span><br><span class="line">        new Tuple2&lt;&gt;(&quot;王五&quot;, &quot;英语&quot;));</span><br><span class="line">JavaPairRDD&lt;String, String&gt; pairRDD = jsc.parallelizePairs(tuple2s);</span><br><span class="line">pairRDD.foreach(t -&gt; System.out.println(t._1 + &quot;: &quot; + t._2));</span><br></pre></td></tr></table></figure><ol start="2"><li>将一个常规的 RDD 转换为 PairRDD</li></ol><p><strong>Scala</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val regularRDD = sc.parallelize(List(&quot;张三 语文&quot;, &quot;李四 数学&quot;, &quot;王五 英语&quot;))</span><br><span class="line">val pairRDD = regularRDD.map(item =&gt; (item.split(&quot; &quot;)(0), item.split(&quot; &quot;)(1)))</span><br><span class="line">pairRDD.foreach(item =&gt; println(item._1 + &quot;: &quot; + item._2))</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; parallelizeRDD = jsc.parallelize(Arrays.asList(&quot;张三 语文&quot;, &quot;李四 数学&quot;, &quot;王五 英语&quot;));</span><br><span class="line">JavaPairRDD&lt;String, String&gt; pairRDD = parallelizeRDD.mapToPair(item -&gt; new Tuple2&lt;&gt;(item.split(&quot; &quot;)[0], item.split(&quot; &quot;)[1]));</span><br><span class="line">pairRDD.foreach(t -&gt; System.out.println(t._1 + &quot;: &quot; + t._2));</span><br></pre></td></tr></table></figure><h3 id="6-8-1-PairRDD-上的-transformation-操作"><a href="#6-8-1-PairRDD-上的-transformation-操作" class="headerlink" title="6.8.1 PairRDD 上的 transformation 操作"></a>6.8.1 PairRDD 上的 transformation 操作</h3><p>PairRDDs 允许使用常规 RDDs 可用的所有转换，支持与常规 RDDs 相同功能。<br>由于 PairRDDs 包含元组，所以我们 需要传递操作元组而不是 单个元素的函数给 Spark。</p><p><strong>1.  filter</strong><br>在 pairRDD 上使用 filter transformation:</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val regularRDD = sc.parallelize(List(&quot;张三 语文&quot;, &quot;李四 数学&quot;, &quot;王五 英语&quot;))</span><br><span class="line">val pairRDD = regularRDD.map(item =&gt; (item.split(&quot; &quot;)(0), item.split(&quot; &quot;)(1)))</span><br><span class="line"></span><br><span class="line">val filterPairRDD = pairRDD.filter(t =&gt; t._2.equals(&quot;语文&quot;))</span><br><span class="line">filterPairRDD.foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; parallelizeRDD = jsc.parallelize(Arrays.asList(&quot;张三 语文&quot;, &quot;李四 数学&quot;, &quot;王五 英语&quot;));</span><br><span class="line">JavaPairRDD&lt;String, String&gt; pairRDD = parallelizeRDD.mapToPair(item -&gt; new Tuple2&lt;&gt;(item.split(&quot; &quot;)[0], item.split(&quot; &quot;)[1]));</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, String&gt; filterRDD = pairRDD.filter(t -&gt; t._2.equals(&quot;数学&quot;));</span><br><span class="line">filterRDD.foreach(t -&gt; System.out.println(t));</span><br></pre></td></tr></table></figure><p><strong>2. reduceByKey—另一个版本的 wordcount</strong></p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val fileRDD = sc.textFile(&quot;in/README.md&quot;)</span><br><span class="line">  .flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">  .map(word =&gt; (word, 1))</span><br><span class="line">  .reduceByKey(_ + _)</span><br><span class="line">  .sortBy(_._2, false)</span><br><span class="line">  .collect()</span><br><span class="line">  .foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; javaRDD = jsc.textFile(&quot;in/README.md&quot;);</span><br><span class="line">JavaRDD&lt;String&gt; wordsRDD = javaRDD.flatMap(line -&gt; Arrays.asList(line.split(&quot; &quot;)).iterator());</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD = wordsRDD.mapToPair(word -&gt; new Tuple2&lt;&gt;(word, 1));</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; wordCounts = javaPairRDD.reduceByKey((a, b) -&gt; (a + b));</span><br><span class="line">JavaRDD&lt;Tuple2&lt;Integer, String&gt;&gt; tuple2JavaRDD = wordCounts.map(t -&gt; new Tuple2&lt;&gt;(t._2, t._1));</span><br><span class="line">JavaRDD&lt;Tuple2&lt;Integer, String&gt;&gt; tuple2JavaRDD1 = tuple2JavaRDD.sortBy(t -&gt; t._1, false, 1);</span><br><span class="line">JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; sortedRDD = tuple2JavaRDD1.map(t -&gt; new Tuple2&lt;&gt;(t._2, t._1));</span><br><span class="line">sortedRDD.foreach(t-&gt; System.out.println(t));</span><br></pre></td></tr></table></figure><p><strong>3. combineByKey</strong><br>combineByKey 是 Spark 中一个核心的高级函数，其他一些 键值对函数底层都是用它实现的。如 groupByKey, reduceByKey 等。</p><p>例：计算平均分数（Scala）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">object learnCombineBeKey &#123;</span><br><span class="line"></span><br><span class="line">  case class ScoreDetail(studentName: String, subject: String, score: Float)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sparkSession = SparkSession.builder()</span><br><span class="line">      .master(&quot;local[*]&quot;)</span><br><span class="line">      .appName(&quot;learnCombineBeKey&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    val sc = sparkSession.sparkContext</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * https://www.edureka.co/blog/apache-spark-combinebykey-explained</span><br><span class="line">      *</span><br><span class="line">      * combineByKey transformation</span><br><span class="line">      * combineByKey API 有三个函数</span><br><span class="line">      * Create combiner function: x</span><br><span class="line">      * Merge value function: y</span><br><span class="line">      * Merger combiners function: z</span><br><span class="line">      *</span><br><span class="line">      * API 格式为 combineByKey(x, y, z)</span><br><span class="line">      * 让我们看一个例子（Scala语言）：本例的目标是找到每个学生的平均分数</span><br><span class="line">      */</span><br><span class="line"></span><br><span class="line">    val scores = List(</span><br><span class="line">      ScoreDetail(&quot;A&quot;, &quot;Math&quot;, 98),</span><br><span class="line">      ScoreDetail(&quot;A&quot;, &quot;English&quot;, 66),</span><br><span class="line">      ScoreDetail(&quot;B&quot;, &quot;Math&quot;, 74),</span><br><span class="line">      ScoreDetail(&quot;B&quot;, &quot;English&quot;, 80),</span><br><span class="line">      ScoreDetail(&quot;C&quot;, &quot;Math&quot;, 98),</span><br><span class="line">      ScoreDetail(&quot;C&quot;, &quot;English&quot;, 96),</span><br><span class="line">      ScoreDetail(&quot;D&quot;, &quot;Math&quot;, 100),</span><br><span class="line">      ScoreDetail(&quot;D&quot;, &quot;English&quot;, 95)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 将测试数据转换为键值对形式--键key为学生名称Student Name，值为ScoreDetail 实例对象</span><br><span class="line">      */</span><br><span class="line">    val scoreWithKey = for (i &lt;- scores) yield (i.studentName, i)</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 创建一个 pairRDD</span><br><span class="line">      */</span><br><span class="line">    val scoreWithKeyRDD = sc.parallelize(scoreWithKey)</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 计算 平均分数</span><br><span class="line">      */</span><br><span class="line">    val avgScoresRDD = scoreWithKeyRDD.combineByKey(</span><br><span class="line">      (x: ScoreDetail) =&gt; (x.score, 1),</span><br><span class="line">      (acc: (Float, Int), x: ScoreDetail) =&gt; (acc._1 + x.score, acc._2 + 1),</span><br><span class="line">      (acc1: (Float, Int), acc2: (Float, Int)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2)</span><br><span class="line">    ).map(&#123;</span><br><span class="line">      case (key, value) =&gt; (key, value._1 / value._2)</span><br><span class="line">    &#125;)</span><br><span class="line">    avgScoresRDD.foreach(println)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>4. sortByKey</strong><br>当我们在（K, V）数据集中应用 sortByKey() 函数时，数据是根据 RDD 中的键 K 排序的。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val data =sc.parallelize(Seq((&quot;maths&quot;,52), (&quot;english&quot;,75), (&quot;science&quot;,82), (&quot;computer&quot;,65), (&quot;maths&quot;,85)))</span><br><span class="line">val sorted = data.sortByKey()</span><br><span class="line">sorted.collect().foreach(println)</span><br></pre></td></tr></table></figure><p><strong>注</strong>：在上面的代码中，sortByKey() 将数据 RDD的 key(String) 按升序排序。</p><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2s = Arrays.asList(new Tuple2&lt;&gt;(&quot;maths&quot;, 52),</span><br><span class="line">        new Tuple2&lt;&gt;(&quot;english&quot;, 75),</span><br><span class="line">        new Tuple2&lt;&gt;(&quot;science&quot;, 82),</span><br><span class="line">        new Tuple2&lt;&gt;(&quot;computer&quot;, 65),</span><br><span class="line">        new Tuple2&lt;&gt;(&quot;maths&quot;, 85));</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD = jsc.parallelizePairs(tuple2s);</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD1 = javaPairRDD.sortByKey();</span><br><span class="line">javaPairRDD1.collect().forEach(t -&gt; System.out.println(t));</span><br></pre></td></tr></table></figure><p><strong>5. join</strong><br>join 是数据库术语。它使用公共值组合两个表中的字段。Spark 中的 join() 操作是在 pairRDD 上定义的。pairRDD 每个元素都以 tuple 的形式出现。tuple 第一个元素是 key，第二个元素是 value。join() 操作根据 key 组合两个数据集。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val data1 = sc.parallelize(Array((&apos;A&apos;, 1), (&apos;B&apos;, 2)))</span><br><span class="line">val data2 = sc.parallelize(Array((&apos;A&apos;, 4), (&apos;A&apos;, 6), (&apos;b&apos;, 7), (&apos;c&apos;, 3), (&apos;c&apos;, 8)))</span><br><span class="line">val result = data1.join(data2)</span><br><span class="line">println(result.collect().mkString(&quot;,&quot;)) // (A,(1,4)),(A,(1,6))</span><br></pre></td></tr></table></figure><h1 id="7-Spark-算子实战–action"><a href="#7-Spark-算子实战–action" class="headerlink" title="7. Spark 算子实战–action"></a>7. Spark 算子实战–action</h1><h2 id="7-1-count"><a href="#7-1-count" class="headerlink" title="7.1 count"></a>7.1 count</h2><p>count() 返回 RDD 中的元素数量。</p><h2 id="7-2-take"><a href="#7-2-take" class="headerlink" title="7.2 take"></a>7.2 take</h2><p>从 RDD 返回 n 个元素。它试图减少它访问的分区数量，不能使用此方法来控制访问元素的顺序。</p><h2 id="7-3-top"><a href="#7-3-top" class="headerlink" title="7.3 top"></a>7.3 top</h2><p>如果 RDD 中元素有序，那么可以使用 top() 从 RDD 中提取前几个元素。</p><p><strong>Scala</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val fileRDD = sc.textFile(&quot;in/README.md&quot;)</span><br><span class="line">val lengthRDD = fileRDD.map(line =&gt; (line,line.length))</span><br><span class="line">lengthRDD.top(3).foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; javaRDD = jsc.textFile(&quot;in/README.md&quot;);</span><br><span class="line">javaRDD.top(3).forEach(item -&gt; System.out.println(item));</span><br></pre></td></tr></table></figure><h2 id="7-4-countByValue"><a href="#7-4-countByValue" class="headerlink" title="7.4 countByValue"></a>7.4 countByValue</h2><p>countByValue()  返回，每个元素都出现在 RDD 中的次数。例如：<br>RDD 中的元素{1, 2, 2, 3, 4, 5, 5, 6}，“rdd.countByValue()” -&gt; {(1,1), (2,2), (3,1), (4,1), (5,2), (6,1)}，返回一个 HashMap(K, Int) ，包括每个 key 的计数。</p><p><strong>Scala</strong>: wordcount 另一种实现</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val fileRDD = sc.textFile(&quot;in/README.md&quot;)</span><br><span class="line">fileRDD.flatMap(line =&gt; line.split(&quot; &quot;)).countByValue().foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; javaRDD = jsc.textFile(&quot;in/README.md&quot;);</span><br><span class="line">JavaRDD&lt;String&gt; javaRDD1 = javaRDD.flatMap(line -&gt; Arrays.asList(line.split(&quot; </span><br><span class="line">&quot;)).iterator());javaRDD1.countByValue().forEach((key, value) -&gt; System.out.println(key + &quot;,&quot; + </span><br><span class="line">value));</span><br></pre></td></tr></table></figure><h2 id="7-5-reduce"><a href="#7-5-reduce" class="headerlink" title="7.5 reduce"></a>7.5 reduce</h2><p>reduce() 函数将 RDD 的两个元素作为输入，然后生成与输入元素相同类型的输出。这种函数的简单形式就是一个加法。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List(12,13,14,43,53,65,34))</span><br><span class="line">val sum = rdd1.reduce(_+_)</span><br><span class="line">println(sum)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; parallelizeRDD = jsc.parallelize(Arrays.asList(12, 13, 14, 43, 53, 65, 34));</span><br><span class="line">int sum = parallelizeRDD.reduce((a, b) -&gt; a + b);</span><br><span class="line">System.out.println(sum);</span><br></pre></td></tr></table></figure><h2 id="7-6-collect"><a href="#7-6-collect" class="headerlink" title="7.6 collect"></a>7.6 collect</h2><p>collect() 是将整个 RDDs 内容返回给 driver 程序的常见且最简单的操作。collect() 的应用是<strong>单元测试</strong>，在单元测试中，期望整个 RDD 能够装入内存。如果使用了 collect 方法，但是 driver 内存不够，则会内存溢出。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val data1 = sc.parallelize(Array((&apos;A&apos;, 1), (&apos;B&apos;, 2)))</span><br><span class="line">val data2 = sc.parallelize(Array((&apos;A&apos;, 4), (&apos;A&apos;, 6), (&apos;b&apos;, 7), (&apos;c&apos;, 3), (&apos;c&apos;, 8)))</span><br><span class="line">val result = data1.join(data2)</span><br><span class="line">println(result.collect().mkString(&quot;,&quot;)) // (A,(1,4)),(A,(1,6))</span><br></pre></td></tr></table></figure><h2 id="7-7-foreach-（无返回值）"><a href="#7-7-foreach-（无返回值）" class="headerlink" title="7.7 foreach （无返回值）"></a>7.7 foreach （无返回值）</h2><p>当我们希望对 RDD 的每个元素应用操作，但它不应该返回值给 driver 程序时。在这种情况下，foreach() 函数是非常合适的。例如，向输入库插入一条记录。</p><h2 id="7-8-foreachParitition（无返回值）"><a href="#7-8-foreachParitition（无返回值）" class="headerlink" title="7.8 foreachParitition（无返回值）"></a>7.8 foreachParitition（无返回值）</h2><p>类似 mapPartitions, 区别在于：1、foreachPartition 是 action 操作 2、foreachPartition 函数没有返回值（返回值是unit）。</p><h2 id="7-9-作业："><a href="#7-9-作业：" class="headerlink" title="7.9 作业："></a>7.9 作业：</h2><ol><li>航班数最多的航空公司，算出前 6 名。</li><li>北京飞往重庆的航空公司，有多少个？</li></ol><p><strong>数据格式</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">阿克苏,41.188341,80.293842,北京,39.92998578,116.395645,3049,CA1276,中国国航,JET,15:40,21:40,阿克苏机场,41.26940127,80.30091874,首都机场,40.06248537,116.5992671,63%,42分钟,1,0,1,0,1,0,1</span><br></pre></td></tr></table></figure><ol><li>航班数最多的航空公司，算出前 6 名。</li></ol><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val textFileRDD = sc.textFile(&quot;in/Flight.csv&quot;)</span><br><span class="line">val airlinesRDD = textFileRDD.map(line =&gt; (line.split(&quot;,&quot;)(8), 1))</span><br><span class="line">val resRDD = airlinesRDD.reduceByKey(_ + _)</span><br><span class="line">val list1 = resRDD.sortBy(_._2, false)</span><br><span class="line">  .collect()</span><br><span class="line">// 取前6名</span><br><span class="line">for (i &lt;- 0 to 5) &#123;</span><br><span class="line">  println(list1(i))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; textFileRDD = jsc.textFile(&quot;in/Flight.csv&quot;);</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD = textFileRDD.mapToPair(line -&gt; new Tuple2&lt;&gt;(line.split(&quot;,&quot;)[8], 1));</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD1 = javaPairRDD.reduceByKey((a, b) -&gt; (a + b));</span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; integerStringJavaPairRDD = javaPairRDD1.mapToPair(t -&gt; new Tuple2&lt;&gt;(t._2, t._1));</span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; integerStringJavaPairRDD1 = integerStringJavaPairRDD.sortByKey(false);</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD2 = integerStringJavaPairRDD1.mapToPair(t -&gt; new Tuple2&lt;&gt;(t._2, t._1));</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;String, Integer&gt;&gt; collect = javaPairRDD2.collect();</span><br><span class="line">for (int i = 0; i &lt; 5; i++) &#123;</span><br><span class="line">    System.out.println(collect.get(i));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>北京飞往重庆的航空公司，有多少个？</li></ol><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val BJ_CQ_RDD = textFileRDD.filter(line =&gt; (line.split(&quot;,&quot;)(0).equals(&quot;北京&quot;) &amp;&amp; line.split(&quot;,&quot;)(3).equals(&quot;重庆&quot;)))</span><br><span class="line">val count = BJ_CQ_RDD.map(line =&gt; (line.split(&quot;,&quot;)(8), 1)).countByKey().size</span><br><span class="line">println(count)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; filterRDD = textFileRDD.filter(line -&gt; (line.split(&quot;,&quot;)[0].equals(&quot;北京&quot;) &amp;&amp; line.split(&quot;,&quot;)[3].equals(&quot;重庆&quot;)));</span><br><span class="line">int size = filterRDD.mapToPair(line -&gt; new Tuple2&lt;&gt;(line.split(&quot;,&quot;)[8], 1)).countByKey().size();</span><br><span class="line">System.out.println(size);</span><br></pre></td></tr></table></figure><h1 id="8-Spark-RDD-分区实战"><a href="#8-Spark-RDD-分区实战" class="headerlink" title="8. Spark RDD 分区实战"></a>8. Spark RDD 分区实战</h1><h2 id="8-1-RDD-partition-概念"><a href="#8-1-RDD-partition-概念" class="headerlink" title="8.1 RDD partition 概念"></a>8.1 RDD partition 概念</h2><p>我们处理大数据时，由于数据量太大，以至于单个节点无法完全存储、计算。所以这些数据需要分割成多个数据块 block，以利用多个集群节点的存储、计算资源。Spark 自动对 RDDs 中的大量数据元素进行分区，并在 worker 节点之间分配分区，计算。分区是逻辑上。</p><h2 id="8-2-RDD-partition-的相关属性"><a href="#8-2-RDD-partition-的相关属性" class="headerlink" title="8.2 RDD partition 的相关属性"></a>8.2 RDD partition 的相关属性</h2><table><thead><tr><th>属性</th><th>描述</th></tr></thead><tbody><tr><td>partitions</td><td>返回包含 RDD 所有分区引用 的一个数组</td></tr><tr><td>partitions.size</td><td>返回 RDD 的分区数量</td></tr><tr><td>partitioner</td><td>返回下列分区器之一：<br>NONE<br>HashPartitioner<br>RangePartitioner<br>自定义分区器</td></tr></tbody></table><p>Spark 使用 partitioner 属性来确定分区算法，以此来确定哪些 worker 需要存储特定的 RDD记录。如果 partitoner 的值为 NONE，意思是分区不是基于数据的特性 ，但是分布是随机的，并且保证在节点之间是均匀地。</p><h2 id="8-3-查看-RDD-partition-信息"><a href="#8-3-查看-RDD-partition-信息" class="headerlink" title="8.3 查看 RDD partition 信息"></a>8.3 查看 RDD partition 信息</h2><p><strong>textfile 方法的 partition size 查看</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">println(textFileRDD.partitions.size)</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：使用 textFile 方法读取数据，可以设置 partition 大小：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val textFileRDD = sc.textFile(&quot;in/Flight.csv&quot;, 2)</span><br></pre></td></tr></table></figure><p>在集群环境中，读取本地文件、HDFS数据，由数据的 block 个数决定，最小为2。</p><p><strong>特殊情况</strong>：如果 local 模式，单线程运行，默认 partitions.size 为1。（项目中不会使用此情况）</p><p><strong>注意</strong>：每个 partition 会运行一个task 来处理其中的数据元素。</p><h2 id="8-4-RDD-的初始分区"><a href="#8-4-RDD-的初始分区" class="headerlink" title="8.4 RDD 的初始分区"></a>8.4 RDD 的初始分区</h2><blockquote><p>conf.set(“spark.default.parallelism”,”3”)   // 设置默认的并行度</p></blockquote><p>local: 一个线程———–sc.defaultParallelism 值为1<br>local[*]: 服务器core 数量——–sc. defaultParallelism 的值为8<br>local[4]: 4个线程———–sc.defaultParallelism 的值为4</p><p><strong>spark.default.parallelism 参数值的说明</strong>：<br>如果 spark-default.conf 或 SparkConf 中设置了 spark.default.parallelism 参数值，那么 spark.default.parallelism = 设置值；<br>如果 spark-default.conf 或 SparkConf 中没有设置，那么：</p><p><strong>local 模式</strong>：<br>local: spark.default.parallelism =1<br>local[4]: spark.default.parallelism = 4</p><p><strong>yarn 和 standalone 模式</strong>：<br>spark.default.parallelism = max(所有 executor 使用的core 总数, 2)</p><p><strong>由上述规则，确定 spark.default.parallelism 的默认值</strong><br>当Spark 程序执行时，会生成个 SparkContext 对象，同时会生成以下两个参数值：<br>sc.defaultParallelism = spark.default.parallelism<br>sc.defaultMinPartitions = min(spark.default.parallelism, 2)</p><p>当sc.defaultParallelism 和 sc.defaultMinPartitions 确认了，就可以推算出RDD 的分区数了。</p><p><strong>有三种产生 RDD 的方式</strong>：</p><ol><li>通过集合创建<blockquote><p>val rdd = sc.parallelize(1 to 100)</p></blockquote></li></ol><p>没有指定分区数，则rdd的分区数 = sc.defaultParallelism</p><ol start="2"><li>通过外部存储创建<blockquote><p>val rdd = sc.textFile(filePath)</p></blockquote></li></ol><p>2.1 从本地 文件生成 RDD，没有指定分区数，则默认分区规则为: rdd 的分区数 = max(本地 file 的分片数, sc.defaultMinPartitions)<br>2.2 从 HDFS 读取数据生成 RDD，没有指定分区数，则默认 分区规则为：rdd 的分区数 = max(HDFS文件的 block 数, sc.defaultMinPartitions)</p><ol start="3"><li>通过已有 RDD 产生新的 RDD，新 RDD的分区数遵循<strong>遗传</strong>特性。见下节。</li></ol><p><strong>注</strong>：项目中，在 spark-default.conf 文件中，spark.default.parallelism 属性值设置为 executor-cores * executors 个数 * 3</p><h2 id="8-5-Transformation-操作对分区的影响"><a href="#8-5-Transformation-操作对分区的影响" class="headerlink" title="8.5 Transformation 操作对分区的影响"></a>8.5 Transformation 操作对分区的影响</h2><h3 id="8-5-1-普通-RDD-操作"><a href="#8-5-1-普通-RDD-操作" class="headerlink" title="8.5.1 普通 RDD 操作"></a>8.5.1 普通 RDD 操作</h3><table><thead><tr><th>API调用</th><th>RDD 分区属性值<br>partition.size</th><th>RDD 分区属性值<br>partitioner</th></tr></thead><tbody><tr><td>map(),  flatMap(), distinct()</td><td>与父RDD相同</td><td>NONE</td></tr><tr><td>filter()</td><td>与父RDD相同</td><td>与父RDD相同</td></tr><tr><td>rdd.union(otherRDD)</td><td>rdd.partitions.size + otherRDD.partitions.size</td><td>NONE</td></tr><tr><td>rdd.intersection(otherRDD)</td><td>max(rdd.partitions.size, otherRDD.partitions.size)</td><td>NONE</td></tr><tr><td>rdd.subtract(otherRDD)</td><td>rdd.partitions.size</td><td>NONE</td></tr><tr><td>rdd.cartesian(otherRDD)</td><td>rdd.partitions.size * otherRDD.partitions.size</td><td>NONE</td></tr></tbody></table><h3 id="8-5-2-Key-value-RDD-操作"><a href="#8-5-2-Key-value-RDD-操作" class="headerlink" title="8.5.2 Key-value RDD 操作"></a>8.5.2 Key-value RDD 操作</h3><table><thead><tr><th>API调用</th><th>RDD 分区属性值<br>partition.size</th><th>RDD 分区属性值<br>partitioner</th></tr></thead><tbody><tr><td>reduceByKey(),foldByKey(),combineByKey(),groupByKey()</td><td>与父RDD相同</td><td>HashPartitioner</td></tr><tr><td>sortByKey</td><td>与父RDD相同</td><td>RangePartitioner</td></tr><tr><td>mapValues(), flatMapValues()</td><td>与父RDD相同</td><td>与父RDD相同</td></tr><tr><td>cogroup(),join(),leftOuterJoin(), rightOuterJoin()</td><td>取决于所涉及的两个 RDDs 的某些输入属性</td><td>HashPartitioner</td></tr></tbody></table><h2 id="8-6-有多少分区是合适的（重点！！）"><a href="#8-6-有多少分区是合适的（重点！！）" class="headerlink" title="8.6 有多少分区是合适的（重点！！）"></a>8.6 有多少分区是合适的（重点！！）</h2><p>分区数量太少、太多都有一定的优点和缺点。因此，建议根据集群配置和需求进行明智的分区。<br>Core-partition-task</p><h3 id="8-6-1-分区太少的缺点"><a href="#8-6-1-分区太少的缺点" class="headerlink" title="8.6.1 分区太少的缺点"></a>8.6.1 分区太少的缺点</h3><p>减少并发性——您没有使用并行性的优点。可能存在空闲的 wroker 节点。<br>数据倾斜和不恰当的资源利用——数据可能在一个分区上倾斜，因此一个 worker 可能比其他 worker 做的更多，因此可能会出现资源问题。</p><h3 id="8-6-2-分区太多的缺点"><a href="#8-6-2-分区太多的缺点" class="headerlink" title="8.6.2 分区太多的缺点"></a>8.6.2 分区太多的缺点</h3><p>任务调度可能比实际执行时间花费更多的时间。</p><p><strong>因此，在分区的数量之间存在权衡。推荐如下</strong>：</p><ol><li><p>可用 core 数量的2-3 倍。Spark 只为 RDD 的每个分区运行一个并发任务，最多可以同时运行集群中的核心数量个 task，分区数量至少与可用 core 数量相等。可以通过调用 sc.defaultParallelism 获得可用 core 值。单个分区的数据量大小最终取决于执行程序的可用内存。</p></li><li><p>WebUI 上查看任务执行，至少需要 100+ ms 时间。如果所用 时间少于 100ms，那么应用程序可能会花更多的时间来调度任务。此时就要减少 partition 的数量。</p></li></ol><h2 id="8-7-Spark-中的分区器"><a href="#8-7-Spark-中的分区器" class="headerlink" title="8.7 Spark 中的分区器"></a>8.7 Spark 中的分区器</h2><p>要使用分区器，首先要创建 PairRDD类型的 RDD。<br>Spark 有两种类型的分区器。一个是 HashPartitioner，另一个是 RangePartitioner。</p><h3 id="8-7-1-HashPartitioner"><a href="#8-7-1-HashPartitioner" class="headerlink" title="8.7.1 HashPartitioner"></a>8.7.1 HashPartitioner</h3><p>HashPartitioner 基于 Java 的 Object.hashcode() 方法进行分区。</p><h3 id="8-7-2-RangePartitioner"><a href="#8-7-2-RangePartitioner" class="headerlink" title="8.7.2 RangePartitioner"></a>8.7.2 RangePartitioner</h3><p>如果有可排序的记录，那么范围分区将几乎在相同的范围内划分记录。范围 Range 是 通过采样传入 RDD的数据内容来确定的。首先，RangePartitioner 将根据 key 对记录进行排序，然后根据给定的值将记录划分为 若干个分区。</p><h3 id="8-7-3-自定义分区器"><a href="#8-7-3-自定义分区器" class="headerlink" title="8.7.3 自定义分区器"></a>8.7.3 自定义分区器</h3><p>还可以通过扩展 Spark 中的默认分区器类来定制 需要的分区数量和应该存储在这些分区中的内容。</p><p><strong>代码示例</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">val listRDD = sc.parallelize((1 to 10).toList)</span><br><span class="line">val pairRDD = listRDD.map(num =&gt; (num, num))</span><br><span class="line">println(&quot;NumPartitions: &quot; + pairRDD.getNumPartitions) // NumPartitions: 8</span><br><span class="line">println(&quot;Partitioner: &quot; + pairRDD.partitioner)  // Partitioner: None</span><br><span class="line">pairRDD.saveAsTextFile(&quot;out/None&quot;)</span><br><span class="line"></span><br><span class="line">// 使用 HashPartitioner 并 设置分区个数</span><br><span class="line">val hashPartitionerRDD = pairRDD.partitionBy(new HashPartitioner(4))</span><br><span class="line">hashPartitionerRDD.saveAsTextFile(&quot;out/hashPartition4&quot;)</span><br><span class="line"></span><br><span class="line">// coalesce 方法只能用来减少 分区数量，不能用来增加分区数量</span><br><span class="line">// partitionBy 方法可以减少，也可以增加</span><br><span class="line">hashPartitionerRDD.coalesce(2).saveAsTextFile(&quot;out/hashPartition2&quot;)</span><br><span class="line">println(hashPartitionerRDD.partitioner) // Some(org.apache.spark.HashPartitioner@4)</span><br></pre></td></tr></table></figure><h1 id="9-Spark-RDD-数据保存实战"><a href="#9-Spark-RDD-数据保存实战" class="headerlink" title="9. Spark RDD 数据保存实战"></a>9. Spark RDD 数据保存实战</h1><h2 id="9-1-保存数据到-HDFS"><a href="#9-1-保存数据到-HDFS" class="headerlink" title="9.1 保存数据到 HDFS"></a>9.1 保存数据到 HDFS</h2><p><strong>代码示例</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val textFileRDD = sc.textFile(&quot;hdfs://master01:8020/in/README.txt&quot;, 2)</span><br><span class="line">val wordsRDD = textFileRDD.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">val wordcountPairRDD = wordsRDD.map(w =&gt; (w, 1))</span><br><span class="line">val wordcountRDD = wordcountPairRDD.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">wordcountRDD.saveAsTextFile(&quot;hdfs://master01:8020/out/wordcount&quot;)</span><br></pre></td></tr></table></figure><h2 id="9-2-保存数据到-mysql-数据库"><a href="#9-2-保存数据到-mysql-数据库" class="headerlink" title="9.2 保存数据到 mysql 数据库"></a>9.2 保存数据到 mysql 数据库</h2><h3 id="9-2-1-读"><a href="#9-2-1-读" class="headerlink" title="9.2.1 读"></a>9.2.1 读</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">//mysql 读</span><br><span class="line">val jdbcDF = sparkSession.read</span><br><span class="line">  .format(&quot;jdbc&quot;)</span><br><span class="line">  .option(&quot;url&quot;, &quot;jdbc:mysql://master01:3306/test&quot;)</span><br><span class="line">  .option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">  .option(&quot;password&quot;, &quot;Mysql123!&quot;)</span><br><span class="line">  .option(&quot;dbtable&quot;, &quot;flight&quot;)</span><br><span class="line">  .load()</span><br><span class="line">jdbcDF.printSchema()</span><br></pre></td></tr></table></figure><h3 id="9-2-2-写"><a href="#9-2-2-写" class="headerlink" title="9.2.2 写"></a>9.2.2 写</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">val schema = StructType(List(</span><br><span class="line">  StructField(&quot;name&quot;, StringType, nullable = false),</span><br><span class="line">  StructField(&quot;age&quot;, IntegerType, nullable = false),</span><br><span class="line">  StructField(&quot;gender&quot;, StringType, nullable = false)</span><br><span class="line">))</span><br><span class="line"></span><br><span class="line">val rowRDD = sc.parallelize(Seq(</span><br><span class="line">  Row(&quot;张1&quot;, 18, &quot;男&quot;),</span><br><span class="line">  Row(&quot;张2&quot;, 19, &quot;女&quot;),</span><br><span class="line">  Row(&quot;张3&quot;, 10, &quot;男&quot;),</span><br><span class="line">  Row(&quot;张4&quot;, 48, &quot;女&quot;),</span><br><span class="line">  Row(&quot;张5&quot;, 68, &quot;男&quot;),</span><br><span class="line">  Row(&quot;张6&quot;, 16, &quot;男&quot;)</span><br><span class="line">))</span><br><span class="line">val df = sparkSession.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line">// mysql 写</span><br><span class="line">df.write</span><br><span class="line">  .format(&quot;jdbc&quot;)</span><br><span class="line">  .option(&quot;url&quot;, &quot;jdbc:mysql://master01:3306/test&quot;)</span><br><span class="line">  .option(&quot;dbtable&quot;, &quot;user&quot;)</span><br><span class="line">  .option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">  .option(&quot;password&quot;, &quot;Mysql123!&quot;)</span><br><span class="line">  .mode(SaveMode.Overwrite)</span><br><span class="line">  .save()</span><br></pre></td></tr></table></figure><h2 id="9-3-保存数据到-kafka"><a href="#9-3-保存数据到-kafka" class="headerlink" title="9.3 保存数据到 kafka"></a>9.3 保存数据到 kafka</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// producer 配置</span><br><span class="line">val props = new Properties()</span><br><span class="line">props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;master01:9092&quot;)</span><br><span class="line">props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)</span><br><span class="line">props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)</span><br><span class="line"></span><br><span class="line">// producer 发送 RDD 数据</span><br><span class="line">val textFileRDD = sc.textFile(&quot;in/Flight1.csv&quot;)</span><br><span class="line">textFileRDD.foreach(line =&gt; &#123;</span><br><span class="line">  val producer = new KafkaProducer[String, String](props)</span><br><span class="line">  val message = new ProducerRecord[String, String](&quot;myTopic&quot;, line)</span><br><span class="line">  println(message)</span><br><span class="line">  producer.send(message)</span><br><span class="line">  Thread.sleep(3000)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h1 id="10-Spark-RDD-缓存实战"><a href="#10-Spark-RDD-缓存实战" class="headerlink" title="10. Spark RDD 缓存实战"></a>10. Spark RDD 缓存实战</h1><h2 id="10-1-前言"><a href="#10-1-前言" class="headerlink" title="10.1 前言"></a>10.1 前言</h2><p>一个 action 会启动一个 job， 一个 job 里面有一个或多个 stage，一个 stage 里面有一个或者多个 task。</p><p>Repartiton 引起 shuffle 操作，shuffle 操作发生的时候，stage 会一分为二。</p><p>窄依赖， 宽依赖<br>一种性能调优的方式。</p><h2 id="10-2-要点"><a href="#10-2-要点" class="headerlink" title="10.2 要点"></a>10.2 要点</h2><ol><li><p><strong>缓存</strong> 和 <strong>持久化</strong>是 Spark 计算过程中的调优技术。缓存和持久化可以保存中间计算结果，以便在后续的 stage 中重用，而不需要再次从头计算。这些中间结果以 RDD 的形式保存在内存（默认）中，或者磁盘中。</p></li><li><p>StorageLevel 描述了 RDD 是如何被<strong>持久化</strong>（persist）的，可以提供如下相关信息：</p></li></ol><ul><li>RDD 持久化磁盘存储还是内存存储</li><li>RDD 持久化是否使用了 off-heap</li><li>RDD 是否需要被序列化</li><li>缓存的副本是多少（默认是 1）</li></ul><ol start="3"><li><strong>StorageLevel</strong> 的值包括：</li></ol><ul><li>NONE（默认）</li><li>DISK_ONLY: RDD 只是存储在磁盘，内存消耗低，CPU 密集型。</li><li>DISK_ONLY_2</li><li>MEMORY_ONLY（cache 操作）：RDD 以非序列化的 Java 对象存储在 JVM中。如果 RDD 的大小超过了内存大小，那么某些 partition 将会不缓存，下次使用时重新计算。这种存储级别比较耗内存，但是不耗 CPU。数据只存储在内存，不存储在磁盘。</li><li>MEMORY_ONLY_2</li><li>MEMORY_ONLY_SER: RDD 以序列化 Java 对象（每个 partition 一个字节数组）的形式存储。在这个级别，内存空间 使用很低，CPU计算时间高。</li><li>MEMORY_ONLY_SER_2</li><li>MEMORY_AND_DISK: RDD 以非序列化的 Java 对象存储在 JVM 中。当 RDD 的大小超过了内存 大小，多出的 partition  会缓存在磁盘上，后续计算如果用到这些多出的 partiton，会从磁盘获取。这种存储级别比较耗内存，CPU消耗一般。</li><li>MEMORY_AND_DISK_2</li><li>MEMORY_AND_DISK_SER: 与 MEMORY_ONLY_SER 类似，只是将大于内存的partition 数据序列化到磁盘，而不是重新计算。内存消耗低，CPU密集型。</li><li>MEMORY_AND_DISK_SER_2</li><li>OFF_HEAP</li></ul><p>可以使用 <strong>getStorageLevel</strong> 方法查看 RDD 的 StorageLevel:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val textFileRDD = sc.textFile(&quot;\in\README.txt&quot;)</span><br><span class="line">println(textFileRDD.getStorageLevel)</span><br><span class="line">println(textFileRDD.getStorageLevel)</span><br></pre></td></tr></table></figure><p><strong>输出</strong>：</p><blockquote><p>StorageLevel(1 replicas)</p></blockquote><ol start="4"><li><p>RDD 可以被缓存（cache）到内存，使用 cache() 方法，也可以被持久化（persist），使用 persist() 方法。</p></li><li><p>cache() 和 persist() 方法的<strong>区别</strong>在于：cache() 等价于 persist(MEMEORY_ONLY)，即 cache() 仅仅是 persist() 使用默认存储级别 MEMORY_ONLY 的一种情况。使用 persist() 方法可以设置不同的 StorageLevel值。</p></li><li><p>对于<strong>迭代算法</strong>，缓存和持久化是一个重要的工具。因为，当我们在一个节点上缓存了 RDD 的某个 partiton 到内存中，其就可以在下面的计算中重复使用，而不需要从头计算，可以使计算性能提高 <strong>10</strong>倍。如果缓存中的某个 partiton 丢失或者不可用，根据 Spark RDD 的容错特性，Spark 会从头计算这个 partition。</p></li><li><p>什么时候需要对 RDD 进行持久化？在 Spark 中，我们可以多次使用同一个 RDD，如：使用 RDD 计算 count()、max()、min()等 action 操作。而且这些操作可能<strong>很耗内存</strong>，尤其是迭代算法（机器学习）。为了解决<strong>频繁重复计算</strong>的问题，此时就需要对 RDD 进行持久化。</p></li><li><p>Spark 自动监控每个节点的<strong>缓存</strong>和以 <strong>LRU</strong>（最近最少使用）方式删除旧数据分区。LRU算法，保证了最常用的数据被缓存。我们 也可以使用 <strong>RDD.unpersist()</strong> 方法手动删除缓存。</p></li><li><p>Spark 会在 <strong>shuffle</strong> 操作中<strong>自动持久化</strong>一些中间数据（例如 redueByKey），即使没有调用 persist 方法。这样做是为了避免在 shuffle 期间节点故障时重新计算整个输入。如果用户准备重用生成的 RDD，推荐显式调用持久化。</p></li></ol><h2 id="10-3-RDD-持久化存储级别如何选择"><a href="#10-3-RDD-持久化存储级别如何选择" class="headerlink" title="10.3 RDD 持久化存储级别如何选择"></a>10.3 RDD 持久化存储级别如何选择</h2><p>Spark 的存储级别是为了在<strong>内存使用</strong>和 <strong>CPU 效率</strong>之间 提供不同的权衡，具体选择哪个存储级别，可以从以下方面考虑：</p><ul><li>如果 RDDs 数据适合默认存储级别（MEMORY_ONLY），那么就使用默认。此时，RDD 的运算速度最快。</li><li>如果没有，请尝试使用 MEMORY_ONLY_SER 并选择一个快速序列化库，以使对象更节省空间，但访问速度仍然相当快。</li><li>不要持久化到磁盘，除非计算 数据集的函数很耗时，或者 过滤了大量 数据。因为，从磁盘读取分区，可能没有重新计算快。</li><li>如果需要快速的故障恢复，则使用副本存储级别。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-什么是RDD？&quot;&gt;&lt;a href=&quot;#1-什么是RDD？&quot; class=&quot;headerlink&quot; title=&quot;1. 什么是RDD？&quot;&gt;&lt;/a&gt;1. 什么是RDD？&lt;/h1&gt;&lt;p&gt;RDD 是 Resilient Distributed Dataset（&lt;strong&gt;弹性分布式数据集&lt;/strong&gt;） 的简称。它是 Apache Spark 的基本数据结构。它是一个不可变的对象集合，在集群的不同节点上进行计算。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Resilient: 即在 RDD lineage(DAG) 的帮助下具有容错能力，能够重新计算由于节点故障而丢失或损坏的数据分区。&lt;/li&gt;
&lt;li&gt;Distributed: 数据分布在多个节点上。&lt;/li&gt;
&lt;li&gt;Dataset: 表示所操作的数据集。用户可以通过 JDBC 从外部加载数据集，数据集可以是 JSON 文件，CSV 文件，文本文件或数据库。&lt;br&gt;
&lt;br&gt;
    
    </summary>
    
    
      <category term="笔记" scheme="https://miracle-xing.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Spark" scheme="https://miracle-xing.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop 集群 HA 架构配置</title>
    <link href="https://miracle-xing.github.io/2019/07/23/Hadoop-%E9%9B%86%E7%BE%A4-HA-%E6%9E%B6%E6%9E%84%E9%85%8D%E7%BD%AE/"/>
    <id>https://miracle-xing.github.io/2019/07/23/Hadoop-集群-HA-架构配置/</id>
    <published>2019-07-22T20:42:04.000Z</published>
    <updated>2019-07-23T18:27:43.653Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Linux-基础配置"><a href="#1-Linux-基础配置" class="headerlink" title="1. Linux 基础配置"></a>1. Linux 基础配置</h1><h2 id="1-1-设置静态IP："><a href="#1-1-设置静态IP：" class="headerlink" title="1.1 设置静态IP："></a>1.1 设置静态IP：</h2><p>宿主机配置：<br>VMware: NAT模式，不使用DHCP<br>VMnet8: IPv4使用固定ip，子网掩码</p><a id="more"></a><h2 id="1-2-虚拟机配置："><a href="#1-2-虚拟机配置：" class="headerlink" title="1.2 虚拟机配置："></a>1.2 虚拟机配置：</h2><blockquote><p>vim /etc/sysconfig/network-scripts/ifcfg-ens33</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">TYPE=&quot;Ethernet&quot;</span><br><span class="line">PROXY_METHOD=&quot;none&quot;</span><br><span class="line">BROWSER_ONLY=&quot;no&quot;</span><br><span class="line">BOOTPROTO=&quot;static&quot;</span><br><span class="line">DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV4_FAILURE_FATAL=&quot;yes&quot;</span><br><span class="line">NAME=&quot;ens33&quot;</span><br><span class="line">UUID=&quot;a428bf24-b245-408a-88b6-d0934885c452&quot;</span><br><span class="line">DEVICE=&quot;ens33&quot;</span><br><span class="line">ONBOOT=&quot;yes&quot;</span><br><span class="line">IPADDR=&quot;192.168.12.130&quot;</span><br><span class="line">GATEWAY=&quot;192.168.12.2&quot;</span><br><span class="line">DNS1=&quot;192.168.12.2&quot;</span><br></pre></td></tr></table></figure><h2 id="1-3-修改主机名："><a href="#1-3-修改主机名：" class="headerlink" title="1.3 修改主机名："></a>1.3 修改主机名：</h2><blockquote><p>vim /etc/sysconfig/network</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NETWORKING=yes</span><br><span class="line">HOSTNAME=master01</span><br></pre></td></tr></table></figure><blockquote><p>vim /etc/hostname</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">master01</span><br></pre></td></tr></table></figure><h2 id="1-4-设置IP和主机名映射"><a href="#1-4-设置IP和主机名映射" class="headerlink" title="1.4 设置IP和主机名映射:"></a>1.4 设置IP和主机名映射:</h2><blockquote><p>vim /etc/hosts</p></blockquote><h2 id="1-5-设置免密登录"><a href="#1-5-设置免密登录" class="headerlink" title="1.5 设置免密登录"></a>1.5 设置免密登录</h2><blockquote><p>ssh-keygen</p></blockquote><p>生成公钥和私钥</p><blockquote><p>ssh-copy-id -i ~/.ssh/id_rsa.pub 目标机器用户名@目标机器名</p></blockquote><p>将公钥拷贝到目标机器</p><blockquote><p>ssh 目标机器用户名@目标机器名 </p></blockquote><h2 id="1-6-禁用selinux与防火墙"><a href="#1-6-禁用selinux与防火墙" class="headerlink" title="1.6 禁用selinux与防火墙"></a>1.6 禁用selinux与防火墙</h2><h3 id="1-6-1-selinux"><a href="#1-6-1-selinux" class="headerlink" title="1.6.1 selinux"></a>1.6.1 selinux</h3><blockquote><p>vim /etc/sysconfig/selinux</p></blockquote><p>将SELINUX改成disabled</p><h3 id="1-6-2-防火墙"><a href="#1-6-2-防火墙" class="headerlink" title="1.6.2 防火墙"></a>1.6.2 防火墙</h3><blockquote><p>systemctl stop firewalld<br>systemctl disable firewalld</p></blockquote><h2 id="1-7-卸载-Openjdk"><a href="#1-7-卸载-Openjdk" class="headerlink" title="1.7 卸载 Openjdk"></a>1.7 卸载 Openjdk</h2><p>查看jdk情况</p><blockquote><p>rpm -qa | grep java</p></blockquote><p>若使用openjdk则卸载</p><blockquote><p>rpm -e –nodeps ‘查询到的openjdk，多个文件用空格隔开’</p></blockquote><p>解压JDK<br>配置环境变量</p><h1 id="2-Hadoop-配置"><a href="#2-Hadoop-配置" class="headerlink" title="2. Hadoop 配置"></a>2. Hadoop 配置</h1><h2 id="2-1-集群规划："><a href="#2-1-集群规划：" class="headerlink" title="2.1 集群规划："></a>2.1 集群规划：</h2><p>有3台虚拟机，分别是 master01, master02, slave01, slave02, slave03。</p><table><thead><tr><th>节点</th><th>master01</th><th>master02</th><th>slave01</th><th>slave02</th><th>slave03</th></tr></thead><tbody><tr><td><strong>组件</strong></td><td>Namenode<br>DFSZKFailoverController<br>ResourceManager<br>Jobhistory</td><td>Namenode<br>DFSZKFailoverController</td><td>Datanode<br>NodeManager<br>JournalNode</td><td>Datanode<br>NodeManager<br>JournalNode</td><td>Datanode<br>NodeManager<br>JournalNode</td></tr></tbody></table><h2 id="2-2-解压-hadoop-并清理文档"><a href="#2-2-解压-hadoop-并清理文档" class="headerlink" title="2.2 解压 hadoop 并清理文档"></a>2.2 解压 hadoop 并清理文档</h2><blockquote><p>tar -zxvf hadoop-2.7.7.tar.gz -C /opt/modules/<br>mv hadoop-2.7.7/ hadoop277</p></blockquote><p>清理hadoop-2.5.0/share/doc</p><h2 id="2-3-指定-Java-路径"><a href="#2-3-指定-Java-路径" class="headerlink" title="2.3 指定 Java 路径"></a>2.3 指定 Java 路径</h2><p>文件：hadoop-env.sh / mapred-env.sh / yarn-env.sh</p><h2 id="2-4-HDFS-相关修改"><a href="#2-4-HDFS-相关修改" class="headerlink" title="2.4 HDFS 相关修改"></a>2.4 HDFS 相关修改</h2><h3 id="2-4-1-修改-core-site-xml"><a href="#2-4-1-修改-core-site-xml" class="headerlink" title="2.4.1 修改 core-site.xml"></a>2.4.1 修改 core-site.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hdfs://ns&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/opt/modules/hadoop277/data/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;master01:2181,master02:2181,slave01:2181,slave02:2181,slave03:2181&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="2-4-2-修改hdfs-site-xml"><a href="#2-4-2-修改hdfs-site-xml" class="headerlink" title="2.4.2 修改hdfs-site.xml"></a>2.4.2 修改hdfs-site.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;ns&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.ha.namenodes.ns&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;nn1,nn2&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.rpc-address.ns.nn1&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;master01:8020&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.rpc-address.ns.nn2&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;master02:8020&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.http-address.ns.nn1&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;master01:50070&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.http-address.ns.nn2&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;master02:50070&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">       &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;qjournal://slave01:8485;slave02:8485;slave03:8485/ns&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.client.failover.proxy.provider.ns&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;/opt/modules/hadoop277/data/dfs/jn&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;sshfence&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="2-4-3-修改slaves"><a href="#2-4-3-修改slaves" class="headerlink" title="2.4.3 修改slaves"></a>2.4.3 修改slaves</h3><p>从节点主机名</p><blockquote><p>slave01<br>slave02<br>slave03</p></blockquote><h2 id="2-5-MapReduce-与-YARN-修改"><a href="#2-5-MapReduce-与-YARN-修改" class="headerlink" title="2.5 MapReduce 与 YARN 修改"></a>2.5 MapReduce 与 YARN 修改</h2><h3 id="2-5-1-修改-mapred-site-xml"><a href="#2-5-1-修改-mapred-site-xml" class="headerlink" title="2.5.1 修改 mapred-site.xml"></a>2.5.1 修改 mapred-site.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master01:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master01:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="2-5-2-修改-yarn-site-xml"><a href="#2-5-2-修改-yarn-site-xml" class="headerlink" title="2.5.2 修改 yarn-site.xml"></a>2.5.2 修改 yarn-site.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">       &lt;!-- 开启RM高可靠 --&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;!-- 指定RM的cluster id --&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;RM_HA_ID&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;!-- 指定RM的名字 --&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;rm1,rm2&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;!-- 分别指定RM的地址 --&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;master01&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;master02&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;!-- 指定zk集群地址 --&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;master01:2181,master02:2181,slave01:2181,slave02:2181,slave03:2181&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p><strong>分发 hadoop 文件到其他节点</strong></p><h2 id="2-6-集群启动"><a href="#2-6-集群启动" class="headerlink" title="2.6 集群启动"></a>2.6 集群启动</h2><h3 id="2-6-1-启动-zookeeper，再启动-journalnode"><a href="#2-6-1-启动-zookeeper，再启动-journalnode" class="headerlink" title="2.6.1 启动 zookeeper，再启动 journalnode"></a>2.6.1 启动 zookeeper，再启动 journalnode</h3><p>slave01,  slave02, slave03</p><blockquote><p>sbin/hadoop-daemon.sh start journalnode</p></blockquote><h3 id="2-6-2-格式化-namenode"><a href="#2-6-2-格式化-namenode" class="headerlink" title="2.6.2 格式化 namenode"></a>2.6.2 格式化 namenode</h3><p>master01</p><blockquote><p>bin/hdfs namenode -format</p></blockquote><h3 id="2-6-3-同步元数据"><a href="#2-6-3-同步元数据" class="headerlink" title="2.6.3 同步元数据"></a>2.6.3 同步元数据</h3><p>master01 上启动 namenode </p><p>master02：</p><blockquote><p>bin/hdfs namenode -bootstrapStandby</p></blockquote><h3 id="2-6-4-初始化-ZKFC"><a href="#2-6-4-初始化-ZKFC" class="headerlink" title="2.6.4 初始化 ZKFC"></a>2.6.4 初始化 ZKFC</h3><p>master01</p><blockquote><p>bin/hdfs zkfc -formatZK<br>//zk下生成hadoop-ha目录表示成功</p></blockquote><h3 id="2-6-5-启动-HDFS-相关进程"><a href="#2-6-5-启动-HDFS-相关进程" class="headerlink" title="2.6.5 启动 HDFS 相关进程"></a>2.6.5 启动 HDFS 相关进程</h3><p>master01</p><blockquote><p>sbin/start-dfs.sh</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Starting namenodes on [master01 master02]</span><br><span class="line">master02: starting namenode, logging to /opt/modules/hadoop277/logs/hadoop-root-namenode-master02.out</span><br><span class="line">master01: starting namenode, logging to /opt/modules/hadoop277/logs/hadoop-root-namenode-master01.out</span><br><span class="line">slave01: starting datanode, logging to /opt/modules/hadoop277/logs/hadoop-root-datanode-slave01.out</span><br><span class="line">slave02: starting datanode, logging to /opt/modules/hadoop277/logs/hadoop-root-datanode-slave02.out</span><br><span class="line">slave03: starting datanode, logging to /opt/modules/hadoop277/logs/hadoop-root-datanode-slave03.out</span><br><span class="line">Starting journal nodes [slave01 slave02 slave03]</span><br><span class="line">slave02: starting journalnode, logging to /opt/modules/hadoop277/logs/hadoop-root-journalnode-slave02.out</span><br><span class="line">slave03: starting journalnode, logging to /opt/modules/hadoop277/logs/hadoop-root-journalnode-slave03.out</span><br><span class="line">slave01: starting journalnode, logging to /opt/modules/hadoop277/logs/hadoop-root-journalnode-slave01.out</span><br><span class="line">Starting ZK Failover Controllers on NN hosts [master01 master02]</span><br><span class="line">master02: starting zkfc, logging to /opt/modules/hadoop277/logs/hadoop-root-zkfc-master02.out</span><br><span class="line">master01: starting zkfc, logging to /opt/modules/hadoop277/logs/hadoop-root-zkfc-master01.out</span><br></pre></td></tr></table></figure><h3 id="2-6-6-查看进程"><a href="#2-6-6-查看进程" class="headerlink" title="2.6.6 查看进程"></a>2.6.6 查看进程</h3><p><strong>master01 进程</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">6392 Jps</span><br><span class="line">6026 DFSZKFailoverController</span><br><span class="line">2027 QuorumPeerMain</span><br><span class="line">5711 NameNode</span><br><span class="line">6191 ResourceManager</span><br></pre></td></tr></table></figure><p><strong>master02 进程</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">3635 NameNode</span><br><span class="line">3752 DFSZKFailoverController</span><br><span class="line">3884 Jps</span><br><span class="line">1805 QuorumPeerMain</span><br></pre></td></tr></table></figure><p><strong>slave01, slave02, slave03 进程</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">3507 DataNode</span><br><span class="line">2085 QuorumPeerMain</span><br><span class="line">3606 JournalNode</span><br><span class="line">3863 Jps</span><br><span class="line">3759 NodeManager</span><br></pre></td></tr></table></figure><h3 id="2-6-7-验证测试"><a href="#2-6-7-验证测试" class="headerlink" title="2.6.7 验证测试"></a>2.6.7 验证测试</h3><p><strong>HDFS webUI</strong>: <a href="http://master02:50070" target="_blank" rel="noopener">http://master02:50070</a><br><strong>YARN webUI</strong>: <a href="http://master01:8088" target="_blank" rel="noopener">http://master01:8088</a></p><p><strong>测试 YARN</strong>：</p><blockquote><p>hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount /in/README.txt /out</p></blockquote><hr><p><strong>后记</strong>：写完博客，天也快亮了。日拱一卒，功不唐捐。加油！！</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-Linux-基础配置&quot;&gt;&lt;a href=&quot;#1-Linux-基础配置&quot; class=&quot;headerlink&quot; title=&quot;1. Linux 基础配置&quot;&gt;&lt;/a&gt;1. Linux 基础配置&lt;/h1&gt;&lt;h2 id=&quot;1-1-设置静态IP：&quot;&gt;&lt;a href=&quot;#1-1-设置静态IP：&quot; class=&quot;headerlink&quot; title=&quot;1.1 设置静态IP：&quot;&gt;&lt;/a&gt;1.1 设置静态IP：&lt;/h2&gt;&lt;p&gt;宿主机配置：&lt;br&gt;VMware: NAT模式，不使用DHCP&lt;br&gt;VMnet8: IPv4使用固定ip，子网掩码&lt;/p&gt;
    
    </summary>
    
    
      <category term="笔记" scheme="https://miracle-xing.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Hadoop" scheme="https://miracle-xing.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Spark 概念及应用程序架构</title>
    <link href="https://miracle-xing.github.io/2019/07/22/Spark-%E6%A6%82%E5%BF%B5%E5%8F%8A%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E6%9E%B6%E6%9E%84/"/>
    <id>https://miracle-xing.github.io/2019/07/22/Spark-概念及应用程序架构/</id>
    <published>2019-07-21T16:47:55.000Z</published>
    <updated>2019-07-23T18:28:11.467Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><ol><li><p>Spark是计算框架，不是存储框架。类似Hadoop中的MR</p></li><li><p>Spark是分布式的内存计算框架，Spark在计算的时候，内存不够用，数据会写到磁盘。</p><a id="more"></a></li><li><p>Spark和Hadoop没有必然联系，两者是独立的。</p></li><li><p>Spark可以读取HDFS / fileSystem / DB / Kafka / Flume上的数据，可以把数据写到HDFS / fileSystem / DB / Kafka / Flume中。</p></li><li><p>Spark可以使用YARN做资源调度管理器 Spark on YARN。</p></li><li><p><strong>数据不动代码动</strong>。</p></li><li><p>Spark架构：Master Slave架构，主从架构，一主多从。</p></li><li><p>主从架构的突出问题是 <strong>单点故障</strong>，HA（高可用）架构就是为了解决单点故障，心跳消息。</p></li><li><p>主主架构：Flume，Kafka</p></li><li><p>数据本地性：计算时从最近的节点读取数据。</p></li><li><p>粗粒度、细粒度<br>指的是资源分配方式。<br>粗粒度：应用启动，资源就分配给你，你用不用都是你的。<br>细粒度：不提前分配资源，你需要的时候再给你。</p></li><li><p>Spark两种算子 <strong>Transformation</strong> 和 <strong>Action</strong><br>Transformation算子返回值是 RDD，Action算子返回值是计算结果，不是RDD。</p></li><li><p>Spark 有四种部署方式：Standalone / Spark on YARN / Apache Mesos / Kubernetes</p></li><li><p>Spark Shell 是Spark提供的本地交互式脚本，默认启动时Local模式，使用Scala语言。</p></li></ol><p><strong>Scala 一行代码实现 wordcount</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">textFile.flatMap(line=&gt;line.split(&quot; &quot;)).map(word=&gt;(word,1)).reduceByKey(_+_).sortBy(_._2,false).collect().foreach(println)</span><br></pre></td></tr></table></figure><hr><h1 id="Spark-应用程序架构"><a href="#Spark-应用程序架构" class="headerlink" title="Spark 应用程序架构"></a>Spark 应用程序架构</h1><ol><li><p>Spark 应用程序组件：driver, the master, the cluster manager 和运行在worker节点的executor(s)<br><img src="/images/2019/07/22/42616bf0-abd7-11e9-87f2-3dee39091945.png" alt="Spark 应用程序架构.png"><br>所有Spark组件，包括 driver, master 和 executor进程，都在JVM中运行。使用Scala编写的Spark程序编译为Java字节码在JVM上运行。</p></li><li><p>区分Spark运行时应用程序组件 和运行它们的位置和节点类型是很重要的。使用不同的部署模式，这些组件可能运行在不同的位置，所以不要以物理节点或实例的形式考虑这些组件。</p></li></ol><h2 id="1-Spark-Driver"><a href="#1-Spark-Driver" class="headerlink" title="1.  Spark Driver"></a>1.  Spark Driver</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;Spark 应用程序由一个 driver 进程（驱动程序）和一组 executor 进程组成。Spark 应用程序的生命周期从 Spark Driver 程序开始（和结束）。driver 进程负责运行你的 main 函数，此进程位于集群中的一个 节点上。主要负责三件事：</p><ol><li>维护有关 Spark 应用程序的信息；</li><li>响应用户的程序或输入；</li><li>分配和调度 executor 的 task 和资源。</li></ol><p>executors 进程实际执行 driver 分配给他们的工作。这意味着每个 executor 主要负责两件事：</p><ol><li>执行由驱动程序分配给它的代码。</li><li>将执行器 executor 的计算状态报告给 driver 节点。</li></ol><h3 id="1-1-SparkContext"><a href="#1-1-SparkContext" class="headerlink" title="1.1 SparkContext"></a>1.1 SparkContext</h3><p>Spark Driver 程序负责创建 SparkContext。 SparkContext 在 Spark Shell 中对应的变量名为 sc，用于连接 Spark 集群，是与 Spark 集群交互的入口。SparkContext 在 Spark 应用程序（包括 Spark Shell）的开始实例化，并用于整个程序。</p><h3 id="1-2-应用程序执行计划"><a href="#1-2-应用程序执行计划" class="headerlink" title="1.2 应用程序执行计划"></a>1.2 应用程序执行计划</h3><p>Driver 程序的主要功能之一是规划应用程序的执行。驱动程序接受所有请求的 transformation 和 action 操作，并创建一个有向无环图（DAG）。<br><strong>注</strong>：DAG  是计算机科学中常用的表示数据流及其依赖关系的数学结构。DAGs 包含节点和边，节点表示执行计划中的步骤。DAG中的边以定向的方式将一个节点连接到另一个顶点，这样就不会出现 循环引用。</p><p>DAG 由 task 和 stages 组成。task 是 Spark 程序中可调度工作的最小单位。stage是一组可以一起运行的task。多个stage之间是相互依存的 。shuffle是划分stage的依据。</p><p>在进程调度意义上，DAGs 并不是 Spark 独有的。例如，它们被用于其他大数据生态系统项目，如 Tez、Drill 和 Presto 的任务调度。DAGs 是 Spark 的基础！！！</p><h3 id="1-3-应用程序的调度"><a href="#1-3-应用程序的调度" class="headerlink" title="1.3 应用程序的调度"></a>1.3 应用程序的调度</h3><p>driver 程序还协调 DAG 中定义的 stage 和 task 的运行。在调度和运行 task 时涉及的主要driver 程序活动包括：</p><ul><li>跟踪可用资源以执行 task</li><li>调度任务，以便在可能的情况下 “接近”数据运行——数据本地性</li><li>协调数据在 stages 之间的移动</li></ul><h3 id="1-4-其他功能"><a href="#1-4-其他功能" class="headerlink" title="1.4 其他功能"></a>1.4 其他功能</h3><p>除了计划和编排 Spark 程序的执行之外，驱动程序还负责从应用程序返回结果。<br>driver 程序在4040端口上自动创建了应用程序 UI。如果在同一个主机上启动后续应用程序，则会为应用程序 UI 使用连续的端口（例如 4041, 4042 等）。</p><h2 id="2-Executor-和-worker"><a href="#2-Executor-和-worker" class="headerlink" title="2. Executor 和 worker"></a>2. Executor 和 worker</h2><p>Spark executor 是运行来自 Spark DAG 的task 进程。executor 在 Spark 集群中的worker 节点上获取 CPU和内存等计算资源。executor 专用于特定的 Spark 应用程序，并在 应用程序完成时终止。在 Spark 程序中，Spark executor 可以运行成百上千个 task。</p><p>通常情况下，worker 节点（承载 executor 进程）具有有限或固定数量的 executor。因此，一个 spark 集群（包括一定数量的服务器节点）具有有限数量的 executor，可以分配它们来运行 Spark 任务。</p><p>Spark executor 驻留在 JVM 中。executor 的 JVM 分配了一个堆内存，这是一个用于存储和管理对象的专用内存空间。堆内存的大小由 spark 配置文件 spark-default.xml 中的 spark.executor.memory 属性确定，或者 由提交应用程序时 spark-submit 的参数 –executor-memroy  确定。</p><p>worker 和 executor 只知道分配给他们的 task，而 driver 程序负责理解组成应用程序的完整 task 集合它们各自的依赖关系。</p><h2 id="3-Master-和-Cluster-Manager"><a href="#3-Master-和-Cluster-Manager" class="headerlink" title="3. Master 和 Cluster Manager"></a>3. Master 和 Cluster Manager</h2><p>Spark driver 程序计划并协调运行 Spark 应用程序所需的 task 集。task 本身在 executor 中运行，executor 驻留在 worker 节点上。</p><p>Master 和 Cluster Manager 是监控、分配、回收集群（Executor 运行的节点）资源的核心进程，Master 和 Cluster Manager 可以是各自独立的进程（Spark On YARN），也可以组合成一个进程（Standalone 运行模式）。</p><h3 id="3-1-Master"><a href="#3-1-Master" class="headerlink" title="3.1 Master"></a>3.1 Master</h3><p>Spark master 是用于请求集群中的资源并将这些资源提供给 Spark driver 程序的进程。在两种部署模式中，master 节点都与 worker 节点或slave 节点协商资源或容器，并跟踪 它们的状态并监视它们的进展。</p><p>Spark master 进程在 master 进程所在主机上的端口 8080 上，提供 web 用户界面。</p><p><strong>注</strong>：要区分 driver 进程和 master 进程在 Spark 程序运行时的作用。master 只是请求 资源，并使这些资源在应用程序的生命周期内对驱动程序可用。尽管 master 监控这些资源的状态和健康状况，但是它不参与应用程序的执行以及 task 和 stage 的协调。</p><h3 id="3-2-Cluster-Manager（集群管理器）"><a href="#3-2-Cluster-Manager（集群管理器）" class="headerlink" title="3.2 Cluster Manager（集群管理器）"></a>3.2 Cluster Manager（集群管理器）</h3><p>Cluster Manager 进程负责监控分配给 worker 节点上的资源，这些资源是 master 进程请求分配的。然后，master 以 Executor 的形式将这些集群资源提供给 driver 程序。如前所述，Cluster Manager可以独立于 master 进程（Spark On YARN），也可以组合成一个进程（Standalone 运行模式）。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概念&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Spark是计算框架，不是存储框架。类似Hadoop中的MR&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Spark是分布式的内存计算框架，Spark在计算的时候，内存不够用，数据会写到磁盘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="笔记" scheme="https://miracle-xing.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Spark" scheme="https://miracle-xing.github.io/tags/Spark/"/>
    
  </entry>
  
</feed>

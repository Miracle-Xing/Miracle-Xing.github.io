<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>邢大强的blog</title>
  
  <subtitle>Stay hungry. Stay foolish.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://miracle-xing.github.io/"/>
  <updated>2019-07-22T20:58:22.800Z</updated>
  <id>https://miracle-xing.github.io/</id>
  
  <author>
    <name>Miracle</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hadoop 集群 HA 架构配置</title>
    <link href="https://miracle-xing.github.io/2019/07/23/Hadoop-%E9%9B%86%E7%BE%A4-HA-%E6%9E%B6%E6%9E%84%E9%85%8D%E7%BD%AE/"/>
    <id>https://miracle-xing.github.io/2019/07/23/Hadoop-集群-HA-架构配置/</id>
    <published>2019-07-22T20:42:04.000Z</published>
    <updated>2019-07-22T20:58:22.800Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Linux-基础配置"><a href="#1-Linux-基础配置" class="headerlink" title="1. Linux 基础配置"></a>1. Linux 基础配置</h1><h2 id="1-1-设置静态IP："><a href="#1-1-设置静态IP：" class="headerlink" title="1.1 设置静态IP："></a>1.1 设置静态IP：</h2><p>宿主机配置：<br>VMware: NAT模式，不使用DHCP<br>VMnet8: IPv4使用固定ip，子网掩码</p><a id="more"></a><h2 id="1-2-虚拟机配置："><a href="#1-2-虚拟机配置：" class="headerlink" title="1.2 虚拟机配置："></a>1.2 虚拟机配置：</h2><blockquote><p>vim /etc/sysconfig/network-scripts/ifcfg-ens33</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">TYPE=&quot;Ethernet&quot;</span><br><span class="line">PROXY_METHOD=&quot;none&quot;</span><br><span class="line">BROWSER_ONLY=&quot;no&quot;</span><br><span class="line">BOOTPROTO=&quot;static&quot;</span><br><span class="line">DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV4_FAILURE_FATAL=&quot;yes&quot;</span><br><span class="line">NAME=&quot;ens33&quot;</span><br><span class="line">UUID=&quot;a428bf24-b245-408a-88b6-d0934885c452&quot;</span><br><span class="line">DEVICE=&quot;ens33&quot;</span><br><span class="line">ONBOOT=&quot;yes&quot;</span><br><span class="line">IPADDR=&quot;192.168.12.130&quot;</span><br><span class="line">GATEWAY=&quot;192.168.12.2&quot;</span><br><span class="line">DNS1=&quot;192.168.12.2&quot;</span><br></pre></td></tr></table></figure><h2 id="1-3-修改主机名："><a href="#1-3-修改主机名：" class="headerlink" title="1.3 修改主机名："></a>1.3 修改主机名：</h2><blockquote><p>vim /etc/sysconfig/network</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NETWORKING=yes</span><br><span class="line">HOSTNAME=master01</span><br></pre></td></tr></table></figure><blockquote><p>vim /etc/hostname</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">master01</span><br></pre></td></tr></table></figure><h2 id="1-4-设置IP和主机名映射"><a href="#1-4-设置IP和主机名映射" class="headerlink" title="1.4 设置IP和主机名映射:"></a>1.4 设置IP和主机名映射:</h2><blockquote><p>vim /etc/hosts</p></blockquote><h2 id="1-5-设置免密登录"><a href="#1-5-设置免密登录" class="headerlink" title="1.5 设置免密登录"></a>1.5 设置免密登录</h2><blockquote><p>ssh-keygen</p></blockquote><p>生成公钥和私钥</p><blockquote><p>ssh-copy-id -i ~/.ssh/id_rsa.pub 目标机器用户名@目标机器名</p></blockquote><p>将公钥拷贝到目标机器</p><blockquote><p>ssh 目标机器用户名@目标机器名 </p></blockquote><h2 id="1-6-禁用selinux与防火墙"><a href="#1-6-禁用selinux与防火墙" class="headerlink" title="1.6 禁用selinux与防火墙"></a>1.6 禁用selinux与防火墙</h2><h3 id="1-6-1-selinux"><a href="#1-6-1-selinux" class="headerlink" title="1.6.1 selinux"></a>1.6.1 selinux</h3><blockquote><p>vim /etc/sysconfig/selinux</p></blockquote><p>将SELINUX改成disabled</p><h3 id="1-6-2-防火墙"><a href="#1-6-2-防火墙" class="headerlink" title="1.6.2 防火墙"></a>1.6.2 防火墙</h3><blockquote><p>systemctl stop firewalld<br>systemctl disable firewalld</p></blockquote><h2 id="1-7-卸载-Openjdk"><a href="#1-7-卸载-Openjdk" class="headerlink" title="1.7 卸载 Openjdk"></a>1.7 卸载 Openjdk</h2><p>查看jdk情况</p><blockquote><p>rpm -qa | grep java</p></blockquote><p>若使用openjdk则卸载</p><blockquote><p>rpm -e –nodeps ‘查询到的openjdk，多个文件用空格隔开’</p></blockquote><p>解压JDK<br>配置环境变量</p><h1 id="2-Hadoop-配置"><a href="#2-Hadoop-配置" class="headerlink" title="2. Hadoop 配置"></a>2. Hadoop 配置</h1><h2 id="2-1-集群规划："><a href="#2-1-集群规划：" class="headerlink" title="2.1 集群规划："></a>2.1 集群规划：</h2><p>有3台虚拟机，分别是 master01, master02, slave01, slave02, slave03。</p><table><thead><tr><th>节点</th><th>master01</th><th>master02</th><th>slave01</th><th>slave02</th><th>slave03</th></tr></thead><tbody><tr><td><strong>组件</strong></td><td>Namenode<br>DFSZKFailoverController<br>ResourceManager<br>Jobhistory</td><td>Namenode<br>DFSZKFailoverController</td><td>Datanode<br>NodeManager<br>JournalNode</td><td>Datanode<br>NodeManager<br>JournalNode</td><td>Datanode<br>NodeManager<br>JournalNode</td></tr></tbody></table><h2 id="2-2-解压-hadoop-并清理文档"><a href="#2-2-解压-hadoop-并清理文档" class="headerlink" title="2.2 解压 hadoop 并清理文档"></a>2.2 解压 hadoop 并清理文档</h2><blockquote><p>tar -zxvf hadoop-2.7.7.tar.gz -C /opt/modules/<br>mv hadoop-2.7.7/ hadoop277</p></blockquote><p>清理hadoop-2.5.0/share/doc</p><h2 id="2-3-指定-Java-路径"><a href="#2-3-指定-Java-路径" class="headerlink" title="2.3 指定 Java 路径"></a>2.3 指定 Java 路径</h2><p>文件：hadoop-env.sh / mapred-env.sh / yarn-env.sh</p><h2 id="2-4-HDFS-相关修改"><a href="#2-4-HDFS-相关修改" class="headerlink" title="2.4 HDFS 相关修改"></a>2.4 HDFS 相关修改</h2><h3 id="2-4-1-修改-core-site-xml"><a href="#2-4-1-修改-core-site-xml" class="headerlink" title="2.4.1 修改 core-site.xml"></a>2.4.1 修改 core-site.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hdfs://ns&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/opt/modules/hadoop277/data/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;master01:2181,master02:2181,slave01:2181,slave02:2181,slave03:2181&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="2-4-2-修改hdfs-site-xml"><a href="#2-4-2-修改hdfs-site-xml" class="headerlink" title="2.4.2 修改hdfs-site.xml"></a>2.4.2 修改hdfs-site.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;ns&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.ha.namenodes.ns&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;nn1,nn2&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.rpc-address.ns.nn1&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;master01:8020&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.rpc-address.ns.nn2&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;master02:8020&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.http-address.ns.nn1&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;master01:50070&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.http-address.ns.nn2&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;master02:50070&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">       &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;qjournal://slave01:8485;slave02:8485;slave03:8485/ns&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.client.failover.proxy.provider.ns&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;/opt/modules/hadoop277/data/dfs/jn&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;sshfence&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="2-4-3-修改slaves"><a href="#2-4-3-修改slaves" class="headerlink" title="2.4.3 修改slaves"></a>2.4.3 修改slaves</h3><p>从节点主机名</p><blockquote><p>slave01<br>slave02<br>slave03</p></blockquote><h2 id="2-5-MapReduce-与-YARN-修改"><a href="#2-5-MapReduce-与-YARN-修改" class="headerlink" title="2.5 MapReduce 与 YARN 修改"></a>2.5 MapReduce 与 YARN 修改</h2><h3 id="2-5-1-修改-mapred-site-xml"><a href="#2-5-1-修改-mapred-site-xml" class="headerlink" title="2.5.1 修改 mapred-site.xml"></a>2.5.1 修改 mapred-site.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master01:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master01:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="2-5-2-修改-yarn-site-xml"><a href="#2-5-2-修改-yarn-site-xml" class="headerlink" title="2.5.2 修改 yarn-site.xml"></a>2.5.2 修改 yarn-site.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">       &lt;!-- 开启RM高可靠 --&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;!-- 指定RM的cluster id --&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;RM_HA_ID&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;!-- 指定RM的名字 --&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;rm1,rm2&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;!-- 分别指定RM的地址 --&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;master01&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;master02&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;!-- 指定zk集群地址 --&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;master01:2181,master02:2181,slave01:2181,slave02:2181,slave03:2181&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p><strong>分发 hadoop 文件到其他节点</strong></p><h2 id="2-6-集群启动"><a href="#2-6-集群启动" class="headerlink" title="2.6 集群启动"></a>2.6 集群启动</h2><h3 id="2-6-1-启动-zookeeper，再启动-journalnode"><a href="#2-6-1-启动-zookeeper，再启动-journalnode" class="headerlink" title="2.6.1 启动 zookeeper，再启动 journalnode"></a>2.6.1 启动 zookeeper，再启动 journalnode</h3><p>slave01,  slave02, slave03</p><blockquote><p>sbin/hadoop-daemon.sh start journalnode</p></blockquote><h3 id="2-6-2-格式化-namenode"><a href="#2-6-2-格式化-namenode" class="headerlink" title="2.6.2 格式化 namenode"></a>2.6.2 格式化 namenode</h3><p>master01</p><blockquote><p>bin/hdfs namenode -format</p></blockquote><h3 id="2-6-3-同步元数据"><a href="#2-6-3-同步元数据" class="headerlink" title="2.6.3 同步元数据"></a>2.6.3 同步元数据</h3><p>master01 上启动 namenode </p><p>master02：</p><blockquote><p>bin/hdfs namenode -bootstrapStandby</p></blockquote><h3 id="2-6-4-初始化-ZKFC"><a href="#2-6-4-初始化-ZKFC" class="headerlink" title="2.6.4 初始化 ZKFC"></a>2.6.4 初始化 ZKFC</h3><p>master01</p><blockquote><p>bin/hdfs zkfc -formatZK<br>//zk下生成hadoop-ha目录表示成功</p></blockquote><h3 id="2-6-5-启动-HDFS-相关进程"><a href="#2-6-5-启动-HDFS-相关进程" class="headerlink" title="2.6.5 启动 HDFS 相关进程"></a>2.6.5 启动 HDFS 相关进程</h3><p>master01</p><blockquote><p>sbin/start-dfs.sh</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Starting namenodes on [master01 master02]</span><br><span class="line">master02: starting namenode, logging to /opt/modules/hadoop277/logs/hadoop-root-namenode-master02.out</span><br><span class="line">master01: starting namenode, logging to /opt/modules/hadoop277/logs/hadoop-root-namenode-master01.out</span><br><span class="line">slave01: starting datanode, logging to /opt/modules/hadoop277/logs/hadoop-root-datanode-slave01.out</span><br><span class="line">slave02: starting datanode, logging to /opt/modules/hadoop277/logs/hadoop-root-datanode-slave02.out</span><br><span class="line">slave03: starting datanode, logging to /opt/modules/hadoop277/logs/hadoop-root-datanode-slave03.out</span><br><span class="line">Starting journal nodes [slave01 slave02 slave03]</span><br><span class="line">slave02: starting journalnode, logging to /opt/modules/hadoop277/logs/hadoop-root-journalnode-slave02.out</span><br><span class="line">slave03: starting journalnode, logging to /opt/modules/hadoop277/logs/hadoop-root-journalnode-slave03.out</span><br><span class="line">slave01: starting journalnode, logging to /opt/modules/hadoop277/logs/hadoop-root-journalnode-slave01.out</span><br><span class="line">Starting ZK Failover Controllers on NN hosts [master01 master02]</span><br><span class="line">master02: starting zkfc, logging to /opt/modules/hadoop277/logs/hadoop-root-zkfc-master02.out</span><br><span class="line">master01: starting zkfc, logging to /opt/modules/hadoop277/logs/hadoop-root-zkfc-master01.out</span><br></pre></td></tr></table></figure><h3 id="2-6-6-查看进程"><a href="#2-6-6-查看进程" class="headerlink" title="2.6.6 查看进程"></a>2.6.6 查看进程</h3><p><strong>master01 进程</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">6392 Jps</span><br><span class="line">6026 DFSZKFailoverController</span><br><span class="line">2027 QuorumPeerMain</span><br><span class="line">5711 NameNode</span><br><span class="line">6191 ResourceManager</span><br></pre></td></tr></table></figure><p><strong>master02 进程</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">3635 NameNode</span><br><span class="line">3752 DFSZKFailoverController</span><br><span class="line">3884 Jps</span><br><span class="line">1805 QuorumPeerMain</span><br></pre></td></tr></table></figure><p><strong>slave01, slave02, slave03 进程</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">3507 DataNode</span><br><span class="line">2085 QuorumPeerMain</span><br><span class="line">3606 JournalNode</span><br><span class="line">3863 Jps</span><br><span class="line">3759 NodeManager</span><br></pre></td></tr></table></figure><h3 id="2-6-7-验证测试"><a href="#2-6-7-验证测试" class="headerlink" title="2.6.7 验证测试"></a>2.6.7 验证测试</h3><p><strong>HDFS webUI</strong>: <a href="http://master02:50070" target="_blank" rel="noopener">http://master02:50070</a><br><strong>YARN webUI</strong>: <a href="http://master01:8088" target="_blank" rel="noopener">http://master01:8088</a></p><p><strong>测试 YARN</strong>：</p><blockquote><p>hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount /in/README.txt /out</p></blockquote><hr><p><strong>后记</strong>：写完博客，天也快亮了。日拱一卒，功不唐捐。加油！！</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-Linux-基础配置&quot;&gt;&lt;a href=&quot;#1-Linux-基础配置&quot; class=&quot;headerlink&quot; title=&quot;1. Linux 基础配置&quot;&gt;&lt;/a&gt;1. Linux 基础配置&lt;/h1&gt;&lt;h2 id=&quot;1-1-设置静态IP：&quot;&gt;&lt;a href=&quot;#1-1-设置静态IP：&quot; class=&quot;headerlink&quot; title=&quot;1.1 设置静态IP：&quot;&gt;&lt;/a&gt;1.1 设置静态IP：&lt;/h2&gt;&lt;p&gt;宿主机配置：&lt;br&gt;VMware: NAT模式，不使用DHCP&lt;br&gt;VMnet8: IPv4使用固定ip，子网掩码&lt;/p&gt;
    
    </summary>
    
    
      <category term="笔记" scheme="https://miracle-xing.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Spark 概念及应用程序架构</title>
    <link href="https://miracle-xing.github.io/2019/07/22/Spark-%E6%A6%82%E5%BF%B5%E5%8F%8A%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E6%9E%B6%E6%9E%84/"/>
    <id>https://miracle-xing.github.io/2019/07/22/Spark-概念及应用程序架构/</id>
    <published>2019-07-21T16:47:55.000Z</published>
    <updated>2019-07-21T17:24:16.439Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><ol><li><p>Spark是计算框架，不是存储框架。类似Hadoop中的MR</p></li><li><p>Spark是分布式的内存计算框架，Spark在计算的时候，内存不够用，数据会写到磁盘。</p><a id="more"></a></li><li><p>Spark和Hadoop没有必然联系，两者是独立的。</p></li><li><p>Spark可以读取HDFS / fileSystem / DB / Kafka / Flume上的数据，可以把数据写到HDFS / fileSystem / DB / Kafka / Flume中。</p></li><li><p>Spark可以使用YARN做资源调度管理器 Spark on YARN。</p></li><li><p><strong>数据不动代码动</strong>。</p></li><li><p>Spark架构：Master Slave架构，主从架构，一主多从。</p></li><li><p>主从架构的突出问题是 <strong>单点故障</strong>，HA（高可用）架构就是为了解决单点故障，心跳消息。</p></li><li><p>主主架构：Flume，Kafka</p></li><li><p>数据本地性：计算时从最近的节点读取数据。</p></li><li><p>粗粒度、细粒度<br>指的是资源分配方式。<br>粗粒度：应用启动，资源就分配给你，你用不用都是你的。<br>细粒度：不提前分配资源，你需要的时候再给你。</p></li><li><p>Spark两种算子 <strong>Transformation</strong> 和 <strong>Action</strong><br>Transformation算子返回值是 RDD，Action算子返回值是计算结果，不是RDD。</p></li><li><p>Spark 有四种部署方式：Standalone / Spark on YARN / Apache Mesos / Kubernetes</p></li><li><p>Spark Shell 是Spark提供的本地交互式脚本，默认启动时Local模式，使用Scala语言。</p></li></ol><p><strong>Scala 一行代码实现 wordcount</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">textFile.flatMap(line=&gt;line.split(&quot; &quot;)).map(word=&gt;(word,1)).reduceByKey(_+_).sortBy(_._2,false).collect().foreach(println)</span><br></pre></td></tr></table></figure><hr><h1 id="Spark-应用程序架构"><a href="#Spark-应用程序架构" class="headerlink" title="Spark 应用程序架构"></a>Spark 应用程序架构</h1><ol><li><p>Spark 应用程序组件：driver, the master, the cluster manager 和运行在worker节点的executor(s)<br><img src="/images/2019/07/22/42616bf0-abd7-11e9-87f2-3dee39091945.png" alt="Spark 应用程序架构.png"><br>所有Spark组件，包括 driver, master 和 executor进程，都在JVM中运行。使用Scala编写的Spark程序编译为Java字节码在JVM上运行。</p></li><li><p>区分Spark运行时应用程序组件 和运行它们的位置和节点类型是很重要的。使用不同的部署模式，这些组件可能运行在不同的位置，所以不要以物理节点或实例的形式考虑这些组件。</p></li></ol><h2 id="1-Spark-Driver"><a href="#1-Spark-Driver" class="headerlink" title="1.  Spark Driver"></a>1.  Spark Driver</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;Spark 应用程序由一个 driver 进程（驱动程序）和一组 executor 进程组成。Spark 应用程序的生命周期从 Spark Driver 程序开始（和结束）。driver 进程负责运行你的 main 函数，此进程位于集群中的一个 节点上。主要负责三件事：</p><ol><li>维护有关 Spark 应用程序的信息；</li><li>响应用户的程序或输入；</li><li>分配和调度 executor 的 task 和资源。</li></ol><p>executors 进程实际执行 driver 分配给他们的工作。这意味着每个 executor 主要负责两件事：</p><ol><li>执行由驱动程序分配给它的代码。</li><li>将执行器 executor 的计算状态报告给 driver 节点。</li></ol><h3 id="1-1-SparkContext"><a href="#1-1-SparkContext" class="headerlink" title="1.1 SparkContext"></a>1.1 SparkContext</h3><p>Spark Driver 程序负责创建 SparkContext。 SparkContext 在 Spark Shell 中对应的变量名为 sc，用于连接 Spark 集群，是与 Spark 集群交互的入口。SparkContext 在 Spark 应用程序（包括 Spark Shell）的开始实例化，并用于整个程序。</p><h3 id="1-2-应用程序执行计划"><a href="#1-2-应用程序执行计划" class="headerlink" title="1.2 应用程序执行计划"></a>1.2 应用程序执行计划</h3><p>Driver 程序的主要功能之一是规划应用程序的执行。驱动程序接受所有请求的 transformation 和 action 操作，并创建一个有向无环图（DAG）。<br><strong>注</strong>：DAG  是计算机科学中常用的表示数据流及其依赖关系的数学结构。DAGs 包含节点和边，节点表示执行计划中的步骤。DAG中的边以定向的方式将一个节点连接到另一个顶点，这样就不会出现 循环引用。</p><p>DAG 由 task 和 stages 组成。task 是 Spark 程序中可调度工作的最小单位。stage是一组可以一起运行的task。多个stage之间是相互依存的 。shuffle是划分stage的依据。</p><p>在进程调度意义上，DAGs 并不是 Spark 独有的。例如，它们被用于其他大数据生态系统项目，如 Tez、Drill 和 Presto 的任务调度。DAGs 是 Spark 的基础！！！</p><h3 id="1-3-应用程序的调度"><a href="#1-3-应用程序的调度" class="headerlink" title="1.3 应用程序的调度"></a>1.3 应用程序的调度</h3><p>driver 程序还协调 DAG 中定义的 stage 和 task 的运行。在调度和运行 task 时涉及的主要driver 程序活动包括：</p><ul><li>跟踪可用资源以执行 task</li><li>调度任务，以便在可能的情况下 “接近”数据运行——数据本地性</li><li>协调数据在 stages 之间的移动</li></ul><h3 id="1-4-其他功能"><a href="#1-4-其他功能" class="headerlink" title="1.4 其他功能"></a>1.4 其他功能</h3><p>除了计划和编排 Spark 程序的执行之外，驱动程序还负责从应用程序返回结果。<br>driver 程序在4040端口上自动创建了应用程序 UI。如果在同一个主机上启动后续应用程序，则会为应用程序 UI 使用连续的端口（例如 4041, 4042 等）。</p><h2 id="2-Executor-和-worker"><a href="#2-Executor-和-worker" class="headerlink" title="2. Executor 和 worker"></a>2. Executor 和 worker</h2><p>Spark executor 是运行来自 Spark DAG 的task 进程。executor 在 Spark 集群中的worker 节点上获取 CPU和内存等计算资源。executor 专用于特定的 Spark 应用程序，并在 应用程序完成时终止。在 Spark 程序中，Spark executor 可以运行成百上千个 task。</p><p>通常情况下，worker 节点（承载 executor 进程）具有有限或固定数量的 executor。因此，一个 spark 集群（包括一定数量的服务器节点）具有有限数量的 executor，可以分配它们来运行 Spark 任务。</p><p>Spark executor 驻留在 JVM 中。executor 的 JVM 分配了一个堆内存，这是一个用于存储和管理对象的专用内存空间。堆内存的大小由 spark 配置文件 spark-default.xml 中的 spark.executor.memory 属性确定，或者 由提交应用程序时 spark-submit 的参数 –executor-memroy  确定。</p><p>worker 和 executor 只知道分配给他们的 task，而 driver 程序负责理解组成应用程序的完整 task 集合它们各自的依赖关系。</p><h2 id="3-Master-和-Cluster-Manager"><a href="#3-Master-和-Cluster-Manager" class="headerlink" title="3. Master 和 Cluster Manager"></a>3. Master 和 Cluster Manager</h2><p>Spark driver 程序计划并协调运行 Spark 应用程序所需的 task 集。task 本身在 executor 中运行，executor 驻留在 worker 节点上。</p><p>Master 和 Cluster Manager 是监控、分配、回收集群（Executor 运行的节点）资源的核心进程，Master 和 Cluster Manager 可以是各自独立的进程（Spark On YARN），也可以组合成一个进程（Standalone 运行模式）。</p><h3 id="3-1-Master"><a href="#3-1-Master" class="headerlink" title="3.1 Master"></a>3.1 Master</h3><p>Spark master 是用于请求集群中的资源并将这些资源提供给 Spark driver 程序的进程。在两种部署模式中，master 节点都与 worker 节点或slave 节点协商资源或容器，并跟踪 它们的状态并监视它们的进展。</p><p>Spark master 进程在 master 进程所在主机上的端口 8080 上，提供 web 用户界面。</p><p><strong>注</strong>：要区分 driver 进程和 master 进程在 Spark 程序运行时的作用。master 只是请求 资源，并使这些资源在应用程序的生命周期内对驱动程序可用。尽管 master 监控这些资源的状态和健康状况，但是它不参与应用程序的执行以及 task 和 stage 的协调。</p><h3 id="3-2-Cluster-Manager（集群管理器）"><a href="#3-2-Cluster-Manager（集群管理器）" class="headerlink" title="3.2 Cluster Manager（集群管理器）"></a>3.2 Cluster Manager（集群管理器）</h3><p>Cluster Manager 进程负责监控分配给 worker 节点上的资源，这些资源是 master 进程请求分配的。然后，master 以 Executor 的形式将这些集群资源提供给 driver 程序。如前所述，Cluster Manager可以独立于 master 进程（Spark On YARN），也可以组合成一个进程（Standalone 运行模式）。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概念&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Spark是计算框架，不是存储框架。类似Hadoop中的MR&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Spark是分布式的内存计算框架，Spark在计算的时候，内存不够用，数据会写到磁盘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="笔记" scheme="https://miracle-xing.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
</feed>

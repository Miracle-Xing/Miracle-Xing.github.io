<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>邢大强的blog</title>
  
  <subtitle>日拱一卒，功不唐捐。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://miracle-xing.github.io/"/>
  <updated>2019-08-28T13:09:09.704Z</updated>
  <id>https://miracle-xing.github.io/</id>
  
  <author>
    <name>Miracle</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>DataFrame 操作详解</title>
    <link href="https://miracle-xing.github.io/2019/08/28/DataFrame-%E6%93%8D%E4%BD%9C%E8%AF%A6%E8%A7%A3/"/>
    <id>https://miracle-xing.github.io/2019/08/28/DataFrame-操作详解/</id>
    <published>2019-08-28T02:33:25.000Z</published>
    <updated>2019-08-28T13:09:09.704Z</updated>
    
    <content type="html"><![CDATA[<p>DataFrame 和 Dataset 是（分布式的）类似于表的集合，具有定义好的行和列。每个列必须具有与所有其他列相同的行数（尽管您可以使用 null 来指定值的缺失），并且每个列都有类型信息，这些信息必须与集合中的每一行一致。这是因为内部存在一个 schema 的概念，其定义了分布式集合中存储的数据的类型。</p><a id="more"></a><h1 id="1-Schema"><a href="#1-Schema" class="headerlink" title="1. Schema"></a>1. Schema</h1><p>Schema 定义了 DataFrame 的列名和类型。您可以手动定义 Schema 模式或从数据源读取 Schema 模式（通常称为读模式）。Schema 包含列类型，用于声明<strong>什么列存储了什么类型的数据</strong>。</p><h1 id="2-Rows"><a href="#2-Rows" class="headerlink" title="2. Rows"></a>2. Rows</h1><p>一行（Row）只是表示数据的一条记录。DataFrame 中的每条数据记录必须是 Row 类型。我们可以从 SQL 、弹性分布式数据集（RDDs）、数据源或手动创建这些 Rows。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 返回 Row 对象数组</span><br><span class="line">Row[] collect = (Row[])spark.range(2).toDF().collect();</span><br><span class="line">for (Row r :</span><br><span class="line">        collect) &#123;</span><br><span class="line">    System.out.println(r);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="3-Column"><a href="#3-Column" class="headerlink" title="3. Column"></a>3. Column</h1><p>Column 表示一个简单的类型，如 Integer 或 String，复杂类型，如 Array 或 Map，或 Null。Spark 将跟踪所有这些类型的信息，并提供多种方式对 Column 进行转换。</p><h1 id="4-Spark-DataType"><a href="#4-Spark-DataType" class="headerlink" title="4. Spark DataType"></a>4. Spark DataType</h1><p>Spark 有大量的内部类型表示，这样就可以很容易地引用在特定语言（Java、Scala）中，与 Spark 类型相匹配的类型。import org.apache.spark.sql.types.DataTypes 类中。</p><p><strong>Scala type reference</strong>:</p><table><thead><tr><th>Data type</th><th>Value type in Scala</th><th>API to access or create a data type</th></tr></thead><tbody><tr><td>ByteType</td><td>Byte</td><td>ByteType</td></tr><tr><td>ShortType</td><td>Short</td><td>ShortType</td></tr><tr><td>IntegerType</td><td>Int</td><td>IntegerType</td></tr><tr><td>LongType</td><td>Long</td><td>LongType</td></tr><tr><td>FloatType</td><td>Float</td><td>FloatType</td></tr><tr><td>DoubleType</td><td>Double</td><td>DoubleType</td></tr><tr><td>DecimalType</td><td>java.math.BigDecimal</td><td>DecimalType</td></tr><tr><td>StringType</td><td>String</td><td>StringType</td></tr><tr><td>BinaryType</td><td>Array[Byte]</td><td>BinaryType</td></tr><tr><td>BooleanType</td><td>Boolean</td><td>BooleanType</td></tr><tr><td>TimestampType</td><td>java.sql.Timestamp</td><td>TimestampType</td></tr><tr><td>DateType</td><td>java.sql.Date</td><td>DateType</td></tr><tr><td>ArrayType</td><td>scala.collection.Seq</td><td>ArrayType(elementType, [containsNull]. Note: The default value of containsNull is true.</td></tr><tr><td>MapType</td><td>scala.collection.Map</td><td>MapType(keyType, valueType, [valueContainsNull). Note: The default value of valueContainsNull is true.</td></tr><tr><td>StructType</td><td>org.apache.spark.sql.Row</td><td>StructType(fields). Note: fields is an Array of StructFields. Also, fields with the same name are not allowed.</td></tr><tr><td>StructField</td><td>The value type in Scala of the data type of this field(for example, Int for a StructField  with the data type IntegerType)</td><td>StructField(name, dataType, [nullable]). Note: The default value of nullable is true.</td></tr></tbody></table><p><strong>Java type reference</strong>:</p><table><thead><tr><th>Data type</th><th>Value type in Java</th><th>API to access or create a data type</th></tr></thead><tbody><tr><td>ByteType</td><td>byte or Byte</td><td>DataTypes.ByteType</td></tr><tr><td>ShortType</td><td>short or Short</td><td>DataTypes.ShortType</td></tr><tr><td>IntegerType</td><td>int or Integer</td><td>DataTypes.IntegerType</td></tr><tr><td>LongType</td><td>long or Long</td><td>DataTypes.LongType</td></tr><tr><td>FloatType</td><td>float or Float</td><td>DataTypes.FloatType</td></tr><tr><td>DoubleType</td><td>double or Double</td><td>DataTypes.DoubleType</td></tr><tr><td>DecimalType</td><td>java.math.BigDecimal</td><td>DataTypes.createDecimalType(); DataTypes.createDecimalType(precision, scale).</td></tr><tr><td>StringType</td><td>String</td><td>DataTypes.StringType</td></tr><tr><td>BinaryType</td><td>byte[]</td><td>DataTypes.BinaryType</td></tr><tr><td>TimestmapType</td><td>java.sql.Timestamp</td><td>DataTypes.TimestampType</td></tr><tr><td>DateType</td><td>java.sql.Date</td><td>DataTypes.DateType</td></tr><tr><td>ArrayType</td><td>java.util.List</td><td>DataTypes.createArrayType(elementType). Note: The value of containsNull will be true; DataTypes.createArrayType(elementType, containsNull).</td></tr><tr><td>MapType</td><td>java.util.Map</td><td>DataTypes.createMapType(keyType, valueType). Note: The value of valueContainsNull will be true. DataTypes.createMapType(keyType, valueType, valueContainsNull)</td></tr><tr><td>StructType</td><td>org.apache.spark.sql.Row</td><td>DataTypes.createStructType(fields). Note: fields is a List or an array of StructFields. Also, two fields with the same name are not allowed.</td></tr><tr><td>StructField</td><td>The value type in Java of the data type of this field(for example, int for a StructField with the data typee IntegerType)</td><td>DataTypes.createStructField(name, dataType, nullable)</td></tr></tbody></table><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * 使用 List&lt;Row&gt;、Schema 创建 DataFrame</span><br><span class="line"> * 注意： 创建 StructField、StructType，要使用 DataTypes 的工厂方法</span><br><span class="line"> */</span><br><span class="line">List&lt;Row&gt; rows = new ArrayList&lt;&gt;();</span><br><span class="line">rows.add(RowFactory.create(&quot;张三&quot;, 20, &quot;北京&quot;));</span><br><span class="line">rows.add(RowFactory.create(&quot;李四&quot;, 22, &quot;上海&quot;));</span><br><span class="line">StructField[] fields = new StructField[]&#123;</span><br><span class="line">        DataTypes.createStructField(&quot;name&quot;, DataTypes.StringType, false),</span><br><span class="line">        DataTypes.createStructField(&quot;age&quot;, DataTypes.IntegerType, false),</span><br><span class="line">        DataTypes.createStructField(&quot;address&quot;, DataTypes.StringType, false)</span><br><span class="line">&#125;;</span><br><span class="line">StructType schema = DataTypes.createStructType(fields);</span><br><span class="line">Dataset&lt;Row&gt; listSchemaDF = spark.createDataFrame(rows, schema);</span><br><span class="line">listSchemaDF.show();</span><br><span class="line">listSchemaDF.printSchema();</span><br></pre></td></tr></table></figure><h1 id="5-DataFrame-基本操作"><a href="#5-DataFrame-基本操作" class="headerlink" title="5. DataFrame 基本操作"></a>5. DataFrame 基本操作</h1><p>从定义上看，一个 DataFrame 包括一系列的 records（记录，就像 table 中的 rows），这些行的类型是 Row 类型，包括一系列的 columns（就像表格中的列）。Schema 定义了每一列的列名和数据类型。DataFrame 的分区定义了 DataFrame 或 Dataset 在整个集群中的物理分布情况。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 创建 DataFrame</span><br><span class="line">Dataset&lt;Row&gt; df = spark.read().json(&quot;E:\\Java\\IntelliJ IDEA\\spark\\in\\2015-summary.json&quot;);</span><br><span class="line">// 查看 Schema</span><br><span class="line">df.printSchema();</span><br><span class="line">// 查看 schema</span><br><span class="line">System.out.println(df.schema());</span><br></pre></td></tr></table></figure><p>一个 Schema 就是一个 StructType，由多个 StructField 类型的 fields 组成，每个 field 包括一个列名称、一个列类型、一个布尔型的标识（是否可以有缺失值和 Null 值）。</p><h1 id="6-Columns-操作"><a href="#6-Columns-操作" class="headerlink" title="6. Columns 操作"></a>6. Columns 操作</h1><p>Spark 中的列类似于表格中的列。可以从 DataFrame 中选择列、操作列和删除列。对 Spark 来说，列是逻辑结构，它仅仅表示通过一个表达式按每条记录计算出的一个值。这意味着，要得到一个 column 列的真实值，我们需要一行 row 数据，为了得到一行数据，我们需要有一个 DataFrame。<strong>不能在 DataFrame 的上下文之外操作单个列</strong>。必须在 DataFrame 内使用 Spark 转换来操作列。</p><p>有许多不同的方法来构造和引用列，但最简单的两种方法是使用 <strong>col()</strong> 或 <strong>column()</strong>函数。要使用这些函数中的任何一个，需要传入一个列名：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// in Scala</span><br><span class="line">import org.apache.spark.sql.functions.&#123;col, column&#125;</span><br><span class="line">col(&quot;someColumnName&quot;)</span><br><span class="line">column(&quot;someColumnName&quot;)</span><br></pre></td></tr></table></figure><p>如前所述，这个列可能存在于我们的 DataFrames 中，也可能不存在。在将列名称与我们 Catalog 中维护的列进行比较之前，列不会被解析，即列是 <strong>unresolved</strong>。</p><p><strong>注意</strong>：<br>我们刚才提到的两种不同的方法引用列。Scala 有一些独特的语言特性，允许使用更多的简写方式来引用列。以下的语法糖执行完全相同的事情，即创建一个列，但不提供性能改进：<br><strong>$”myColumn”</strong><br><strong>‘myColumn</strong><br>$允许我们将一个字符串指定为一个特殊的字符串，该字符串应该引用一个表达式。标记(‘)是一种特殊的东西，称为符号；这是一个特定于 Scala 语言的，指向某个标识符。它们都执行相同的操作，是按名称引用列的简写方法。当您阅读不同对的人的 Spark 代码时，可能会看到前面提到的所有引用。</p><p><strong>表达式 expression</strong><br><strong>列是表达式</strong>。表达式是什么？表达式是在 DataFrame 中数据记录的一个或多个值上的一组转换。把它想象成一个函数，它将一个或多个列名作为输入，表达式会解析它们，为数据集中的每个记录返回一个单一值。</p><p>在最简单的情况下，expr(“someCol”)等价于 col(“someCol”)。<br><strong>列操作是表达式功能的一个子集</strong>。<br>expr(“someCol - 5”) 与执行 col(“someCol”) - 5，或甚至 expr(“someCol”) - 5 的转换相同。这是因为 Spark 将它们编译为一个逻辑树，逻辑树指定了操作的顺序。</p><p><strong>一些 DataFrames 操作列的示例</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// 创建 DataFrame</span><br><span class="line">Dataset&lt;Row&gt; df = spark.read().json(&quot;E:\\Java\\IntelliJ IDEA\\spark\\in\\2015-summary.json&quot;);</span><br><span class="line">// Dataset API</span><br><span class="line">df.select(&quot;DEST_COUNTRY_NAME&quot;).show(5);</span><br><span class="line">// SQL</span><br><span class="line">df.createOrReplaceTempView(&quot;flight&quot;);</span><br><span class="line">spark.sql(&quot;select DEST_COUNTRY_NAME from flight limit 5&quot;).show();</span><br></pre></td></tr></table></figure><p><strong>可以使用相同的查询样式选择多个列，只需在 select 方法调用中添加更多的列名字符串参数</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// Dataset API</span><br><span class="line">df.select(&quot;DEST_COUNTRY_NAME&quot;,&quot;ORIGIN_COUNTRY_NAME&quot;).show(5);</span><br><span class="line">// SQL</span><br><span class="line">df.createOrReplaceTempView(&quot;flight&quot;);</span><br><span class="line">spark.sql(&quot;select DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME from flight limit 5&quot;).show();</span><br></pre></td></tr></table></figure><p><strong>可以用许多不同的方式引用列，可以交替使用它们</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// in Scala</span><br><span class="line">import org.apache.spark.sql.functions.&#123;expr, col, column&#125;</span><br><span class="line">df.select(</span><br><span class="line">    df.col(&quot;DEST_COUNTRY_NAME&quot;),</span><br><span class="line">    col(&quot;DEST_COUNTRY_NAME&quot;),</span><br><span class="line">    column(&quot;DEST_COUNTRY_NAME&quot;),</span><br><span class="line">    &apos;DEST_COUNTRY_NAME,</span><br><span class="line">    $&quot;DEST_COUNTRY_NAME&quot;,</span><br><span class="line">    expr(&quot;DEST_COUNTRY_NAME&quot;))</span><br><span class="line">  .show(2)</span><br></pre></td></tr></table></figure><p><strong>一个常见的错误是混合使用列对象和列字符串。例如，下列代码将导致编译错误</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(col(&quot;DEST_COUNTRY_NAME&quot;), &quot;EST_COUNTRY_NAME&quot;)</span><br></pre></td></tr></table></figure><p><strong>expr 是我们可以使用的最灵活的引用。它可以引用一个简单的列或一个列字符串操作。<br>为了说明这一点，让我们更改列名，然后通过 AS 关键字来更改它</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// Dataset API</span><br><span class="line">df.select(functions.expr(&quot;DEST_COUNTRY_NAME as destination&quot;)).show(2);</span><br><span class="line">// SQL</span><br><span class="line">df.createOrReplaceTempView(&quot;flight&quot;);</span><br><span class="line">spark.sql(&quot;select DEST_COUNTRY_NAME as dest from flight limit 2&quot;).show();</span><br></pre></td></tr></table></figure><h1 id="7-字面常量转换为-Spark-类型（Literals）"><a href="#7-字面常量转换为-Spark-类型（Literals）" class="headerlink" title="7. 字面常量转换为 Spark 类型（Literals）"></a>7. 字面常量转换为 Spark 类型（Literals）</h1><p>有时，我们需要将显式字面常量值传递给 Spark，它只是一个值（而不是一个新列）。这可能是一个常数值或者我们以后需要比较的值。我们的方法是通过 Literals，将给定编程语言的字面值转换为 Spark 能够理解的值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// Dataset API</span><br><span class="line">df.select(functions.expr(&quot;*&quot;), functions.lit(1).as(&quot;One&quot;)).show();</span><br><span class="line">// SQL</span><br><span class="line">df.createOrReplaceTempView(&quot;flight&quot;);</span><br><span class="line">spark.sql(&quot;select *, 1 as One from flight&quot;).show();</span><br></pre></td></tr></table></figure><h1 id="8-添加列"><a href="#8-添加列" class="headerlink" title="8. 添加列"></a>8. 添加列</h1><p>将新列添加到 DataFrame 中，这是通过在 DataFrame 上使用 withColumn 方法来实现的，例如，让我们添加一个列，将数字 1 添加为一个列，列名为 numberOne：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//  Dataset API</span><br><span class="line">df.withColumn(&quot;numberOne&quot;, functions.lit(1)).show();</span><br><span class="line">// SQL</span><br><span class="line">df.createOrReplaceTempView(&quot;flight&quot;);</span><br><span class="line">spark.sql(&quot;select *, 1 as One from flight&quot;).show();</span><br></pre></td></tr></table></figure><p>另一个例子：设置一个布尔标志，表示源国与目标国相同：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.withColumn(&quot;ifSame&quot;, functions.expr(&quot;DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME&quot;)).filter(&quot;ifSame = true&quot;).show();</span><br></pre></td></tr></table></figure><p>注意：withColumn 函数由两个参数：<strong>列名</strong> 和 <strong>为 DataFrame 中的给定行创建值的表达式</strong>。</p><p>我们也可以这样 重命名列：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 复制某列但重新定义列名</span><br><span class="line">df.withColumn(&quot;destination&quot;, functions.expr(&quot;DEST_COUNTRY_NAME&quot;)).show();</span><br></pre></td></tr></table></figure><h1 id="9-重命名列"><a href="#9-重命名列" class="headerlink" title="9. 重命名列"></a>9. 重命名列</h1><p>虽然我们可以按照刚才描述的方式重命名列，但是另一种方法是使用 withcolumnrename 方法。这会将第一个参数中的字符串的名称重命名为第二个参数中的字符串：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 重命名列</span><br><span class="line">df.withColumnRenamed(&quot;DEST_COUNTRY_NAME&quot;,&quot;dest&quot;).show();</span><br></pre></td></tr></table></figure><h1 id="10-删除列"><a href="#10-删除列" class="headerlink" title="10. 删除列"></a>10. 删除列</h1><p>可能已经注意到我们可以通过使用 select 来实现这一点。然而，还有一个专门的方法叫做 drop：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 删除列</span><br><span class="line">df.drop(&quot;DEST_COUNTRY_NAME&quot;).show();</span><br></pre></td></tr></table></figure><h1 id="11-更改列类型"><a href="#11-更改列类型" class="headerlink" title="11. 更改列类型"></a>11. 更改列类型</h1><p>有时，可能性需要列从一种类型转换为另一种类型，例如，如果有一组 StringType 应该是整数。我们可以将列从一种类型转换为另一种类型，例如，让我们将 count 列从整数转换为类型 Long：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 更改列类型</span><br><span class="line">df.withColumn(&quot;count&quot;, functions.col(&quot;count&quot;).cast(&quot;int&quot;)).printSchema();</span><br><span class="line"></span><br><span class="line">// SQL</span><br><span class="line">df.createOrReplaceTempView(&quot;flight&quot;);</span><br><span class="line">spark.sql(&quot;select *, cast(count as int) as count2 from flight&quot;).printSchema();</span><br></pre></td></tr></table></figure><h1 id="12-过滤行"><a href="#12-过滤行" class="headerlink" title="12. 过滤行"></a>12. 过滤行</h1><p>为了过滤行，我们创建一个计算值为 true 或 false 的表达式。然后用一个等于 false 的表达式过滤掉这些行。使用 DataFrames 执行此操作的最常见方法是将表达式创建为字符串，或者使用一组列操作构建表达式。执行此操作有两种方法：您可以使用 where 或 filter，它们都将执行相同的操作，并在使用 DataFrames 时接受相同的参数类型。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// API</span><br><span class="line">df.filter(functions.col(&quot;count&quot;).$less(2)).show(2);</span><br><span class="line">df.where(&quot;count &lt; 2&quot;).show(2);</span><br><span class="line"></span><br><span class="line">// SQL</span><br><span class="line">df.createOrReplaceTempView(&quot;flight&quot;);</span><br><span class="line">spark.sql(&quot;select * from flight where count &lt; 2&quot;).show(2);</span><br></pre></td></tr></table></figure><p>你可能希望将多个过滤器放入相同的表达式中。尽管这是可能的，但它并不总是有用的，因为 Spark 会自动执行所有的过滤操作，而不考虑过滤器的排序。这意味着，如果你想指定多个过滤器，只需将它们按顺序连接起来，让 Spark 处理其余部分：</p><p>多重过滤：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// API</span><br><span class="line">df.where(&quot;count &lt; 2&quot;).where(&quot;DEST_COUNTRY_NAME != &apos;United States&apos;&quot;).show();</span><br><span class="line">// SQL</span><br><span class="line">df.createOrReplaceTempView(&quot;flight&quot;);</span><br><span class="line">spark.sql(&quot;select * from flight where count &lt; 2 and DEST_COUNTRY_NAME != &apos;United States&apos;&quot;).show();</span><br></pre></td></tr></table></figure><h1 id="13-行去重"><a href="#13-行去重" class="headerlink" title="13. 行去重"></a>13. 行去重</h1><p>一个常见的用例是在一个 DataFrame 中提取唯一的或不同的值。这些值可以在一个或多个列中。我们这样做的方法是在 DataFrame 上使用不同的方法，它允许我们对该 DataFrame 中的任何行进行删除重复行。例如，让我们在数据集中获取唯一的起源地。当然，这是一个转换，它将返回一个新的 DataFrame，只有唯一的行：</p><p>去除重复航线：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// API</span><br><span class="line">System.out.println(df.select(&quot;DEST_COUNTRY_NAME&quot;, &quot;ORIGIN_COUNTRY_NAME&quot;).distinct().count());</span><br><span class="line"></span><br><span class="line">// SQL</span><br><span class="line">df.createOrReplaceTempView(&quot;flight&quot;);</span><br><span class="line">spark.sql(&quot;select count(distinct(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME)) as count from flight&quot;).show();</span><br></pre></td></tr></table></figure><h1 id="14-DataFrame-union"><a href="#14-DataFrame-union" class="headerlink" title="14. DataFrame union"></a>14. DataFrame union</h1><p>DataFrame 是<strong>不可变</strong>的。这意味着用户不能向 DataFrame 追加，因为这会改变它。要附加到 DataFrame，必须将原始的 DataFrame 与新的 DataFrame 结合起来 。这只是连接了两个 DataFrames。对于 Union 2 DataFrames，<strong>必须确保他们具有相同的模式和列数（Schema）</strong>，否则，union 将会失败。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Person&gt; p1 = new ArrayList&lt;&gt;();</span><br><span class="line">p1.add(new Person(&quot;张三&quot;, 20, &quot;北京&quot;));</span><br><span class="line">p1.add(new Person(&quot;李四&quot;, 22, &quot;上海&quot;));</span><br><span class="line">Dataset&lt;Row&gt; df1 = spark.createDataFrame(p1, Person.class);</span><br><span class="line"></span><br><span class="line">List&lt;Person&gt; p2 = new ArrayList&lt;&gt;();</span><br><span class="line">p2.add(new Person(&quot;wangwu&quot;, 20, &quot;北京&quot;));</span><br><span class="line">p2.add(new Person(&quot;qianliu&quot;, 22, &quot;上海&quot;));</span><br><span class="line">Dataset&lt;Row&gt; df2 = spark.createDataFrame(p2, Person.class);</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; union = df1.union(df2);</span><br><span class="line">union.show();</span><br></pre></td></tr></table></figure><h1 id="15-行排序"><a href="#15-行排序" class="headerlink" title="15. 行排序"></a>15. 行排序</h1><p>在对 DataFrame 中的值进行排序时，我们总是希望对 DataFrame 顶部的最大或最小值进行排序。有两个相同的操作可以实现：sort 和 orderBy。它们接受列表达式，字符串以及多个列。默认 是按升序排序：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df.sort(&quot;count&quot;).show(5);</span><br><span class="line"></span><br><span class="line">// 二次排序</span><br><span class="line">df.orderBy(&quot;count&quot;, &quot;DEST_COUNTRY_NAME&quot;).show(5);</span><br><span class="line"></span><br><span class="line">// 如果要指定升序、降序，需要用asc 和 desc 函数</span><br><span class="line">df.orderBy(functions.desc(&quot;ORIGIN_COUNTRY_NAME&quot;),functions.desc(&quot;count&quot;)).show();</span><br></pre></td></tr></table></figure><p>出于优化的目的，有时建议在另一组转换之前对每个分区进行排序。可以使用 sortWithinPartitons 方法来执行以下操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.sortWithinPartitions(&quot;count&quot;).show();</span><br></pre></td></tr></table></figure><h1 id="16-Limit"><a href="#16-Limit" class="headerlink" title="16. Limit"></a>16. Limit</h1><p>通常，你可能想要限制从 DataFrame 中提取的内容；例如，您可能只想要一些 DataFrame 的前十位。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// API</span><br><span class="line">df.sortWithinPartitions(&quot;count&quot;).limit(10).show();</span><br><span class="line">// SQL</span><br><span class="line">df.sortWithinPartitions(&quot;count&quot;).createOrReplaceTempView(&quot;flight&quot;);</span><br><span class="line">spark.sql(&quot;select * from flight limit 10&quot;).show();</span><br></pre></td></tr></table></figure><h1 id="17-Repartition-和-Coalesce"><a href="#17-Repartition-和-Coalesce" class="headerlink" title="17. Repartition 和 Coalesce"></a>17. Repartition 和 Coalesce</h1><p>一个重要的优化方式是根据一些经常过滤的列对数据进行分区，它控制跨集群的数据的物理布局，包括分区计划和分区数量。</p><p><strong>Repartition</strong> 将导致数据的完全 shuffle，无论是否需要重新 shuffle。这意味着<strong>只有当将来的分区数目大于当前的分区数目时，或者当你希望通过一组列进行分区时</strong>，你才应该使用 <strong>Repartition</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 获取当前 分区数量</span><br><span class="line">System.out.println(df.rdd().getNumPartitions());</span><br><span class="line">// 重分区</span><br><span class="line">Dataset&lt;Row&gt; repartition = df.repartition(2);</span><br><span class="line">System.out.println(repartition.rdd().getNumPartitions());</span><br><span class="line"></span><br><span class="line">// 如果需要经常对某个列进行过滤，那么基于该列进行重新分区是值得的</span><br><span class="line">Dataset&lt;Row&gt; dest_country_name = df.repartition(5, functions.col(&quot;DEST_COUNTRY_NAME&quot;));</span><br></pre></td></tr></table></figure><p>另一方面，<strong>Coalesce</strong> 不会导致完全 shuffle，并尝试合并分区。<br>下面操作将根据目标国家的名称将你的数据转移到 5 个分区中，然后合并它们（没有完全shuffle）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 合并成 2 个分区</span><br><span class="line">Dataset&lt;Row&gt; coalesce = dest_country_name.coalesce(2);</span><br></pre></td></tr></table></figure><h1 id="18-收集数据到-driver"><a href="#18-收集数据到-driver" class="headerlink" title="18. 收集数据到 driver"></a>18. 收集数据到 driver</h1><ul><li>collect： 从整个 DataFrame 中获取所有数据</li><li>take: 选取 DataFrame 的前几行</li><li>show: 打印出几行数据</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.take(2);</span><br><span class="line">df.collect();</span><br><span class="line">// Whether truncate long strings. If true, strings more than 20 characters will</span><br><span class="line">// be truncated and all cells will be aligned right</span><br><span class="line">df.show(5, false);</span><br></pre></td></tr></table></figure><p>更多相关操作参考官网：<br><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DataFrame 和 Dataset 是（分布式的）类似于表的集合，具有定义好的行和列。每个列必须具有与所有其他列相同的行数（尽管您可以使用 null 来指定值的缺失），并且每个列都有类型信息，这些信息必须与集合中的每一行一致。这是因为内部存在一个 schema 的概念，其定义了分布式集合中存储的数据的类型。&lt;/p&gt;
    
    </summary>
    
    
      <category term="笔记" scheme="https://miracle-xing.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Spark" scheme="https://miracle-xing.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>SparkSQL 入门</title>
    <link href="https://miracle-xing.github.io/2019/08/27/SparkSQL-%E5%85%A5%E9%97%A8/"/>
    <id>https://miracle-xing.github.io/2019/08/27/SparkSQL-入门/</id>
    <published>2019-08-27T03:20:33.000Z</published>
    <updated>2019-08-28T01:49:27.441Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-概览"><a href="#1-概览" class="headerlink" title="1. 概览"></a>1. 概览</h1><h2 id="1-1-简述"><a href="#1-1-简述" class="headerlink" title="1.1 简述"></a>1.1 简述</h2><p>SparkSQL 是 Spark 计算框架的一个模块。与基础的 Spark RDD API 不同，SparkSQL 为 Spark 提供了更多的<strong>数据结构 schema</strong> 信息。在内部，SparkSQL 使用这些额外的数据结构信息做进一步的<strong>优化操作</strong>。与 SparkSQL 交互的方式有多种，包括<strong>SQL 语句交互</strong>、<strong>Dataset API 交互</strong>。当使用 SparkSQL 获取数据处理结果时，无论使用什么样的交互方式，无论什么样的语言编写的程序，其底层的执行引擎都是相同的。这就意味着，Spark 开发人员可以很容易在不同 API 之间来回切换，而不同担心性能方面的问题。</p><a id="more"></a><p><img src="/images/2019/08/27/6e0cbcb0-c878-11e9-a36a-35ce21c8b830.png" alt="spark框架.png"></p><h2 id="1-2-SparkSQL-的用法之一：SQL-语句交互"><a href="#1-2-SparkSQL-的用法之一：SQL-语句交互" class="headerlink" title="1.2 SparkSQL 的用法之一：SQL 语句交互"></a>1.2 SparkSQL 的用法之一：SQL 语句交互</h2><p>SparkSQL 的用途之一是执行 SQL 查询。SparkSQL 也可以从已有 Hive数据仓库中读取数据(Spark on Hive)。SparkSQL 语句的执行结果是一个 <strong>DataFrame或Dataset</strong> 对象。同时支持命令行执行 SQL 和 JDBC/ODBC 连接。</p><h2 id="1-3-SparkSQL-的用法之二：Dataset-API-交互"><a href="#1-3-SparkSQL-的用法之二：Dataset-API-交互" class="headerlink" title="1.3 SparkSQL 的用法之二：Dataset API 交互"></a>1.3 SparkSQL 的用法之二：Dataset API 交互</h2><p>Dataset 是一个<strong>分布式的数据集合</strong>。在 Spark 1.6 版本中，Dataset 作为一个新接口添加进来，兼具 RDD 的优点（强类型、使用强大的 lambda 函数的能力）和 SparkSQL 优化引擎的优势。Dataset 可以从 JVM 对象来构建，然后使用一系列转换函数（map、flatmap、filter等）进行计算。Dataset API 在 Scala 和 Java 语言中是支持的，Python 语言不支持 Dataset API。但是，由于 Python 的动态特性，Dataset API 的许多优势，Python 语言已经具备了。R 语言类似。</p><p>DataFrame(Dataset&lt;Row&gt;)是一个由<strong>命令列</strong>组成的 Dataset。<strong>概念上相当于关系型数据库中的一个表</strong>，但底层提供了更丰富的优化操作。DataFrames 可以从一系列广泛的数据来源中构建，如结构化数据文件，Hive 中的表、外部数据库、或者已有 RDD。</p><p>DataFrame API 在 Scala、Java、Python、R 语言中都支持。在 Scala API 中，DataFrame 是 Dataset[Row] 的类型别名。但是在 Java API 中，开发人员需要使用 Dataset&lt;Row&gt; 来表示一个 DataFrame。</p><h2 id="1-4-SparkSession"><a href="#1-4-SparkSession" class="headerlink" title="1.4 SparkSession"></a>1.4 SparkSession</h2><p>SparkSession 提供了与底层 Spark 功能交互的入口，允许使用 DataFrame 和 Dataset API 对 Spark 进行编程。最重要的是，<strong>它限制了概念的数量</strong>（SparkContext、SQLContext 和 HiveContext），并且构建了开发人员在与 Spark 交互时必须兼顾的结构。</p><p>启动 spark-shell 控制台时，SparkSession 被实例化为 Spark 变量，可以直接使用。<br><img src="/images/2019/08/27/8f63f9d0-c87a-11e9-a36a-35ce21c8b830.png" alt="sparkshell.png"></p><p>运行代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val myRange = spark.range(1000).toDF(&quot;number&quot;)</span><br></pre></td></tr></table></figure><p>我们创建了一个 DataFrame(Dataset&lt;Row&gt;)，其中一个列包含 1000 行，值从 0 到 999。这一系列数字代表一个分布式集合。</p><p><strong>注意</strong>：<br>DataFrame 的概念并不是 Spark 特有的。R 和 Python 都有类似的概念。然而，Python/R DataFrames（有一些列外）存在于一台机器上，而不是多台机器上。这限制了给定的 DataFrame 只能使用某一台特定机器上存在的资源 。但是，因为 Spark 具有 Python 和 R 的语言接口，很容易将 Pandas(Python) 的 DataFrames、R DataFrames 转换为 Spark DataFrames。</p><p>DataFrame 和 Dataset 具有和 RDD 相同的概念。</p><h2 id="1-5-Partition-分区"><a href="#1-5-Partition-分区" class="headerlink" title="1.5 Partition 分区"></a>1.5 Partition 分区</h2><p>为了使每个 executor 执行器并行执行任务，Spark 将数据分为 partition（分区）。每个分区是集群中的一个物理机器上的<strong>行集合</strong>。DataFrame 的分区表示了在执行过程中数据是如何在集群中物理分布的。</p><p>需要注意的是，对于 DataFrames 操作，<strong>大多数情况下不需要手动或单独操作分区</strong>，因为使用 DataFrame 的高级 transformation 操作，底层会做一些优化操作，然后转化为 RDD 进行计算。</p><h2 id="1-6-Transformation"><a href="#1-6-Transformation" class="headerlink" title="1.6 Transformation"></a>1.6 Transformation</h2><p>执行一个简单的转换，以在当前的 DataFrame 中找到所有偶数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 使用 Dataset API</span><br><span class="line">Dataset&lt;Row&gt; where = number.where(&quot;number % 2 = 0&quot;);</span><br><span class="line">// where.show();</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：这些操作没有输出。这是因为我们只声明了一个抽象转换（Transformation) where。Spark 将不会对转换进行操作，知道我们调用一个 Action 操作。</p><h2 id="1-7-延迟计算"><a href="#1-7-延迟计算" class="headerlink" title="1.7 延迟计算"></a>1.7 延迟计算</h2><p>延迟计算意味着 Spark 将等到最后一刻才执行一系列 Transformation。在 SparkSQL 中，不会在执行某个 Transformation 操作时立即修改数据，Spark 会构建一个应用于源数据的 Plan。直到最后 Action 时执行代码，Spark 将这个计划从原始的 DataFrame 转换为 Physical Plan，该计划将在整个集群中高效地运行，因为 Spark 可以从端到端优化整个数据流。</p><p>例如 DataFrame 的<strong>谓词下推</strong> pushdown 优化方式：如果我们构建一个大型的 Spark 作业，在最后指定一个过滤器 where，只需要从源数据中获取一行。最有效的执行方式从数据源过滤所需的单个记录。Spark 实际上是通过自动将过滤器下推来优化的。</p><h2 id="1-8-Action"><a href="#1-8-Action" class="headerlink" title="1.8 Action"></a>1.8 Action</h2><p>为了触发计算，需要运行一个 Action 操作。Action 操作使 Spark 通过执行一系列 Transformation 转换，得到计算结果。最简单的 Action 操作是 <strong>count</strong>，它给出了 DataFrame 中记录的总数（行数）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">System.out.println(where.count());</span><br></pre></td></tr></table></figure><p>输出：500</p><p>有三种类型的 Action：</p><ul><li>在控制台中查看数据的 Action</li><li>数据收集的 Action 操作</li><li>输出到第三方存储系统的 Action 操作</li></ul><p>在执行这个 count 操作时，启动了一个 Spark job，运行过滤器 where 转换（一个窄依赖转换），然后是一个聚合（一个宽依赖转换），它在每个分区基础上执行计数，然后是一个收集 Action，它将我们的结果返回到 driver 端。通过检查 Spark UI，可以看到所有这一切。</p><h2 id="1-9-注册为表或视图"><a href="#1-9-注册为表或视图" class="headerlink" title="1.9 注册为表或视图"></a>1.9 注册为表或视图</h2><p>可以通过一个简单的方法 将任何 DataFrame 转换为一个表或视图：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">number.createOrReplaceTempView(&quot;number&quot;);</span><br><span class="line">Dataset&lt;Row&gt; sql = spark.sql(&quot;select * from number where number % 2 =0&quot;);</span><br></pre></td></tr></table></figure><p>现在我们可以用 SQL 查询我们的数据了。使用 spark.sql(sqlString)，spark 是我们的 SparkSession 变量，返回一个新的 DataFrame。</p><p>Spark 中的 DataFrames（和 SQL ) 已经有大量可用的操作。您可以使用和导入数百个函数来帮助您更快地解决大数据问题。</p><h1 id="2-重要概念"><a href="#2-重要概念" class="headerlink" title="2. 重要概念"></a>2. 重要概念</h1><h2 id="2-1-DataFrame"><a href="#2-1-DataFrame" class="headerlink" title="2.1 DataFrame"></a>2.1 DataFrame</h2><ul><li>SparkSQL 从 Spark 1.3 开始引入了一个名为 DataFrame 的表格式数据抽象。</li><li>DataFrame 是用于处理结构化和半结构化的数据抽象。</li><li>DataFrame 利用其 <strong>Schema</strong> 以比原始 RDDs 更有效的方式存储数据。</li><li>DataFrame 利用 RDD 的不可变的、内存计算的、弹性的、分布式的和并行的特性，并对数据应用一个称为 Schema 的数据结构，允许 Spark 管理 Schema，以比 Java 序列化更有效的方式在集群节点之间传递数据。</li><li>与 RDD 不同，DataFrame 中的数据被组织到指定的 columns 中，就像关系数据库中的表一样。</li></ul><h2 id="2-2-Dataset"><a href="#2-2-Dataset" class="headerlink" title="2.2 Dataset"></a>2.2 Dataset</h2><ul><li>从 Spark 1.6 版本开始提供 Dataset API, Dataset API提供了：<ul><li>面向对象的编程风格</li><li>像 RDD API 一样的<strong>编译时类型安全，编译时异常，运行时异常</strong></li><li>利用 Schema 处理结构化数据的优势</li></ul></li><li>Dataset 是结构化数据集，数据集泛型可以是 Row(DataFrame)，也可以是特定的数据类型。</li><li>Java 和 Spark 在编译时将指导数据集中数据的类型。</li></ul><h2 id="2-3-DataFrame-和-Dataset"><a href="#2-3-DataFrame-和-Dataset" class="headerlink" title="2.3 DataFrame 和 Dataset"></a>2.3 DataFrame 和 Dataset</h2><ul><li>从 Spark 2.0 开始，DataFrame API 和 Dataset API 合并。</li><li>Dataset 提供了两个截然不同的 API 特性：strong typed API 和 untyped API。</li><li>可以将 DataFrame 看作是 Dataset 的 untyped 类型：Dataset&lt;Row&gt;，Row 是一个 untyped 的 JVM 对象。</li><li>Dataset 是强类型 JVM 对象的集合。</li></ul><h1 id="3-代码实战"><a href="#3-代码实战" class="headerlink" title="3. 代码实战"></a>3. 代码实战</h1><p>创建 DataFrame 方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// 集合创建 DataFrame</span><br><span class="line">List&lt;Person&gt; personList = new ArrayList&lt;&gt;();</span><br><span class="line">personList.add(new Person(&quot;张三&quot;, 20, &quot;深圳&quot;));</span><br><span class="line">personList.add(new Person(&quot;李四&quot;, 22, &quot;天津&quot;));</span><br><span class="line">Dataset&lt;Row&gt; dataFrame = spark.createDataFrame(personList, Person.class);</span><br><span class="line">dataFrame.printSchema();</span><br><span class="line">dataFrame.show();</span><br><span class="line"></span><br><span class="line">// RDD 创建 DataFrame</span><br><span class="line">JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());</span><br><span class="line">JavaRDD&lt;Person&gt; parallelize = jsc.parallelize(personList);</span><br><span class="line">Dataset&lt;Row&gt; dataFrame1 = spark.createDataFrame(parallelize, Person.class);</span><br><span class="line">dataFrame1.printSchema();</span><br><span class="line">dataFrame1.show();</span><br><span class="line"></span><br><span class="line">// Json/CSV 文件创建 DataFrame</span><br><span class="line">Dataset&lt;Row&gt; json = spark.read().json(&quot;E:\\Java\\IntelliJ IDEA\\spark\\in\\2015-summary.json&quot;);</span><br><span class="line">json.createOrReplaceTempView(&quot;summary&quot;);</span><br><span class="line">Dataset&lt;Row&gt; sql = spark.sql(&quot;select * from summary where count &gt; 50 order by count desc limit 10&quot;);</span><br><span class="line">sql.show();</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-概览&quot;&gt;&lt;a href=&quot;#1-概览&quot; class=&quot;headerlink&quot; title=&quot;1. 概览&quot;&gt;&lt;/a&gt;1. 概览&lt;/h1&gt;&lt;h2 id=&quot;1-1-简述&quot;&gt;&lt;a href=&quot;#1-1-简述&quot; class=&quot;headerlink&quot; title=&quot;1.1 简述&quot;&gt;&lt;/a&gt;1.1 简述&lt;/h2&gt;&lt;p&gt;SparkSQL 是 Spark 计算框架的一个模块。与基础的 Spark RDD API 不同，SparkSQL 为 Spark 提供了更多的&lt;strong&gt;数据结构 schema&lt;/strong&gt; 信息。在内部，SparkSQL 使用这些额外的数据结构信息做进一步的&lt;strong&gt;优化操作&lt;/strong&gt;。与 SparkSQL 交互的方式有多种，包括&lt;strong&gt;SQL 语句交互&lt;/strong&gt;、&lt;strong&gt;Dataset API 交互&lt;/strong&gt;。当使用 SparkSQL 获取数据处理结果时，无论使用什么样的交互方式，无论什么样的语言编写的程序，其底层的执行引擎都是相同的。这就意味着，Spark 开发人员可以很容易在不同 API 之间来回切换，而不同担心性能方面的问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="笔记" scheme="https://miracle-xing.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Spark" scheme="https://miracle-xing.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>自定义分区器 + 二次排序</title>
    <link href="https://miracle-xing.github.io/2019/08/18/%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E5%8C%BA%E5%99%A8-%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F/"/>
    <id>https://miracle-xing.github.io/2019/08/18/自定义分区器-二次排序/</id>
    <published>2019-08-18T15:13:09.000Z</published>
    <updated>2019-08-19T03:30:00.303Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-有一份数据，需求如下："><a href="#1-有一份数据，需求如下：" class="headerlink" title="1. 有一份数据，需求如下："></a>1. 有一份数据，需求如下：</h1><ol><li>根据 Department 字段对数据进行分组，然后根据 Annual Salary 排序，然后根据 first name 排序。</li><li>将每个部门的工资最高的前三名，存入 Mysql 数据库，数据库表包括如下字段(Department, Annual Salary, Firstname)。</li></ol><a id="more"></a><p>数据格式如下：</p><blockquote><p>数据字段说明：<br>First name: 名<br>Last name: 姓<br>Job Title: 职称<br>Department: 部门<br>Full or Part-time: 全职或兼职<br>Salary: 工资<br>Typical Hours: 小时工<br>Annual Salary: 年薪<br>Hourly Rate: 时薪</p></blockquote><h2 id="1-1-根据-Department-字段对数据进行分组，然后根据-Annual-Salary-排序，然后根据-first-name-排序。"><a href="#1-1-根据-Department-字段对数据进行分组，然后根据-Annual-Salary-排序，然后根据-first-name-排序。" class="headerlink" title="1.1 根据 Department 字段对数据进行分组，然后根据 Annual Salary 排序，然后根据 first name 排序。"></a>1.1 根据 Department 字段对数据进行分组，然后根据 Annual Salary 排序，然后根据 first name 排序。</h2><p><strong>思路</strong>：</p><ol><li>使用 repartitionAndSortWithinPartitions(partitioner: Partitioner) 算子，可以自定义分区器，根据部门分区，并对分区内的数据排序；</li><li>构造 EmployKey 实体类，属性有 Annual Salary, first name, Department，实现 Comparable 接口，重写 compareTo 方法，实现二次排序。</li></ol><p><strong>代码</strong>：</p><p><strong>EmployKey</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">package com.miracle.sparkproject;</span><br><span class="line"></span><br><span class="line">import java.io.Serializable;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * @program: sparkdemo</span><br><span class="line"> * @description:</span><br><span class="line"> * @author: Miracle</span><br><span class="line"> * @create: 2019-04-16 21:31</span><br><span class="line"> **/</span><br><span class="line">public class EmployKey implements Comparable&lt;EmployKey&gt;, Serializable &#123;</span><br><span class="line">    private static final long serialVersionUID = 6280745086974614533L;</span><br><span class="line">    private String name;</span><br><span class="line">    private String department;</span><br><span class="line">    private double salary;</span><br><span class="line"></span><br><span class="line">    public EmployKey(String name, String department, double salary) &#123;</span><br><span class="line">        this.name = name;</span><br><span class="line">        this.department = department;</span><br><span class="line">        this.salary = salary;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static long getSerialVersionUID() &#123;</span><br><span class="line">        return serialVersionUID;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String getName() &#123;</span><br><span class="line">        return name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setName(String name) &#123;</span><br><span class="line">        this.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String getDepartment() &#123;</span><br><span class="line">        return department;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setDepartment(String department) &#123;</span><br><span class="line">        this.department = department;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public double getSalary() &#123;</span><br><span class="line">        return salary;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setSalary(double salary) &#123;</span><br><span class="line">        this.salary = salary;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public int compareTo(EmployKey o) &#123;</span><br><span class="line">        //先按salary排序</span><br><span class="line">        int compare=(int)o.getSalary()-(int)this.getSalary();</span><br><span class="line">        //如果salary相同，再按name排</span><br><span class="line">        if(compare==0)&#123;</span><br><span class="line">            compare=this.getName().compareTo(o.getName());</span><br><span class="line">        &#125;</span><br><span class="line">        return compare;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>EmployValue</strong>：存储其余信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">package com.miracle.sparkproject;</span><br><span class="line"></span><br><span class="line">import java.io.Serializable;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * @program: sparkdemo</span><br><span class="line"> * @description:</span><br><span class="line"> * @author: Miracle</span><br><span class="line"> * @create: 2019-04-16 21:50</span><br><span class="line"> **/</span><br><span class="line">public class EmployValue implements Serializable &#123;</span><br><span class="line">    private static final long serialVersionUID = 5053727575607097704L;</span><br><span class="line">    private String jobTitle;</span><br><span class="line">    private String lastName;</span><br><span class="line"></span><br><span class="line">    public EmployValue(String jobTitle, String lastName) &#123;</span><br><span class="line">        this.jobTitle = jobTitle;</span><br><span class="line">        this.lastName = lastName;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static long getSerialVersionUID() &#123;</span><br><span class="line">        return serialVersionUID;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String getJobTitle() &#123;</span><br><span class="line">        return jobTitle;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setJobTitle(String jobTitle) &#123;</span><br><span class="line">        this.jobTitle = jobTitle;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String getLastName() &#123;</span><br><span class="line">        return lastName;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setLastName(String lastName) &#123;</span><br><span class="line">        this.lastName = lastName;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>自定义分区器 Sec_Partitioner</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">package com.miracle.review;</span><br><span class="line"></span><br><span class="line">import com.miracle.sparkproject.EmployKey;</span><br><span class="line">import org.apache.spark.Partitioner;</span><br><span class="line"></span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import java.util.concurrent.ConcurrentHashMap;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * @program: spark</span><br><span class="line"> * @description:</span><br><span class="line"> * @author: Miracle</span><br><span class="line"> * @create: 2019-08-19 02:02</span><br><span class="line"> **/</span><br><span class="line">public class Sec_Partitioner extends Partitioner &#123;</span><br><span class="line"></span><br><span class="line">    private Map&lt;String, Integer&gt; hashCodePartitionIndexMap = new ConcurrentHashMap();</span><br><span class="line"></span><br><span class="line">    private int numPartitions;</span><br><span class="line"></span><br><span class="line">    public int getNumPartitions() &#123;</span><br><span class="line">        return numPartitions;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setNumPartitions(int numPartitions) &#123;</span><br><span class="line">        this.numPartitions = numPartitions;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public Sec_Partitioner(List&lt;EmployKey&gt; keyList) &#123;</span><br><span class="line">        super();</span><br><span class="line">        int flag =0;</span><br><span class="line">        for (int i = 0; i &lt; keyList.size(); i++) &#123;</span><br><span class="line">            if (hashCodePartitionIndexMap.get(keyList.get(i).getDepartment()) == null)&#123;</span><br><span class="line">                hashCodePartitionIndexMap.put(keyList.get(i).getDepartment(), flag);</span><br><span class="line">                flag++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        setNumPartitions(hashCodePartitionIndexMap.keySet().size());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public int numPartitions() &#123;</span><br><span class="line">        return getNumPartitions();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public int getPartition(Object key) &#123;</span><br><span class="line">        EmployKey employKey = (EmployKey) key;</span><br><span class="line">        return hashCodePartitionIndexMap.get(employKey.getDepartment());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public boolean equals(Object obj) &#123;</span><br><span class="line">        if (this == obj)</span><br><span class="line">            return true;</span><br><span class="line">        if (obj == null)</span><br><span class="line">            return false;</span><br><span class="line">        if (getClass() != obj.getClass())</span><br><span class="line">            return false;</span><br><span class="line">        Sec_Partitioner other = (Sec_Partitioner) obj;</span><br><span class="line">        if (other.getNumPartitions() == this.getNumPartitions())</span><br><span class="line">            return true;</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>主类 SecondarySort</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">package com.miracle.review;</span><br><span class="line"></span><br><span class="line">import com.miracle.sparkproject.EmployKey;</span><br><span class="line">import com.miracle.sparkproject.EmployValue;</span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * @program: spark</span><br><span class="line"> * @description: 二次排序</span><br><span class="line"> * 需求1：根据Department字段对数据进行分组，然后根据Annual Salary排序，然后根据first name排序。</span><br><span class="line"> * 需求2：将每个部门的工资最高的前三名，存入Mysql数据库，数据库表包括如下字段(Department, Annual Salary, Firstname, )</span><br><span class="line"> * @author: Miracle</span><br><span class="line"> * @create: 2019-08-18 22:45</span><br><span class="line"> **/</span><br><span class="line">public class SecondarySort &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        SparkConf conf = new SparkConf().setAppName(&quot;SecondarySort&quot;).setMaster(&quot;local[*]&quot;);</span><br><span class="line">        JavaSparkContext jsc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;String&gt; javaRDD = jsc.textFile(&quot;in/employee_all.csv&quot;);</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;EmployKey, EmployValue&gt; pair = javaRDD.mapToPair(data -&gt; &#123;</span><br><span class="line">            String[] fields = data.split(&quot;,&quot;);</span><br><span class="line">            double salary = fields[7].length() &gt; 0 ? Double.parseDouble(fields[7]) : 0.00;</span><br><span class="line">            return new Tuple2&lt;&gt;(</span><br><span class="line">                    new EmployKey(fields[0], fields[3], salary),</span><br><span class="line">                    new EmployValue(fields[2], fields[1])</span><br><span class="line">            );</span><br><span class="line">        &#125;);</span><br><span class="line">        JavaPairRDD&lt;EmployKey, EmployValue&gt; employKeyEmployValueJavaPairRDD = pair.repartitionAndSortWithinPartitions(new Sec_Partitioner(pair.keys().collect()));</span><br><span class="line">        for (Tuple2&lt;EmployKey, EmployValue&gt; tuple2 : employKeyEmployValueJavaPairRDD.collect()) &#123;</span><br><span class="line">            System.out.println(tuple2._1.getDepartment() + &quot; &quot; + tuple2._1.getSalary() + &quot; &quot; + tuple2._1.getName() + &quot; &quot; + tuple2._2.getLastName() + &quot; &quot; + tuple2._2.getJobTitle());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        jsc.stop();</span><br><span class="line">        jsc.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="1-2-将每个部门的工资最高的前三名，存入-Mysql-数据库，数据库表包括如下字段-Department-Annual-Salary-Firstname"><a href="#1-2-将每个部门的工资最高的前三名，存入-Mysql-数据库，数据库表包括如下字段-Department-Annual-Salary-Firstname" class="headerlink" title="1.2 将每个部门的工资最高的前三名，存入 Mysql 数据库，数据库表包括如下字段(Department, Annual Salary, Firstname)"></a>1.2 将每个部门的工资最高的前三名，存入 Mysql 数据库，数据库表包括如下字段(Department, Annual Salary, Firstname)</h2><p><strong>代码</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">employKeyEmployValueJavaPairRDD.foreachPartition(partition -&gt; &#123;</span><br><span class="line">    int flag = 0;</span><br><span class="line"></span><br><span class="line">    // 存入 MySQL 数据库</span><br><span class="line">    Connection conn = JdbcUtils.getConnection();</span><br><span class="line">    PreparedStatement preparedStatement = null;</span><br><span class="line"></span><br><span class="line">    while (partition.hasNext() &amp;&amp; flag &lt; 3) &#123;</span><br><span class="line">        Tuple2&lt;EmployKey, EmployValue&gt; tuple2 = partition.next();</span><br><span class="line">        System.out.println(tuple2._1.getDepartment() + &quot; &quot; + tuple2._1.getSalary() + &quot; &quot; + tuple2._1.getName() + &quot; &quot; + tuple2._2.getLastName() + &quot; &quot; + tuple2._2.getJobTitle());</span><br><span class="line"></span><br><span class="line">        String sql = &quot;insert into employee (firstname,department,annual_salary) values(?,?,?)&quot;;</span><br><span class="line">        preparedStatement = conn.prepareStatement(sql);</span><br><span class="line">        preparedStatement.setString(1, tuple2._1.getName());</span><br><span class="line">        preparedStatement.setString(2, tuple2._1.getDepartment());</span><br><span class="line">        preparedStatement.setDouble(3, tuple2._1.getSalary());</span><br><span class="line">        preparedStatement.executeUpdate();</span><br><span class="line"></span><br><span class="line">        flag++;</span><br><span class="line">    &#125;</span><br><span class="line">    JdbcUtils.free(preparedStatement, conn);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p><strong>JdbcUtils 工具类</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">package com.miracle.utils;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import java.sql.*;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * @program: sparkdemo</span><br><span class="line"> * @description:</span><br><span class="line"> * @author: Miracle</span><br><span class="line"> * @create: 2019-03-31 13:24</span><br><span class="line"> **/</span><br><span class="line">public class JdbcUtils &#123;</span><br><span class="line"></span><br><span class="line">    private static String url=&quot;jdbc:mysql://master01:3306/test?characterEncoding=UTF-8&quot;;</span><br><span class="line">    private static String user=&quot;user&quot;;</span><br><span class="line">    private static String pwd=&quot;pwd&quot;;</span><br><span class="line"></span><br><span class="line">    private JdbcUtils()&#123;&#125;</span><br><span class="line"></span><br><span class="line">    // 1. 注册驱动 oracle.jdbc.driver.OracleDriver</span><br><span class="line">    static &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            Class.forName(&quot;com.mysql.jdbc.Driver&quot;);</span><br><span class="line">        &#125; catch (ClassNotFoundException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            System.out.println(&quot;数据库驱动加载失败！&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 2.  建立一个连接</span><br><span class="line">    public static Connection getConnection() throws SQLException&#123;</span><br><span class="line">        return DriverManager.getConnection(url,user,pwd);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 3. 关闭资源</span><br><span class="line">    public static void free(Statement statement,Connection connection)&#123;</span><br><span class="line">        if(statement!=null)&#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                statement.close();</span><br><span class="line">            &#125; catch (SQLException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;finally &#123;</span><br><span class="line">                if(connection!=null)&#123;</span><br><span class="line">                    try &#123;</span><br><span class="line">                        connection.close();</span><br><span class="line">                    &#125; catch (SQLException e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 3. 关闭资源2</span><br><span class="line">    public static void free2(ResultSet rs,Statement statement,Connection connection)&#123;</span><br><span class="line">        if(rs!=null)&#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                rs.close();</span><br><span class="line">            &#125; catch (SQLException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;finally &#123;</span><br><span class="line">                free(statement,connection);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // main</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            Connection connection=getConnection();</span><br><span class="line">            String sql=&quot;insert into textFile (filename) values (?)&quot;;</span><br><span class="line">            PreparedStatement preparedStatement;</span><br><span class="line">            preparedStatement=(PreparedStatement)connection.prepareStatement(sql);</span><br><span class="line">            preparedStatement.setString(1,&quot;test&quot;);</span><br><span class="line">            preparedStatement.executeUpdate();</span><br><span class="line">            free(preparedStatement,connection);</span><br><span class="line">            System.out.println(&quot;**************&quot;+connection);</span><br><span class="line">        &#125; catch (SQLException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-有一份数据，需求如下：&quot;&gt;&lt;a href=&quot;#1-有一份数据，需求如下：&quot; class=&quot;headerlink&quot; title=&quot;1. 有一份数据，需求如下：&quot;&gt;&lt;/a&gt;1. 有一份数据，需求如下：&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;根据 Department 字段对数据进行分组，然后根据 Annual Salary 排序，然后根据 first name 排序。&lt;/li&gt;
&lt;li&gt;将每个部门的工资最高的前三名，存入 Mysql 数据库，数据库表包括如下字段(Department, Annual Salary, Firstname)。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="笔记" scheme="https://miracle-xing.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Spark" scheme="https://miracle-xing.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 实践细节</title>
    <link href="https://miracle-xing.github.io/2019/08/14/Spark-%E5%AE%9E%E8%B7%B5%E7%BB%86%E8%8A%82/"/>
    <id>https://miracle-xing.github.io/2019/08/14/Spark-实践细节/</id>
    <published>2019-08-14T13:20:15.000Z</published>
    <updated>2019-08-14T15:02:26.584Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-使用-Yarn-Cluster-运行模式-代码无法控制-AppName"><a href="#1-使用-Yarn-Cluster-运行模式-代码无法控制-AppName" class="headerlink" title="1. 使用 Yarn Cluster 运行模式 代码无法控制 AppName"></a>1. 使用 Yarn Cluster 运行模式 代码无法控制 AppName</h1><p>今天使用 Yarn Cluster 运行模式提交 Spark 任务，在代码中设置了 AppName，但是在 WebUI 显示的【包名.类名】，Google 了之后才知道 Cluster 模式不会读取代码里配置，直接读取命令行配置，因此代码里面配置的 AppName 不起作用。</p><p><strong>代码</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val spark = SparkSession.builder()</span><br><span class="line">  .appName(&quot;Test Name&quot;)</span><br><span class="line">  .getOrCreate()</span><br><span class="line">val sc = spark.sparkContext</span><br></pre></td></tr></table></figure><a id="more"></a><p><strong>WebUI</strong>:</p><p><img src="/images/2019/08/14/4fbf22a0-be9d-11e9-b6e9-91273bc4af1a.png" alt="20190814_22h07_43.png"></p><p><strong>选择在 spark-submit 中参数设置 AppName</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --class com.miracle.sparkcore_review.Test \</span><br><span class="line">    --name TestName \</span><br><span class="line">    /opt/sparkAPP/spark-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><p><strong>WebUI</strong>:</p><p><img src="/images/2019/08/14/b8af8c90-bea3-11e9-b6e9-91273bc4af1a.png" alt="修改后.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-使用-Yarn-Cluster-运行模式-代码无法控制-AppName&quot;&gt;&lt;a href=&quot;#1-使用-Yarn-Cluster-运行模式-代码无法控制-AppName&quot; class=&quot;headerlink&quot; title=&quot;1. 使用 Yarn Cluster 运行模式 代码无法控制 AppName&quot;&gt;&lt;/a&gt;1. 使用 Yarn Cluster 运行模式 代码无法控制 AppName&lt;/h1&gt;&lt;p&gt;今天使用 Yarn Cluster 运行模式提交 Spark 任务，在代码中设置了 AppName，但是在 WebUI 显示的【包名.类名】，Google 了之后才知道 Cluster 模式不会读取代码里配置，直接读取命令行配置，因此代码里面配置的 AppName 不起作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;代码&lt;/strong&gt;：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;val spark = SparkSession.builder()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  .appName(&amp;quot;Test Name&amp;quot;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  .getOrCreate()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;val sc = spark.sparkContext&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="笔记" scheme="https://miracle-xing.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Spark" scheme="https://miracle-xing.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 调优</title>
    <link href="https://miracle-xing.github.io/2019/08/02/Spark-%E8%B0%83%E4%BC%98/"/>
    <id>https://miracle-xing.github.io/2019/08/02/Spark-调优/</id>
    <published>2019-08-02T04:49:17.000Z</published>
    <updated>2019-08-02T04:58:04.990Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Spark-调优"><a href="#1-Spark-调优" class="headerlink" title="1. Spark 调优"></a>1. Spark 调优</h1><p>参考链接：<a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations</a></p><p><strong>spark core 调优</strong>：Spark 应用程序的优化涉及到多个方面，包括 Spark 应用程序调优，资源调优，网络调优，硬盘调优等多个方面。本节课主要关注 Spark 应用程序调优和资源调优方面的内容，这也是 Spark 开发工程师的主要工作之一。<br>Spark应用程序调优，在编写 Spark 应用程序时主要考虑以下几个方面：</p><a id="more"></a><h2 id="1-1-避免重复创建-RDD，尽可能复用-RDD"><a href="#1-1-避免重复创建-RDD，尽可能复用-RDD" class="headerlink" title="1.1 避免重复创建 RDD，尽可能复用 RDD"></a>1.1 避免重复创建 RDD，尽可能复用 RDD</h2><p>开发 Spark 应用程序时，一般的步骤是：首先基于某个数据源（比如 HDFS 文件）创建一个初始的 RDD；接着对此 RDD 执行某个算子操作，得到下一个 RDD，以此类推，最后调用 action 操作得出我们想要的结果。在此过程中，多个 RDD 形成了<strong>RDD 的血缘关系链（lineage）</strong>。</p><p>对于同一份数据，只应该创建一个 RDD，不应该创建多个 RDD。如果基于一份数据创建了多个 RDD。Spark 作业会进行多次重复计算，增加了作业的性能开销。</p><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// 错误的做法</span><br><span class="line">val rdd1 = sc.textFile(&quot;in/README.md&quot;)</span><br><span class="line">rdd1.map(..)</span><br><span class="line">val rdd2 = sc.textFile(&quot;in/README.md&quot;)</span><br><span class="line">rdd2.reduce(..)</span><br><span class="line"></span><br><span class="line">// 正确的做法</span><br><span class="line">val rdd1 = sc.textFile(&quot;in/README.md&quot;)</span><br><span class="line">rdd1.map(..)</span><br><span class="line">rdd2.reduce(..)</span><br></pre></td></tr></table></figure><h2 id="1-2-对重复使用的-RDD-进行持久化"><a href="#1-2-对重复使用的-RDD-进行持久化" class="headerlink" title="1.2 对重复使用的 RDD 进行持久化"></a>1.2 对重复使用的 RDD 进行持久化</h2><p>参见 RDD 缓存持久化章节</p><h2 id="1-3-尽量避免使用会触发-shuffle-的算子"><a href="#1-3-尽量避免使用会触发-shuffle-的算子" class="headerlink" title="1.3 尽量避免使用会触发 shuffle 的算子"></a>1.3 尽量避免使用会触发 shuffle 的算子</h2><p>如果有可能，要尽量避免使用 shuffle 类算子。因为 Spark 作业运行过程中，最消耗性能的地方就是 shuffle 过程。shuffle 过程，就是将分布在集群中多个节点上的相同 key 的数据，拉取到同一个节点上，进行聚合或 join 等操作。比如 reduceByKey、join 等 算子，都会触发 shuffle。</p><p>shuffle，中文意思就是<strong>洗牌</strong>，洗牌是把扑克牌打乱，而 spark 的 shuffle 过程与洗牌的过程恰恰相反 ，spark shuffle 是将数据按 key 梳理好。</p><p>shuffle 过程中，各个节点上的相同 key 数据都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同 key。相同 key 都拉取到同一个节点进行聚合操作，此时有可能会因为某个节点上处理的 key 过多（数据倾斜），导致内存不够存放，进而溢写到磁盘文件中。因此在 shuffle 过程中，可能会发生大量的磁盘文件读写的 <strong>IO操作</strong>，以及<strong>数据的网络传输</strong>操作。磁盘 IO 和网络数据传输是 shuffle 性能较差的主要原因。</p><p>因此开发过程中，尽可能避免使用 reduceByKey、join、distinct、repartition等会触发 shuffle 的算子，尽量使用 map 类的非 shuffle 算子，尽量使用广播变量来避免 shuffle，优先选用 reduceByKey、aggregateByKey、combineByKey替换 groupByKey，应用 reduceByKey 算子内部使用了预聚合操作。</p><h2 id="1-4-使用高性能算子"><a href="#1-4-使用高性能算子" class="headerlink" title="1.4 使用高性能算子"></a>1.4 使用高性能算子</h2><h3 id="1-4-1-使用-mapPartitions-替代普通-map"><a href="#1-4-1-使用-mapPartitions-替代普通-map" class="headerlink" title="1.4.1 使用 mapPartitions 替代普通 map"></a>1.4.1 使用 mapPartitions 替代普通 map</h3><p>mapPartitions 类的算子，一次函数调用会处理一个 partition 所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用 mapPartitions 会出现 OOM（out of memory 内存溢出）的问题。因为每次函数调用就要处理一个 partition 中的所有数据，如果内存不够，会频繁 GC（垃圾回收），可能出现 OOM 异常。</p><h3 id="1-4-2-使用-foreachPartitions-替代-foreach"><a href="#1-4-2-使用-foreachPartitions-替代-foreach" class="headerlink" title="1.4.2 使用 foreachPartitions 替代 foreach"></a>1.4.2 使用 foreachPartitions 替代 foreach</h3><p>原理类似于 6.4.1。比如在 foreach 函数中，将 RDD 中所有数据写入 MySQL 数据库，就会一条数据一条数据地写，每次函数调用 可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用 foreachPartitions 算子一次性处理一个 partition 的数据，那么对于每个 partition，只要创建一个数据库连接即可，然后执行批量插入炒作，此时性能是比较高的。</p><h3 id="1-4-3-filter-算子之后使用-coalesce-算子"><a href="#1-4-3-filter-算子之后使用-coalesce-算子" class="headerlink" title="1.4.3 filter 算子之后使用 coalesce 算子"></a>1.4.3 filter 算子之后使用 coalesce 算子</h3><h3 id="1-4-4-使用-repartitionAndSortWithinPartitions-替代-repartition-与-sort-类操作"><a href="#1-4-4-使用-repartitionAndSortWithinPartitions-替代-repartition-与-sort-类操作" class="headerlink" title="1.4.4 使用 repartitionAndSortWithinPartitions 替代 repartition 与 sort 类操作"></a>1.4.4 使用 repartitionAndSortWithinPartitions 替代 repartition 与 sort 类操作</h3><p>见<strong>二次排序</strong>的例子</p><h2 id="1-5-将大变量广播出去"><a href="#1-5-将大变量广播出去" class="headerlink" title="1.5 将大变量广播出去"></a>1.5 将大变量广播出去</h2><p>在开发过程中，一旦出现<strong>在算子函数中使用外部变量的场景</strong>（尤其是较大的变量，注意：超过 4M需要修改默认配置），此时就应该使用 Spark 的广播（Broadcast）变量功能来提升性能。</p><p>在算子函数中使用外部变量时，默认情况下，Spark 会将该变量复制多个副本，通过网络传输到 task 中，此时每个 task 都有一个变量副本。如果变量本身比较大，那么大量的变量副本在网络中传输的性能开销，以及在各个节点的 Executor 中占用过多内存导致的频繁 GC 垃圾回收（miner GC -&gt; full GC 的时候，程序时停止运行的），会极大地影响性能。使用 Spark 广播功能，对该变量进行广播。广播后的变量，每个 Executor 只保留一份变量副本，而 Executor 中的 task 执行时共享该 Executor 中变量副本。这样，就可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对 Executor 内存的占用开销，降低 GC 的频率。</p><h2 id="1-6-使用-kryo-序列化方式来优化序列化性能"><a href="#1-6-使用-kryo-序列化方式来优化序列化性能" class="headerlink" title="1.6 使用 kryo 序列化方式来优化序列化性能"></a>1.6 使用 kryo 序列化方式来优化序列化性能</h2><p>在 Spark 中，主要有三个地方涉及到了序列化：</p><ol><li><p>在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输，此时需要对此变量序列化。</p></li><li><p>将自定义的类作为 RDD 的泛型类型时（比如二次排序例子中，JavaPairRDD&lt;Employee_Key，Employee_Value&gt;，Employee_key 和 Employee_Value 是自定义类型），所有自定义类的对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现 Serializable接口。</p></li><li><p>使用可序列化的持久化策略时（比如 MEMORY_ONLY_SER），Spark 会将 RDD 中的每个 partition 都序列化成一个大的字节数组。</p></li></ol><p>序列化大大减少了数据在内存、硬盘中占用的空间，减少了网络数据传输的开销，但是使用数据中，需要将数据进行反序列化，会消耗 CPU、延长程序执行时间，从而降低了 Spark 的性能，所以，序列化实际上利用了<strong>时间换空间</strong>的套路。</p><p>Spark 默认使 Java 序列化机制（ObjectOutputStream/ObjectInputStream API）来进行序列化和反序列化。Spark 同时支持使用 Kryo 序列化库，Kryo 序列化类库的性能比 Java序列化类库的性能高很多。官方介绍：性能高 10 倍左右。Spark 之所以默认没有使用 Kryo 作为序列化类库，是因为它不支持所有对象的序列化，同时 Kryo 需要用户在使用前注册需要序列化的类型，不够方便。<br><img src="/images/2019/07/25/9d155000-aea4-11e9-8908-0b6e2efa6f40.png" alt="注册需要.png"></p><p><strong>Kryo</strong> 相关的配置项：<br><img src="/images/2019/07/25/bebc45b0-aea4-11e9-8908-0b6e2efa6f40.png" alt="Kryo 相关.png"></p><p><strong>主要使用步骤</strong>：</p><ol><li><p>设置 spark 序列化使用的库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sparkconf.set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;);// 使用 Kryo 序列化库</span><br></pre></td></tr></table></figure></li><li><p>在该库中注册用户定义的类型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sparkconf.set(&quot;spark.kyro.registrator&quot;, MyKryoRegistrator.class.getName());// 在 Kryo 序列化库中注册自定义的类集合</span><br></pre></td></tr></table></figure></li><li><p>MyKryoRegistrator 类需要实现 KryoRegistrator 接口的 registerClasses 方法<br><img src="/images/2019/07/25/c7a80e20-aea4-11e9-8908-0b6e2efa6f40.png" alt="MyKryoRegistrator.png"></p></li></ol><p><img src="/images/2019/07/25/cda20dd0-aea4-11e9-8908-0b6e2efa6f40.png" alt="def register.png"></p><p>此时使用持久化方法，查看持久化数据的大小：<br>修改二次排序入口类，增加如下几行代码：<br><img src="/images/2019/07/25/da85c460-aea4-11e9-8908-0b6e2efa6f40.png" alt="修改二次排序.png"></p><p><img src="/images/2019/07/25/df456f50-aea4-11e9-8908-0b6e2efa6f40.png" alt="JavaRDD.png"></p><p>查看默认序列化和 Kryo 序列化所占空间的差别：</p><p><strong>Kryo 序列化</strong> ：<br><img src="/images/2019/07/25/e4228d50-aea4-11e9-8908-0b6e2efa6f40.png" alt="kryo 序列化.png"></p><p><strong>默认序列化</strong>：<br><img src="/images/2019/07/25/e82502c0-aea4-11e9-8908-0b6e2efa6f40.png" alt="默认序列化.png"></p><p><strong>进一步优化，启用 RDD 压缩</strong>：<br><img src="/images/2019/07/25/eceb6470-aea4-11e9-8908-0b6e2efa6f40.png" alt="进一步优化.png"></p><p><img src="/images/2019/07/25/f505ac10-aea4-11e9-8908-0b6e2efa6f40.png" alt="ShuffledRDD.png"></p><p><strong>注意</strong>：压缩机制虽然更进一步节省了空间，但是使用数据时，需要解压，耗费了 CPU。</p><h2 id="1-7-使用优化的数据结构"><a href="#1-7-使用优化的数据结构" class="headerlink" title="1.7 使用优化的数据结构"></a>1.7 使用优化的数据结构</h2><p>Java 中，有三种类型比较耗内存：</p><ol><li><p><strong>自定义对象</strong>，每个 Java 对象都有对象头、引用等额外的信息，因此比较占用内存空间。</p></li><li><p><strong>字符串</strong>，每个字符串内部都有一个字符数组以及长度等额外信息。</p></li><li><p><strong>集合类型</strong>，比如 HashMap、LinkedList 等，因为集合类型内部通常会使用一些内部类封装集合元素，比如 Map.Entry。</p></li></ol><p>Spark 官方建议，在 Spark 编码实现中，特别是对于算子函数中的代码，尽量使用字符串替代对象，使用原始类型（比如 Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而减低 GC 频率，提升性能。注意：前提是保证代码可运行，易维护。</p><h2 id="1-8-数据倾斜调优"><a href="#1-8-数据倾斜调优" class="headerlink" title="1.8 数据倾斜调优"></a>1.8 数据倾斜调优</h2><p><strong>大数据计算中一个最棘手的问题——数据倾斜</strong>。数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证 Spark 作业的性能。</p><p><strong>如何知道是否发生了数据倾斜</strong>？如果绝大多数 task 执行的都非常快，但个别 task 执行极慢。比如，总共有 100 个 task ，99 个都在 1 分钟之内执行完了，但是剩余一个 task 却要一两个小时。另外，数据倾斜严重的话，就发生 OOM 错误，导致这个 Application 失败。</p><p><strong>为什么会发生数据倾斜</strong>？使用了引起 shuffle 的算子，进行 shuffle 的时候，必须将各个节点上相同的 key 拉取到某个节点上的一个 task 来进行处理，比如按照 key 进行聚合或 join 等操作。此时如果某个 key 对应的数据量特别大的话，就会发生数据倾斜。比如大部分 key 对应 10 条数，但是个别 key 却对应了 100 万条数据，那么大部分 task 可能就只会分配到 10 条数据，1 秒钟就运行完了；但是个别 task 可能分配到了 100 万数据，要运行一两个小时。因此，整个 Spark 作业的运行进度是由运行时间最长的那个 task 决定的。</p><p><strong>如何定位发生数据倾斜的位置</strong>？shuffle 导致了数据倾斜，常见导致 shuffle 的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition 等。出现了数据倾斜，直接在代码中找这样的算子。</p><p><strong>如何定位是哪个算子</strong>？这样的算子会产生 shuffle ，shuffle 会划分 stage，所以，从 web UI 中查看发生数据倾斜的 task（即运行时间较长的 task）发生在哪个 stage中。无论是 spark standalone 模式还是 spark on yarn 模式的应用程序，都可以在 spark history server 中看到详细执行信息。也可以通过 yarn logs 命令查看详细的日志信息。<br><img src="/images/2019/07/25/ff7c5a40-aea4-11e9-8908-0b6e2efa6f40.png" alt="细的执行信息.png"></p><p>定位了数据倾斜发生在哪里之后，接着需要分析一下那个执行了 shuffle 操作并且导致了数据倾斜的 RDD/Hive 表，查看一下其中 key 的分布情况。这主要是为之后选择那一种技术方案提供依据。查看 key 分布的方式：</p><ol><li><p>如果是 Spark SQL 中的 group by、join 语句导致的数据倾斜，那么就查询一下 SQL 中使用的表的 key 分布情况。</p></li><li><p>如果是对 Spark RDD 执行 shuffle 算子导致的数据倾斜，那么可以在 Spark 作业中加入查看 key 分布的代码，比如 RDD.countByKey()。然后对统计出来的各个 key 出现的次数，collect、take 到客户端打印一下，就可以看到 key 的分布情况。</p></li></ol><p><strong>如何解决数据倾斜问题</strong>？<br><strong>方法一、过滤引起数据倾斜的 key</strong><br><strong>适用场景</strong>：如果发现导致倾斜的 key 就少数几个，而且对计算本身的影响并不大的话，适合使用这种方法。比如 90% 的 key 就对应 10 条数据，但是只有一个 key 对应了 100 万数据，从而导致了数据倾斜。</p><p><strong>实现思路</strong>：countByKey 确定数据量超多的某个 key，使用 filter 方法过滤。SparkSQL 中使用 where 方法过滤。</p><p>此方法实现简单 ，而且效果也很好，可以完全规避掉数据倾斜。但是，适用场景不多，大多数情况下，导致倾斜的 key 还是很多的，并不是只有少数几个。</p><p><strong>方法二、提高 shuffle 操作的并行度</strong><br><strong>适用场景</strong>：无法使用方法一规避，只有直面数据倾斜问题。</p><p><strong>实现思路</strong>：执行 RDD shuffle 算子时，给 shuffle 算子传入一个参数，比如 reduceByKey(100)，该参数就设置了这个 shuffle 算子执行时 shuffle read task 的数量。对于 Spark SQL 中的 shuffle 类语句，比如 groupByKey、join 等，需要设置一个参数，即 spark.sql.shuffle.partitions，该参数代表了 shuffle read task 的并行度，该值默认是 200，对于很多场景来说都有点过小。</p><p>此方法虽然实现简单，但是治标不治本。比如某个 key 对应的数据量有 100万，那么无论你的 task 数量增加到多少，这个对应着 100万数量的 key 肯定还是会分配到一个 task 中去处理，因此还是会发生数据倾斜的。</p><p><strong>方法三、对数据倾斜 key 使用随机数，实现两阶段聚合</strong><br><strong>适用场景</strong>：对 RDD 执行 reduceByKey 等聚合类 shuffle 算子或者在 Spark SQL 中使用 group by 语句进行分组聚合时，比较使用这种方法。</p><p><strong>实现思路</strong>：这个方案的核心实现思路就是<strong>进行两阶段聚合</strong>。第一阶段是<strong>局部聚合</strong>，先给每个 key 都打上一个随机数，比如 10 以内的随机数，此时原先一样的 key 就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行 reducyByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个 key 的随机数给去掉，就会变成(hello,2)(hello,2)，在此进行<strong>全局聚合</strong>操作，就可以得到最终结果了，比如（hello,4)。</p><p>如果<strong>聚合类的 shuffle 算子</strong>导致的数据倾斜，效果是非常不错的。如果是 join 类的 shuffle 算子，则不适合。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// 1. 加随机前缀</span><br><span class="line">val prefixRDD = wordPairRDD.map(t =&gt; &#123;</span><br><span class="line">  val random = new Random()</span><br><span class="line">  val prefix = random.nextInt(100)</span><br><span class="line">  (prefix + &quot;_&quot; + t._1, t._2)</span><br><span class="line">&#125;)</span><br><span class="line">// 2. 局部聚合</span><br><span class="line">val partialReduceRDD = prefixRDD.reduceByKey(_ + _)</span><br><span class="line">// 3. 去除前缀</span><br><span class="line">val noPrefixRDD = partialReduceRDD.map(t =&gt; (t._1.split(&quot;_&quot;)(1), t._2))</span><br><span class="line">// 4. 全局聚合</span><br><span class="line">val allReduceRDD = noPrefixRDD.reduceByKey(_ + _)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// 第一步，加随机前缀</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; randomPrefixRDD = javaPairRDD.mapToPair(t -&gt; &#123;</span><br><span class="line">    Random random = new Random();</span><br><span class="line">    int prefix = random.nextInt(100);</span><br><span class="line">    return new Tuple2&lt;&gt;(prefix +&quot;_&quot;+t._1, t._2);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">// 第二步，局部聚合</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD1 = randomPrefixRDD.reduceByKey((a, b) -&gt; (a + b));</span><br><span class="line"></span><br><span class="line">// 第三步，去除 prefix 前缀</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD2 = javaPairRDD1.mapToPair(t -&gt; (new Tuple2&lt;&gt;(t._1.split(&quot;_&quot;)[1], t._2)));</span><br><span class="line"></span><br><span class="line">// 第四步，全局聚合</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD3 = javaPairRDD2.reduceByKey((a, b) -&gt; (a + b));</span><br></pre></td></tr></table></figure><p><strong>方法四、将 hash shuffle join 转换为 map join</strong><br><strong>适用场景</strong>：在对 RDD 使用 join 类操作，或者是在 Spark SQL 中使用 join 语句时，而且 join 操作中的一个 RDD 或表的数据量比较小（比如几百兆），比较适合此方案。</p><p><strong>实现思路</strong>：不适用 join 算子进行连接操作，而使用 Broadcast 变量与 map 类算子实现 join 操作，进而完全规避掉 shuffle 类的操作，彻底避免数据倾斜的发生和出现。见广播变量课程中的 map join 代码。</p><p>对 join 操作导致的数据倾斜，效果非常好，因为不会发生 shuffle，也就不会发生数据倾斜。但是此方法适用场景较少，只适用于一个大 RDD 和一个小 RDD 的情况。</p><p><strong>方法五、使用 partitioner 优化 hash shuffle join</strong><br>为了对两个 RDD 中的数据进行 join，Spark 需要对两个 RDD 上的数据拉取同一个分区。Spark 中 join 的默认实现是 shuffled hash join：通过使用与第一个数据集相同的默认分区器对第二个数据集进行分区，从而确保每个分区上的数据将包含相同的 key，从而使两个数据集具有相同哈希值的键位于同一个分区中。虽然这种方法总是可以运行的，但是此种操作比较 耗费资源，因为它需要一次 shuffle。</p><p>如果两个 RDD 都有一个已知的分区器，则可以避免 shuffle，如果它们有相同分区器，则数据可能被本地合并；避免网络传输，因此，建议在 join 两个 RDD 之前，调用 partitionBy 方法，并且使用相同对的分区器。</p><p><strong>代码示例</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val partitioner = new HashPartitioner(10)</span><br><span class="line">agesRDD.partitionBy(partitioner)</span><br><span class="line">addressRDD.partitionBy(partitioner)</span><br></pre></td></tr></table></figure><p><strong>方法六、综合使用前面的方法</strong><br>如果只是处理较为简单的数据倾斜场景，使用上述方法中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方法组合起来使用。例如：针对出现了多个数据倾斜环节的 Spark 作业，可以先运用方法一和方法二，预处理一部分数据，并过滤一部分数据来缓解；其次可以对某些 shuffle 操作提升并行度，优化其性能；最后还可以针对不同的聚合或 join 操作，选择一种方法来优化其性能。需要对这些方法的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决数据倾斜问题。</p><h2 id="1-9-资源调优"><a href="#1-9-资源调优" class="headerlink" title="1.9 资源调优"></a>1.9 资源调优</h2><p>理解了 Spark 作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓 Spark 资源参数调优，其实主要就是对 Spark 运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升 Spark 作业的执行性能。</p><p>资源调优主要涉及到<strong>内存、CPU</strong>等资源的分配，参见资源分配相关内容。</p><h2 id="1-10-JVM-内存管理简介"><a href="#1-10-JVM-内存管理简介" class="headerlink" title="1.10 JVM 内存管理简介"></a>1.10 JVM 内存管理简介</h2><p>Java 的堆内存分为两个区域：新生代和老生代。新生代保存的是生命周期比较短的对象，老生代保存生命周期比较长的对象。新生代又分了三个区域（Eden, Survivor1, Survivor2）。</p><p>垃圾收集过程简要说明：当 Eden 已满时，Eden 上运行了一个 minor GC，并将 Eden 和 Survivor1 中存在的对象复制到 Survivor2。Survivor 将进行交换。如果一个足够老，或者 Survivor2 已满，则会移动到老年代。最后当老年代接近满的时候，会触发 <strong>full GC</strong>。</p><ol><li><p>通过收集垃圾回收信息，判断是否有太多的垃圾回收过程。假如 full gc 在一个 task 完成之前触发了好几次，那说明运行 task 的内存空间不足，需要加内存。</p></li><li><p>配置 JVM 相关信息的位置 spark-default.con<br>spark.executor.extraJavaOptions<br>spark.driver.extraJavaOptions</p></li></ol><p><strong>注意</strong>：driver 和 executor 的 JVM 堆内存的大小通过 driver-memory 和 executor.memory 配置项设置。<br><img src="/images/2019/07/25/0ad0e550-aea5-11e9-8908-0b6e2efa6f40.png" alt="配置项设置.png"></p><p>推荐一篇 JVM 文章：<a href="https://blog.csdn.net/kidoo1012/article/details/54599046" target="_blank" rel="noopener">https://blog.csdn.net/kidoo1012/article/details/54599046</a></p><h2 id="1-11-filter-算子导致数据倾斜"><a href="#1-11-filter-算子导致数据倾斜" class="headerlink" title="1.11 filter 算子导致数据倾斜"></a>1.11 filter 算子导致数据倾斜</h2><p>使用 filter 算子进行过滤操作，会在每个 partition 进行单独过滤，如果某个 partition 过滤后没有符合条件的元素，RDD 就为空，这样的结果是<strong>数据倾斜</strong>。<br><strong>解决方案</strong>：执行完 filter 之后进行 repartition 操作。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-Spark-调优&quot;&gt;&lt;a href=&quot;#1-Spark-调优&quot; class=&quot;headerlink&quot; title=&quot;1. Spark 调优&quot;&gt;&lt;/a&gt;1. Spark 调优&lt;/h1&gt;&lt;p&gt;参考链接：&lt;a href=&quot;https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;spark core 调优&lt;/strong&gt;：Spark 应用程序的优化涉及到多个方面，包括 Spark 应用程序调优，资源调优，网络调优，硬盘调优等多个方面。本节课主要关注 Spark 应用程序调优和资源调优方面的内容，这也是 Spark 开发工程师的主要工作之一。&lt;br&gt;Spark应用程序调优，在编写 Spark 应用程序时主要考虑以下几个方面：&lt;/p&gt;
    
    </summary>
    
    
      <category term="笔记" scheme="https://miracle-xing.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Spark" scheme="https://miracle-xing.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>SparkCore 知识点</title>
    <link href="https://miracle-xing.github.io/2019/07/25/SparkCore-%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    <id>https://miracle-xing.github.io/2019/07/25/SparkCore-知识点/</id>
    <published>2019-07-25T06:06:59.000Z</published>
    <updated>2019-08-19T09:08:50.626Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Spark-共享变量实战"><a href="#1-Spark-共享变量实战" class="headerlink" title="1. Spark 共享变量实战"></a>1. Spark 共享变量实战</h1><p>通常，Spark 程序计算的时候，我们传递的函数时在远程集群节点上执行的，在函数中使用的所有<strong>变量副本</strong>会传递到远程节点，计算任务使用<strong>变量副本</strong>进行计算。这些变量被复制到每台机器上，对远程机器上的变量的更新不会返回 driver 程序。</p><p>跨任务支持通用的读写共享变量将是低效的。但是，Spark 为两种常见的使用模式提供了两种有限 功能的共享变量：<strong>广播变量</strong> 和 <strong>累加器</strong>。</p><a id="more"></a><h2 id="1-1-广播变量-Broadcast-Variables"><a href="#1-1-广播变量-Broadcast-Variables" class="headerlink" title="1.1 广播变量 Broadcast Variables"></a>1.1 广播变量 Broadcast Variables</h2><p>广播变量允许 Spark 程序员将 <strong>只读变量</strong> 缓存在每台机器上，而不是将它的副本与 task 一起发送出去。例如，可以使用广播变量以高效的方式为每个 worker 节点提供一个大型输入数据集的副本。广播变量是只读的，对每个 worker 节点只需要传输一次。这样，就从每个 task 一份变量副本 ，变成了 <strong>一个 executor 一个变量副本</strong>，executor 中执行的 task 共用这个副本。如果有多个 worker 节点，<strong>各个 worker 上的 executor 中的变量副本并不都是来自 driver</strong>，因为 Spark 采用了高效地广播算法（TorrentBroadcast）来分配广播变量，以降低通信成本。</p><p><strong>代码片段</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">    val list1=List((&quot;zhangsan&quot;,20),(&quot;lisi&quot;,18),(&quot;wangwu&quot;,30))</span><br><span class="line">    val list2=List((&quot;zhangsan&quot;,&quot;Math&quot;),(&quot;lisi&quot;,&quot;English&quot;),(&quot;wangwu&quot;,&quot;Science&quot;))</span><br><span class="line"></span><br><span class="line">    val rdd1=sc.parallelize(list1)</span><br><span class="line">    val rdd2=sc.parallelize(list2)</span><br><span class="line"></span><br><span class="line">    // 直接join会进行shuffle，导致性能低下</span><br><span class="line">//    val joinRDD=rdd1.join(rdd2)</span><br><span class="line">//    joinRDD.foreach(println)</span><br><span class="line"></span><br><span class="line">    // 可以将较小的rdd广播出去</span><br><span class="line">    val rddData=rdd1.collectAsMap()</span><br><span class="line">    val rddBC=sc.broadcast(rddData)</span><br><span class="line">    val rdd3=rdd2.mapPartitions(partition=&gt;&#123;</span><br><span class="line">      val bc=rddBC.value</span><br><span class="line">      for&#123;</span><br><span class="line">        (key,value)&lt;-partition</span><br><span class="line">        if(rddData.contains(key))</span><br><span class="line">      &#125;yield (key,bc.get(key).getOrElse(&quot;&quot;),value)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    rdd3.foreach(t=&gt;println(t._1+&quot;,&quot;+t._2+&quot;,&quot;+t._3))</span><br></pre></td></tr></table></figure><h2 id="1-2-累加器-Accumulator"><a href="#1-2-累加器-Accumulator" class="headerlink" title="1.2 累加器 Accumulator"></a>1.2 累加器 Accumulator</h2><ol><li><p>顾名思义，累加器是只能 累加的变量。在 task 中只能对累加器进行添加数值，而不能获取累加器的值。<strong>累加器只能在 driver 端获取</strong>。</p></li><li><p>累加器支持加法交换律和结合律，因此 可以有效的支持 spark task 的并行计算。</p></li><li><p>累加器可用于 Spark 任务<strong>计数场景</strong>或者 Spark 任务<strong>求和场景</strong>。Spark 内置了几种累加器，支持自定义累加器。作为 Spark 开发工程师，可以创建命名的或未命名的累加器。如下所示，一个命名累加器（在这个实例计数器 中）将显示在修改该累加器的 stage web UI 中。</p></li><li><p>Spark 显示 “Tasks”表中由任务修改的每个累加器的值。<br><img src="/images/2019/07/24/20923920-ae19-11e9-8908-0b6e2efa6f40.png" alt="累加器.png"></p></li></ol><h3 id="1-2-1-注意使用累加器的陷阱"><a href="#1-2-1-注意使用累加器的陷阱" class="headerlink" title="1.2.1 注意使用累加器的陷阱"></a>1.2.1 注意使用累加器的陷阱</h3><ol><li>执行多个 action !!</li><li>Task 缓存数据被踢出时，下次用到时，会重新计算，此时累加器会重复计数。综上，实际项目中，累加器仅用于程序调试。</li></ol><p><strong>代码片段</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">AccumulatorV2&lt;Long, Long&gt; longAccumulator = jsc.sc().longAccumulator(&quot;longAccumulator&quot;);</span><br><span class="line">JavaRDD&lt;String&gt; javaRDD = jsc.textFile(&quot;in/README.md&quot;);</span><br><span class="line">JavaRDD&lt;String&gt; map = javaRDD.map(line -&gt; &#123;</span><br><span class="line">    longAccumulator.add(Long.valueOf(1));</span><br><span class="line">    return line.toUpperCase();</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">// 执行 action 操作才会执行以上 transformation</span><br><span class="line">map.count();</span><br><span class="line">System.out.println(&quot;count: &quot; + javaRDD.count());</span><br><span class="line">System.out.println(&quot;i: &quot; + longAccumulator.value());</span><br></pre></td></tr></table></figure><p><strong>输出</strong>：</p><blockquote><p>count: 103<br>i: 103</p></blockquote><h2 id="1-2-2-累加器和广播变量的不同"><a href="#1-2-2-累加器和广播变量的不同" class="headerlink" title="1.2.2 累加器和广播变量的不同"></a>1.2.2 累加器和广播变量的不同</h2><ol><li><p>广播变量一般用于 Spark 程序调优，如果不使用广播变量，程序计算结果不会错误，只是性能可能低下。</p></li><li><p>累加器不同，如果应该使用累加器的场景，你不使用，此时，程序计算结果就是错误的。</p></li></ol><h1 id="2-Spark-应用程序调度过程"><a href="#2-Spark-应用程序调度过程" class="headerlink" title="2. Spark 应用程序调度过程"></a>2. Spark 应用程序调度过程</h1><p><img src="/images/2019/07/24/2f0d3180-ae19-11e9-8908-0b6e2efa6f40.png" alt="spark 应用程序调度过程.png"></p><h2 id="2-1-Client-提交-Job-Driver-会向-master-申请资源，master-收到请求之后，会向-worker-进行资源调度，启动-Executor，然后，Executor-向-Driver-进行注册。"><a href="#2-1-Client-提交-Job-Driver-会向-master-申请资源，master-收到请求之后，会向-worker-进行资源调度，启动-Executor，然后，Executor-向-Driver-进行注册。" class="headerlink" title="2.1 Client 提交 Job, Driver 会向 master 申请资源，master 收到请求之后，会向 worker 进行资源调度，启动 Executor，然后，Executor 向 Driver 进行注册。"></a>2.1 Client 提交 Job, Driver 会向 master 申请资源，master 收到请求之后，会向 worker 进行资源调度，启动 Executor，然后，Executor 向 Driver 进行注册。</h2><h2 id="2-2-此时Spark-应用程序就会知道哪些-worker-上面的-executor-已经就绪。接着开始执行-spark-任务："><a href="#2-2-此时Spark-应用程序就会知道哪些-worker-上面的-executor-已经就绪。接着开始执行-spark-任务：" class="headerlink" title="2.2 此时Spark 应用程序就会知道哪些 worker 上面的 executor 已经就绪。接着开始执行 spark 任务："></a>2.2 此时Spark 应用程序就会知道哪些 worker 上面的 executor 已经就绪。接着开始执行 spark 任务：</h2><ol><li><p>遇到 action 操作—&gt; 创建一个 job—&gt; 提交给 DAGScheduler，DAGScheduler 会 把 Job分为多个 stages（shuffle: 最后一个 stage 里面的 task 叫 ResultTask，前面 stage 里面的 task 叫 ShuffleMapTask），为每个 stage 创建 一个 taskset 集合，集合中的 task 计算逻辑完全 相同，只是处理的数据不同。Task 的数量等于 partition 的数量，但是同时执行的 task  的数量等于集群 core 的数量。整个 Application 运行完成时间，等于最后一个执行完成的 task 的时间。（数据倾斜问题）</p></li><li><p>然后 DAGScheduler 会把 taskset 交给 TaskScheduler ，TaskScheduler 会把 taskset 里面的 task 发送给 Executor。Executor 接收到 task，会启动一个线程池 TaskRunner，在里面运行 task。</p></li></ol><blockquote><p>spark-submit –master spark://bigdata01:7077 –class sparkcore.learnTextFile<br>F:\train\data\sparkapp\learnTextFile.jar</p></blockquote><h2 id="2-3-从-Spark-集群角度（静态）："><a href="#2-3-从-Spark-集群角度（静态）：" class="headerlink" title="2.3 从 Spark 集群角度（静态）："></a>2.3 从 Spark 集群角度（静态）：</h2><h3 id="2-3-1-Master"><a href="#2-3-1-Master" class="headerlink" title="2.3.1 Master"></a>2.3.1 Master</h3><p>standalone 模式下的集群管理器，负责资源的分配。相当于 YARN 模式下的 ResourceManager。</p><h3 id="2-3-2-Worker"><a href="#2-3-2-Worker" class="headerlink" title="2.3.2 Worker"></a>2.3.2 Worker</h3><p>集群中任何可以运行 Application 代码的节点，类似于 YARN 中的 NodeManager 节点。在 Standalone 模式中指的就是通过 Slave 文件配置的 Worker 节点，在 Spark on YARN 模式中指的就是 NodeManager 节点。</p><h2 id="2-4-从-Spark-应用程序的角度（动态）："><a href="#2-4-从-Spark-应用程序的角度（动态）：" class="headerlink" title="2.4 从 Spark 应用程序的角度（动态）："></a>2.4 从 Spark 应用程序的角度（动态）：</h2><h3 id="2-4-1-Application"><a href="#2-4-1-Application" class="headerlink" title="2.4.1 Application"></a>2.4.1 Application</h3><p>Spark 应用程序，包含 Driver 功能代码和分布在集群中多个节点上运行的 Executor 代码。</p><h3 id="2-4-2-Driver"><a href="#2-4-2-Driver" class="headerlink" title="2.4.2 Driver"></a>2.4.2 Driver</h3><p>运行 Application 的 main() 函数并创建 SparkContext，其中创建 SparkContext 的目的是为了准备 Spark 应用程序的运行环境。在Spark 中有 SparkContext 负责和 ClusterManager 通信，进行资源的申请、任务的分配和监控等。当 Executor 部分运行完毕后， Driver 负责将 SparkContext 关闭。</p><h3 id="2-4-3-Action"><a href="#2-4-3-Action" class="headerlink" title="2.4.3 Action"></a>2.4.3 Action</h3><h3 id="2-4-4-Transformation"><a href="#2-4-4-Transformation" class="headerlink" title="2.4.4 Transformation"></a>2.4.4 Transformation</h3><h3 id="2-4-5-Job"><a href="#2-4-5-Job" class="headerlink" title="2.4.5 Job"></a>2.4.5 Job</h3><p>包含多个 Task 组成的并行计算，由 Spark Action 生成，一个 Job 包含多个 RDD 及作用于 RDD 上的各种 transformation。</p><h3 id="2-4-6-Stage"><a href="#2-4-6-Stage" class="headerlink" title="2.4.6 Stage"></a>2.4.6 Stage</h3><p>每个 Job 会被拆分很多组 Task，每组 task 被称为 Stage，也可称 TaskSet，一个作业分为多个 stage。</p><h3 id="2-4-7-Task"><a href="#2-4-7-Task" class="headerlink" title="2.4.7 Task"></a>2.4.7 Task</h3><p>被送到 Executor 上的工作任务，运行计算逻辑的地方。</p><h3 id="2-4-8-Executor"><a href="#2-4-8-Executor" class="headerlink" title="2.4.8 Executor"></a>2.4.8 Executor</h3><p>运行在 Worker 节点上的一个进程，该进程负责运行 Task，并且负责将数据存在内存或者磁盘上，每个 Application 都有各自独立的一批 Executor。</p><h1 id="3-Spark-Master-高可用配置-HA"><a href="#3-Spark-Master-高可用配置-HA" class="headerlink" title="3. Spark Master 高可用配置 HA"></a>3. Spark Master 高可用配置 HA</h1><p>默认情况下，Standalone 调度集群能够处理 worker 故障（通过将计算任务转移到其他 worker）。然而，调度器使用一个 Master 来做出调度决策，这（默认情况下）会产生一个单点故障问题：如果 Master 崩溃，就不能提交新的应用程序。为了克服这一点，有两个高可用性方案，详细如下。</p><h2 id="3-1-Standby-Masters-with-ZooKeeper"><a href="#3-1-Standby-Masters-with-ZooKeeper" class="headerlink" title="3.1 Standby Masters with ZooKeeper"></a>3.1 Standby Masters with ZooKeeper</h2><h3 id="3-1-1-概述"><a href="#3-1-1-概述" class="headerlink" title="3.1.1 概述"></a>3.1.1 概述</h3><p>使用 ZooKeeper 来提供 leader 选举和一些状态存储，您可以在集群中启动多个连接到同一个 ZooKeeper 实例的 master 服务器。其中一个将被选为“leader”，其他的将保持 standby 状态。如果当前的 leader  挂掉 ，将选举另一个 master 成为 leader，恢复旧 master 的状态，然后恢复调度工作。整个恢复过程需要 1 到 2 分钟。注意，这种延迟只影响调度新应用程序——在 master 故障转移期间已经运行的应用程序不受影响。</p><h3 id="3-1-2-配置"><a href="#3-1-2-配置" class="headerlink" title="3.1.2 配置"></a>3.1.2 配置</h3><p>为了启动这个恢复模式，您可以在 spark-env.sh 中设置 SPARK_DAEMON_JAVA_OPTS，配置 spark.deploy.recoveryMode 和 spark.deploy.zookeeper.* 相关的配置。参考： <a href="http://spark.apache.org/docs/latest/configuration.html#deploy" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/configuration.html#deploy</a></p><h3 id="3-1-3-可能的陷阱"><a href="#3-1-3-可能的陷阱" class="headerlink" title="3.1.3 可能的陷阱"></a>3.1.3 可能的陷阱</h3><p>如果您的集群中有多个 master，但是没有正确地配置 master 来使用 ZooKeeper，那么 master 将无法发现彼此，并认为他们都是 leader。这将不会导致一个健康的集群状态（因为所有 master 都将独立调度）。</p><h3 id="3-1-4-详细"><a href="#3-1-4-详细" class="headerlink" title="3.1.4 详细"></a>3.1.4 详细</h3><p>在设置了 ZooKeeper 集群之后，启用高可用性就很简单了。只需只用相同的 ZooKeeper 配置（ZooKeeper URL和目录）在不同的节点上启动多个 master 进程。master  可以在任何时候添加和删除。</p><p>为了安排新的应用程序或将 worker 添加到集群中，他们需要知道当前 leader master 的 IP 地址。这可以通过简单地传递一个 master URL 列表来实现，您以前在这些 master 列表中只传递一个 master URL。例如，您可以启动 SparkContext，指向spark://host1:port1,host2:port2。这将导致您的 SparkContext 尝试向两个 master 注册——如果 host1 宕机，这个配置仍然是正确的，因为我们将找到新的 leader: host2。</p><p>“注册到一个 Master 服务器”和正常 操作之间有一个重要的区别。在启动时，应用程序或 worker 需要能够找到并注册到当前的 master。但是，一旦注册成功，master 就是“在系统中”（即存储在 ZooKeeper）。如果发生故障转移，新 master leader 将联系所有以前注册的应用程序和 worker，通知他们 leader 的更改，因此他们在启动时甚至不需要知道新 master的存在。</p><p>由于这个属性，可以在任何时候创建新的 master，唯一需要担心的是，新的应用程序和 worker 可以找到它进行注册，以防它成为 leader。</p><h3 id="3-1-5-HA-配置实战"><a href="#3-1-5-HA-配置实战" class="headerlink" title="3.1.5 HA 配置实战"></a>3.1.5 HA 配置实战</h3><ol><li>spark-env.sh 配置（所有节点都需要配置）<blockquote><p>export SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=ZOOKEEPER </p></blockquote></li></ol><p>-Dspark.deploy.zookeeper.url=master01:2181master02:2181,slave01:2181,slave02:2181,slave03:2181<br>-Dspark.deploy.zookeeper.dir=/opt/modules/spark213/meta”</p><ol start="2"><li><p>master01</p><blockquote><p>start.all.sh</p></blockquote></li><li><p>master02</p><blockquote><p>start-master.sh</p></blockquote></li><li><p>master01</p><blockquote><p>stop-master.sh</p></blockquote></li><li><p>查看 master 是否切换，standby-&gt;active</p></li></ol><h2 id="3-2-带有本地文件系统的单节点恢复"><a href="#3-2-带有本地文件系统的单节点恢复" class="headerlink" title="3.2 带有本地文件系统的单节点恢复"></a>3.2 带有本地文件系统的单节点恢复</h2><h3 id="3-2-1-概述"><a href="#3-2-1-概述" class="headerlink" title="3.2.1 概述"></a>3.2.1 概述</h3><p>ZooKeeper 是实现 产品级高可用性的最佳方法，但如果您只是想在 master 服务器宕机时重启它，那么文件系统模式 可以解决这个问题。当应用程序和工作者注册时，它们有足够的状态被写入到提供的目录中，以便在 master 进程重新启动时可以恢复它们。</p><h3 id="3-2-2-配置"><a href="#3-2-2-配置" class="headerlink" title="3.2.2 配置"></a>3.2.2 配置</h3><p>为了启动这个恢复模式，可以在 spark-env.sh 中设置 SPARK_DAEMON_JAVA_OPTS 的相关配置：</p><table><thead><tr><th>属性</th><th>含义</th></tr></thead><tbody><tr><td>spark.deploy.recoveryMode</td><td>将文件系统设置为启用单节点恢复莫斯（默认 ：无）</td></tr><tr><td>spark.deploy.recoveryDirectory</td><td>Spark 将存储恢复状态的目录，从 master 的角度进行访问</td></tr></tbody></table><h3 id="3-2-3-细节"><a href="#3-2-3-细节" class="headerlink" title="3.2.3 细节"></a>3.2.3 细节</h3><p>这个解决方案可以与进程监视器、管理器（如 monitor）一起使用，或者只是通过重新启动启用手动恢复。</p><p>虽然文件系统恢复看起来比不进行任何恢复要好得多，但是对于某些开发或实验目的来说，这种模式可能不是最优的。特别是，通过 kill master 来停止 master 不会清理它的恢复状态，所以无论何时启动一个新主，它都会进入恢复模式。如果需要等待所有以前注册的工人、客户端超时，这将使启动时间增加最多 1 分钟。</p><p>虽然 它不受官方支持，但您可以将 NFS 目录挂载为恢复目录。如果原始的 master 节点完全死亡，那么您可以在另一个节点 上启动一个主节点，它将正确地恢复所有以前注册的 worker、应用程序（相当于 ZooKeeper 恢复）。然而，为了注册，未来的应用程序必须能够找到新的主程序。</p><h1 id="4-Spark-应用程序内存和-CPU-的分配"><a href="#4-Spark-应用程序内存和-CPU-的分配" class="headerlink" title="4. Spark 应用程序内存和 CPU 的分配"></a>4. Spark 应用程序内存和 CPU 的分配</h1><h2 id="4-1-Spark-运行模式"><a href="#4-1-Spark-运行模式" class="headerlink" title="4.1 Spark 运行模式"></a>4.1 Spark 运行模式</h2><h3 id="4-1-1-local"><a href="#4-1-1-local" class="headerlink" title="4.1.1 local"></a>4.1.1 local</h3><p>本地模式，不需要安装 spark，也不需要启动 spark 集群。用于开发环境</p><h3 id="4-1-2-standalone"><a href="#4-1-2-standalone" class="headerlink" title="4.1.2 standalone"></a>4.1.2 standalone</h3><p>需要安装 Spark 需要启动 Spark 集群。<br><strong>client</strong>: driver 运行在和 sparksubmit 同一个进程中，此时如果关闭命令行窗口，相当于取消程序运行。可以从控制台看到程序输出内容<br><strong>cluster</strong>: driver 运行在 worker 节点上，spark-submit 提交后即退出，此时命令行窗口关闭，不影响程序运行，但是从控制台看不到程序输出内容。</p><h3 id="4-1-3-yarn"><a href="#4-1-3-yarn" class="headerlink" title="4.1.3 yarn"></a>4.1.3 yarn</h3><p>需要安装 spark，不需要启动 spark 集群。<br><strong>client</strong>: driver 运行在 client 进程中 ，此时如果关闭命令行窗口，相当于取消 程序运行。可以从控制台看到程序输出内容。<br><strong>cluster</strong>: driver 运行在 ApplicationMaster 进程 中，client 端提交后即退出，此时命令行窗口关闭，不影响程序运行，但是控制台上看不到程序输出内容</p><h3 id="4-1-4-mesos"><a href="#4-1-4-mesos" class="headerlink" title="4.1.4 mesos"></a>4.1.4 mesos</h3><h2 id="4-2-如何设置-Spark-配置属性"><a href="#4-2-如何设置-Spark-配置属性" class="headerlink" title="4.2 如何设置 Spark 配置属性"></a>4.2 如何设置 Spark 配置属性</h2><p>在 spark-defaults.conf 文件中设置 spark 的资源配置，资源分配参数名为 spark.xx.xx，如 spark.driver.cores</p><h3 id="4-2-1-有三种方式设置spark-的配置属性"><a href="#4-2-1-有三种方式设置spark-的配置属性" class="headerlink" title="4.2.1 有三种方式设置spark 的配置属性"></a>4.2.1 有三种方式设置spark 的配置属性</h3><p>（按优先级从高到低）</p><ol><li>在程序代码中通过 SparkConf 对象设置；</li><li>通过 spark-submit 任务提交参数设置；</li><li>通过 spark-defaults.conf 文件设置。</li></ol><h3 id="4-2-2-典型的-spark-defaults-conf-文件内容"><a href="#4-2-2-典型的-spark-defaults-conf-文件内容" class="headerlink" title="4.2.2 典型的 spark-defaults.conf 文件内容"></a>4.2.2 典型的 spark-defaults.conf 文件内容</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.executor.memory   8G</span><br><span class="line">spark.driver.memory 16G</span><br><span class="line">spark.driver.maxResultSize  8G</span><br><span class="line">spark.akka.frameSize    512</span><br></pre></td></tr></table></figure><h2 id="4-3-集群资源配置实战"><a href="#4-3-集群资源配置实战" class="headerlink" title="4.3 集群资源配置实战"></a>4.3 集群资源配置实战</h2><p>在 spark-env.sh 中配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SPARK_WORKER_CORES=1</span><br><span class="line">SPARK_WORKER_MEMORY=900m</span><br><span class="line">SPARK_WORKER_INSTANCES=2</span><br></pre></td></tr></table></figure><p><img src="/images/2019/07/24/8d693ad0-ae19-11e9-8908-0b6e2efa6f40.png" alt="sparkenv配置.png"></p><h2 id="4-4-Spark-应用程序-资源配置实战"><a href="#4-4-Spark-应用程序-资源配置实战" class="headerlink" title="4.4 Spark 应用程序 资源配置实战"></a>4.4 Spark 应用程序 资源配置实战</h2><h3 id="4-4-1-–executor-cores"><a href="#4-4-1-–executor-cores" class="headerlink" title="4.4.1 –executor-cores"></a>4.4.1 –executor-cores</h3><ol><li><blockquote><p>spark-submit –master spark://master01:7077 –executor-memory 900m –executor-cores 6 –class spark.learnTextFile /opt/sparkapp/learnTextFile.jar</p></blockquote></li></ol><p>上面的命令中，所需要的的单个 executor 的 cores 数量 6 超过了 worker 节点的 cores 数量（配置为单核），程序无法运行。<br><img src="/images/2019/07/24/db3c0530-ae19-11e9-8908-0b6e2efa6f40.png" alt="cores 配置超过物理节点 所有的核心数.png"></p><ol start="2"><li>如果 worker 有足够的资源，对于同一个应用，会在一个 worker 上启动多个 executor。如：<blockquote><p>spark-submit –master spark://master01:7077 –executor-memory 918m –executor-cores 1 –class sparkcore.learnTextFile /opt/sparkapp/learnTextFile.jar</p></blockquote></li></ol><p><img src="/images/2019/07/24/e80555f0-ae19-11e9-8908-0b6e2efa6f40.png" alt="一个 worker 启动多 exe1.png"></p><p><img src="/images/2019/07/24/fb0fe890-ae19-11e9-8908-0b6e2efa6f40.png" alt="一个 worker 启动多 exe2.png"></p><p>另一个例子：</p><blockquote><p>spark-submit –master spark://master01:7077 –executor-memory 1200m –executor-cores 1 –class sparkcore.learnTextFile /opt/sparkapp/learnTextFile.jar<br><img src="/images/2019/07/24/008527e0-ae1a-11e9-8908-0b6e2efa6f40.png" alt="一个 worker 没启动多 exe1.png"></p></blockquote><p><img src="/images/2019/07/24/04841ae0-ae1a-11e9-8908-0b6e2efa6f40.png" alt="一个 worker 没启动多 exe2.png"></p><p>另一种参数赋值的方式：<br>–conf PROP=VALUE</p><blockquote><p>spark-submit –master spark://master01:7077 –conf spark.executor.memory-1201m –conf spark.executor.cores=1 –class sparkcore.learnTextFile /opt/sparkapp/learnTextFile.jar</p></blockquote><h3 id="4-4-2-参数优先级"><a href="#4-4-2-参数优先级" class="headerlink" title="4.4.2 参数优先级"></a>4.4.2 参数优先级</h3><p>spark.default.conf &lt; spark-submit -conf &lt; SparkConf 代码</p><p><strong>测试</strong>：</p><ol><li><p>代码：<br><img src="/images/2019/07/24/0deefe60-ae1a-11e9-8908-0b6e2efa6f40.png" alt="参数优先级 代码设置.png"></p></li><li><p>spark-submit:</p><blockquote><p>spark-submit –master spark://master01:7077 –conf spark.executor.memory=1201m  –conf spark.executor.cores=1 –class sparkcore.learnTextFile /opt/sparkapp/learnTextFile.jar</p></blockquote></li><li><p>运行结果：<br><img src="/images/2019/07/24/14ce5500-ae1a-11e9-8908-0b6e2efa6f40.png" alt="参数优先级 结果1.png"></p></li></ol><p><img src="/images/2019/07/24/19130390-ae1a-11e9-8908-0b6e2efa6f40.png" alt="参数优先级 结果2.png"></p><h3 id="4-4-3-几个常用的配置项"><a href="#4-4-3-几个常用的配置项" class="headerlink" title="4.4.3 几个常用的配置项"></a>4.4.3 几个常用的配置项</h3><p>Spark 常用配置项，主要是对 Spark  运行过程中各个使用资源的地方，通过调整参数值，来优化资源使用效率，提升 Spark 作业性能。以下每个参数都对应着 spark 应用程序运行原理中的某个部分，同时给出了一些参考值。</p><p><strong>1. num-executors</strong></p><ul><li><p><strong>参数说明</strong>：该参数用于<strong>设置 Spark 作业要用多少个 Executor 进程来执行</strong>。Driver 在向 YARN 集群管理器申请资源时，YARN 集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的 Executor 进程。默认只会给你启动少量的 Executor 进程，此时你的 Spark  作业的运行速度是非常慢的。</p></li><li><p><strong>参数调优建议</strong>：参考 yarn container 的大小，设置太少或太多的 Executor 进程都不好。设置的太少，无法充分利用集群资源；设置 的太多的话，大部分队列可能无法给予充分的资源。</p></li></ul><p><strong>2. executor-memory (spark.executor.memory)</strong></p><ul><li><p><strong>参数说明</strong>：该参数用于设置每个 Executor 进程的内存 。Executor 内存的大小，很多时候直接决定了 Spark 作业的性能，而且跟常见的 JVM OOM异常，也有直接的关联。Executor 8G 4Core; 2Executor 4G 2Core</p></li><li><p><strong>参数调优建议</strong>：每个 Executor 进程的内存设置 4G ~ 8G 较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors 乘以 executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源 队列最大总内存的 1/3~1/2，避免你自己的 Spark 作业占用了队列所有的资源，导致别的作业无法运行。</p></li></ul><p><strong>3. executor-cores (spark.executor.cores)</strong></p><ul><li><strong>参数说明</strong>：该参数用于设置每个 Executor 进程的 CPU core数量。这个参数决定了每个 Executor 进程并行执行 task 线程的能力。因为每个 CPU core 同一时间只能执行一个 task 线程，因此每个 Executor 进程的 CPU core 数量越多，越能够快速地执行完分配给自己的所有 task 线程。</li><li><strong>参数调优建议</strong>：Executor 的 CPU core 数量设置为2 ~ 3 个较为合适。同样地根据不同部门的资源队列来定，可以看看自己的资源队列的最大 CPU core限制是多少，再依据设置的 Executor 数量，来决定每个 Executor 进程可以分配到几个 CPU core。同样建议，如果是跟他人共享这个队列，那么 num-executors * executor-cores 不要超过队列总 CPU core 的1/3~1/2 左右比较合适，也是避免影响其他同学的作业运行。</li></ul><p><strong>4. driver-memory</strong></p><ul><li><strong>参数说明</strong>：该参数用于设置 Driver 进程的内存。</li><li><strong>参数调优建议</strong>：Driver 的内存通常来说不设置，或者设置 1G 左右应该就够了。唯一需要注意的一点是，如果需要使用 <strong>collect 算子</strong>将RDD的数据全部拉取到 Driver 上进行处理，那么必须确保 Driver 的内存足够大，  否则会出现 OOM 内存溢出的问题。</li></ul><p><strong>5. spark.default.parallelism</strong></p><ul><li><strong>参数说明</strong>：该参数用于设置每个 stage 的默认 task 数量。这个参数极为重要，如果不设置可能会直接影响你的 Spark 作业性能。</li><li><strong>参数调优建议</strong>：Spark 作业的默认 task  数量为 500 ~ 1000 个较为合适。如果不设置这个参数，那么此时就会导致 Spark 自己根据底层 HDFS的 block数量来设置 task的数量，默认是一个 HDFS block 对应一个 task。通常来说，Spark 默认设置的数量是偏少的，如果task 数量偏少的话，就会导致你前面设置好的 Executor 的参数都无法提升性能。比如，无论你的 Executor 内存和 CPU 有多大，但是  task 只有一个或者几个，那么 90% 的 Executor 进程可能根本就没有 task  执行，也就是白白浪费了资源！因此 Spark 官网建议的设置原则是，<strong>设置该参数为 num-executors * executor-cores 的 2~3 倍比较合适</strong>。</li></ul><p><strong>5. spark.storage.memoryFraction</strong></p><ul><li><p><strong>参数说明</strong>：该参数用于设置 RDD 持久化数据在 Executor 内存中能占的比例，默认是 0.6。也就是说，默认 Executor 60% 的内存，可以用来保存持久化的 RDD数据。</p></li><li><p><strong>参数调优建议</strong>：如果 Spark 作业中，有较多的 RDD 持久化操作，该参数的值可以适当 提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果 Spark 作业中的 shuffle 类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的 gc 导致运行缓慢 （通过 spark web ui 可以观察到作业的 gc 耗时），意味着 task 执行用户代码的内存不够用，那么同样建议调低这个参数的值。</p></li></ul><p><strong>6. spark.shuffle.memoryFraction</strong></p><ul><li><strong>参数说明</strong>：该参数用于设置 shuffle 过程中一个 task 拉取到上个 stage 的task 的输出后，进行聚合操作时能够使用的 Executor 内存的比例，默认是 0.2。也就是说，Executor 默认只有 20% 的内存用来进行该操作。shuffle 操作在进行聚合时，如果发现使用的内存超过了这个 20% 的内存用来进行该操作。shuffle 操作在进行聚合时，如果发现使用的内存超过了这个 20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li><li><strong>参数调优建议</strong>：如果 Spark 作业中的 RDD 持久化操作较少，shuffle 操作较多时，建议降低持久化操作的内存占比，提高 shuffle 操作的内存占比比例，避免 shuffle 过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现由于频繁的 gc 导致运行缓慢，意味着 task 执行用户代码的内存不够用，那么同样建议调低这个参数值。</li></ul><p><strong>注</strong>：资源参数的调优，没有一个固定的值，需要根据自己的实际情况（包括 Spark 作业中的 shuffle 操作数量、RDD 持久化操作数量以及 spark web UI 中显示的作业 gc 情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。</p><h1 id="5-Spark-on-yarn"><a href="#5-Spark-on-yarn" class="headerlink" title="5. Spark on yarn"></a>5. Spark on yarn</h1><h2 id="5-1-概述"><a href="#5-1-概述" class="headerlink" title="5.1 概述"></a>5.1 概述</h2><ol><li><p>Spark 支持可插拔的集群管理器（Standalone, YARN），集群管理负责启动 executor 进程。Spark  支持的四种集群模式（standalone, mesos, yarn, Kubernetes），三种集群模式都是由两个组件组成：master 和 slave。Master 服务（YARN ResourceManager, Mesos master 和 Spark standalone master）决定哪些 application 可以运行，何时运行以及在哪里运行。而 slave 服务（YARN NodeManager, Mesos slave 和 Spark standalone worker）是运行 executor 进程。</p></li><li><p>当在 YARN 上运行 Spark 作业，每个 Spark executor 作为一个 YARN 容器（container）运行。Spark 应用程序的多个 Tasks 在同一个容器（container）里面运行。注意：MapReduce作业为每个 Task 开启不同的 JVM 来运行。</p></li><li><p>不需要启动 spark 集群，但是需要配置 SPARK_HOME。<br>配置 HADOOP_CONF_DIR=$HADOOP_HOME。</p></li><li><p>提交命令：</p><blockquote><p>spark-submit –class sparkcore.learnTextFile –master yarn –deploy cluster /opt/sparkapp/learnTextFile.jar</p></blockquote></li><li><p>查看输出：</p><blockquote><p>yarn logs –applicationId application_xxx</p></blockquote></li><li><p>查看任务运行状态：</p><blockquote><p>yarn application -status application_xx</p></blockquote></li></ol><h2 id="5-2-资源分配"><a href="#5-2-资源分配" class="headerlink" title="5.2 资源分配"></a>5.2 资源分配</h2><p>资源分配包括内存的分配和 CPU 的分配</p><h3 id="5-2-1-CPU-的分配"><a href="#5-2-1-CPU-的分配" class="headerlink" title="5.2.1 CPU 的分配"></a>5.2.1 CPU 的分配</h3><p>Spark 计算的关键抽象是 RDD，RDD 包括多个 partitions, 一个 partition 对应一个 task，对应一个 CPU core。一个 spark job 的并行度依赖于 RDD的partitions 和可用的 CPU cores。</p><h3 id="5-2-2-内存的分配"><a href="#5-2-2-内存的分配" class="headerlink" title="5.2.2 内存的分配"></a>5.2.2 内存的分配</h3><p><strong>spark 内存主要用于执行 job（执行内存）和存储数据（存储内存）</strong>。<br>执行内存可以驱逐存储内存（仅在存储内存达到一定阈值时）。存储内存不可以驱逐执行内存。</p><p><strong>执行内存</strong>用于 shuffle、joins、sorts 和 aggregations。</p><p><strong>存储内存</strong>用于缓存数据和集群内传播数据。</p><h2 id="5-3-如何给-Spark-分配资源"><a href="#5-3-如何给-Spark-分配资源" class="headerlink" title="5.3 如何给 Spark 分配资源"></a>5.3 如何给 Spark 分配资源</h2><p><strong>1.</strong> 当 Spark 应用运行在 yarn 上面，对 yarn 来说，Spark 应用只是一个应用，就像 MapReduce 应用一样，只是一个运行在 yarn 上面的应用而已。这是极好的！因为你已经学得有关 yarn 架构、资源分配和调优知识，对 Spark on yarn 完全有效。</p><p><strong>2.</strong> 如前所述，yarn 有两个关键的实体：<br><strong>ResourceManager</strong>–管理集群资源；<br><strong>ApplicationMaster</strong>–负责从 ResourceManager 请求资源，进而将资源分配给每个集群节点上的 <strong>NodeManager</strong>，这样集群就可以执行每个 task。</p><p><strong>3.</strong> ApplicationMaster 是属于某个特定 Application的，执行 MapReduce 应用时，yarn 使用了 MapReduce 框架特定的 ApplicationMaster，当执行 Spark job 时，yarn 使用了 Spark 框架特定的 ApplicationMaster。</p><p><strong>4.</strong> yarn 通过逻辑抽象单元 container 来分配资源，container 代表一个资源集——内存和 CPU。例如，一个 container 可以由 2个 CPU core 和 4G 内存组成。当 Spark ApplicationMaster 从 ResourceManager 请求资源时，通过评估 job 的资源需求，然后请求一定数量的 container 来完成 job。基于集群可用资源，ApplicationMaster 会要求 worker 节点上的 NodeManaer 运行一定数量的 container。</p><p><strong>5.</strong> 当运行 Spark on yarn 应用时，Spark 位于 yarn 架构之上，使用相同的过程请求资源，yarn 用相同的方式将 container 分配给 Spark job。Spark 所有的 executor 和 driver 都运行在 container 中，ApplicationMaster 负责 container 间的通信。</p><p><strong>6.</strong> ApplicationMaster 运行在一个单独的 container 中。Executor 也运行在 yarn container 中，一个 Executor 运行在一个 container 中，在 MapReduce 应用资源分配过程中，一个 map/reduce task 运行在单独的 container 中。但是，在 spark Executor 中，Executor container 包含一个更细粒度的实体——task。每个 Executor container 可以运行一个 task 集合，来完成实际的工作。</p><p><strong>7.</strong> Spark 使用了由 YARN 管理的两个关键资源：CPU 和内存。虽然磁盘 I/O 和网络性能对应用程序性能有影响，YARN 并不是真正关注这些资源。</p><h2 id="5-4-Spark-应用资源分配的限制"><a href="#5-4-Spark-应用资源分配的限制" class="headerlink" title="5.4 Spark 应用资源分配的限制"></a>5.4 Spark 应用资源分配的限制</h2><p>可以配置 YARN 属性来限制 YARN 可以使用的最大内存和 CPU core。Spark 的资源使用受限于这些配置属性。总结一下这些属性。</p><ul><li><p><strong>yarn.nodemanager.resource.memory-mb</strong> 参数设置了分配给集群中一个运行 NodeManager 节点 上所有 container 的内存上限。此内存设置对 Spark on yarn 同样有效。</p></li><li><p><strong>yarn.nodemanager.resource.cpu-vcores</strong> 参数设置了集群中一个运行 NodeManager 节点上所有 containers 可以使用的最大 CPU 核心数。</p></li></ul><ol><li><p>yarn 以块的形式分配内存，内存块的大小依赖于参数 yarn.scheduler.minimum-allocation-mb 值—— yarn 可以为每个 container 请求分配的最小内存块。</p></li><li><p>yarn 以 core 的形式分配 CPU，core 的个数依赖于参数 yarn.scheduler.minimum-allocation-vcores 值—— yarn 可以为每个 container 请求分配的最小 core 数。</p></li></ol><h2 id="5-5-spark-on-yarn-应用的提交方式"><a href="#5-5-spark-on-yarn-应用的提交方式" class="headerlink" title="5.5 spark on yarn 应用的提交方式"></a>5.5 spark on yarn 应用的提交方式</h2><ol><li><p>yarn-client 模式：spark driver 运行在客户端 client 进程中。YARN ApplicationMaster 进程代表应用向 YARN 请求资源；<br>client 向 yarn 的 RM 申请 container，用于运行 AM，RM 在 NodeManager 上启动 container 运行 AM （里面没有 SparkContext），SparkContext 在 client 端实例化，会通知 AM，申请资源，AM 向 RM 申请资源，AM 在 NodeManager 上启动 container(executor), SparkContext 分配 task 给 executor，executor 启动线程池执行，运行完成后，driver 会通知 RM 收回资源，sparkcontext.close()。</p></li><li><p>yarn-cluster 模式：spark driver 运行在 YARN 管理的 ApplicationMaster 进程中——client 将定期轮询 AM 以获取状态更新，并在控制台显示它们。一旦应用程序运行完毕，client 将退出。</p><br>client 向 yarn 的 RM 申请 container，用于运行 AM，RM 在 NodeManager 上启动 container 运行 AM，**AM 类里面会实例化 SparkContext(driver)**，AM 向 RM 申请资源，AM 在 NodeManager 上启动 container(executor), sparkContext 分配 task 给 executor，executor 启动线程池执行，运行完成后，driver 会通知 RM 收回资源，sparkcontext.close()。</li></ol><h2 id="5-6-Spark-on-yarn-的资源分配依赖于-spark-的运行模式"><a href="#5-6-Spark-on-yarn-的资源分配依赖于-spark-的运行模式" class="headerlink" title="5.6 Spark on yarn 的资源分配依赖于 spark 的运行模式"></a>5.6 Spark on yarn 的资源分配依赖于 spark 的运行模式</h2><p>因此，我们分别讨论 client 和 cluster 模式中的资源分配。</p><p>注意，关于配置 YARN 资源分配的所有知识，为 Spark 分配资源仍然有效。</p><p><strong>yarn 需要为以下 Spark 关键实体分配资源</strong>：</p><ol><li>driver</li><li>Executor</li></ol><h3 id="5-6-1-为-spark-driver-分配资源"><a href="#5-6-1-为-spark-driver-分配资源" class="headerlink" title="5.6.1 为 spark driver 分配资源"></a>5.6.1 为 spark driver 分配资源</h3><p>在开始讨论如何分配资源给 Spark driver 之前，请允许我总结一下 driver 所扮演的角色。</p><p><strong>driver 的职责</strong><br>spark on yarn 应用程序的 driver 进程完成如下功能：</p><ol><li>使用 spark 执行引擎，将应用程序分成 job，stages 和 tasks。</li><li>为 Executor 进程提供包依赖服务。 –jars</li><li>与 YARN ResourceManager 交互，获取资源，分配给各个节点用于执行应用程序的 task。</li></ol><p><strong>注意</strong>：</p><ul><li>在 Spark 运行环境中，driver负责：</li></ul><ol><li>定义和调用 RDD 的 action；</li><li>跟踪 RDD 的血统 lineage。</li></ol><ul><li>workers(executors)负责：</li></ul><ol><li>存储 RDD partitions;</li><li>执行 RDD transformation。</li></ol><p>一旦 driver 获取资源执行 Spark Application, 其会创建一个执行计划——根据应用程序代码中的 action 和 transformation 生成一个有向无环图 DAG，并发送给 worker节点。</p><p>driver 进程包括两个组件，用来处理 task 分配：</p><ol><li>DAG Scheduler 进程将 DAG 划分为 task；</li><li>task scheduler 在集群各个节点间调度 task。一旦 task scheduler 完成了 task 分配，Executor 开始执行 DAG 中的操作。如果有 task 失败或者出现延迟，task scheduler 会重启失败（失败次数可配置 4 次）的 task 或创建新的 task 来替换延迟的 task。</li></ol><h2 id="5-7-Client-模式下-AM、Driver-的资源分配"><a href="#5-7-Client-模式下-AM、Driver-的资源分配" class="headerlink" title="5.7 Client 模式下 AM、Driver 的资源分配"></a>5.7 Client 模式下 AM、Driver 的资源分配</h2><p>Spark on yarn cient 模式，Spark 应用程序对应的 AM 的资源分配依赖于下面两个配置参数：</p><table><thead><tr><th>配置参数</th><th>参数描述</th><th>默认值</th><th>例子</th></tr></thead><tbody><tr><td>spark.yarn.am.memory</td><td>AM 的 JVM 堆内存</td><td>512M</td><td>spark.yarn.am.memory 777m</td></tr><tr><td>spark.yarn.am.cores</td><td>AM 可用的 core 数量</td><td>1</td><td>spark.yarn.am.cores 4</td></tr></tbody></table><p>由于 YARN 分配资源的单位是 container，那么 AM 运行所在的 container 的大小是多少呢？使用参数spark.yarn.am.memory 设置的内存并不意味着 yarn 为 container 分配的内存就是 777m。</p><p>在 client 模式下，Spark 为 AM 进程分配了一定量的堆外内存。配置参数 spark.yarn.am.memoryOverhead 设置了堆外内存的大小。默认值为：<br><strong>AM Memory * 0.1，最小值为 384M</strong></p><p>那么，我们为 AM 设置了 777MB，777MB*0.1 = 77.7MB，比 384MB 小，所以堆外内存为 384MB。AM 的 cotainer 大小为 777MB+384MB=1161MB，因为 yarn.scheduler.minimum-allocation-mb 的默认值为 1G，yarn 会做舍入操作，为 AM 分配一个内存为 2G 的 container。所以，上例中的 container 大小为：2GB 内存（java 堆内存：-Xmx777M）和 4 CPU core。</p><p>Client 模式下的 driver 内存，使用 spark.driver.memeory</p><h2 id="5-8-Cluster-模式下-AM、Driver-的资源分配"><a href="#5-8-Cluster-模式下-AM、Driver-的资源分配" class="headerlink" title="5.8 Cluster 模式下 AM、Driver 的资源分配"></a>5.8 Cluster 模式下 AM、Driver 的资源分配</h2><p>在 cluster 模式下，spark driver 运行在 yarn ApplicationMaster 进程中。所以，分配给 AM 的资源决定了 driver 的可用资源。下面看下 driver 相关的资源配置参数，这些配置参数，控制了 ApplicationMaster 的资源分配。</p><table><thead><tr><th>配置参数</th><th>参数描述</th><th>默认值</th><th>例子</th></tr></thead><tbody><tr><td>spark.drivers.cores</td><td>AM 可用的 core 数量</td><td>1</td><td>spark.driver.cores 2</td></tr><tr><td>spark.driver.memory</td><td>AM 的 JVM 堆内存</td><td>512M</td><td>spark.driver.memory 1665m</td></tr><tr><td>spark.driver.memoryOverhead</td><td>Driver 堆外内存</td><td>driverMemory * 0.1，至少 384M</td><td></td></tr></tbody></table><p><strong>注意</strong>：与 client 模式一样，cluster 模式也有一个类似的配置参数：<br>spark.driver.memoryOverhead 用于指定 cluster模式下的堆外存大小。该属性默认值为分配给 ApplicationMaster 内存的 10%，最小值为 384M。</p><p><strong>提示</strong>：在 cluster 模式中，当你配置 spark driver 的资源时，间接配置了 yarn AM 服务的资源，因为 driver 运行在 AM 中。</p><p>因为 1665+Max(384, 1665*0.1) = 1665+384 = 2049 &gt; 2048，并且 yarn.scheduler.minimum-allocation-mb = 1024，所以 container 大小为：3GB 内存（Java 堆内存：-Xmx1665M）和 2 CPU core。</p><h2 id="5-9-Executor-的资源配置"><a href="#5-9-Executor-的资源配置" class="headerlink" title="5.9 Executor 的资源配置"></a>5.9 Executor 的资源配置</h2><p>Spark 任务的真正执行是在 worker 节点上——也就是说所有的 task 运行在 worker 节点上。<strong>spark job 的大部分资源应该分配给 Executor</strong>。相比而言，driver 的资源分配要小的多。</p><p><strong>对于 Spark executor 资源，yarn-client 和 yarn-cluster 模式使用相同的配置</strong>。</p><table><thead><tr><th>配置参数</th><th>参数描述</th><th>默认值</th></tr></thead><tbody><tr><td>spark.executor.instances(–num-executors)</td><td>用于静态分配 executor 的数量</td><td>2</td></tr><tr><td>spark.executor.cores(–executor-cores)</td><td>单个 executor 的 core 数量</td><td>1</td></tr><tr><td>spark.executor.memeory(–executor-memeory)</td><td>每个 executor 的堆内存大小</td><td>1G</td></tr><tr><td>spark.executor.memoryOverhead</td><td>每个 executor 的堆外存大小</td><td>executorMemory*0.1，至少 384M</td></tr></tbody></table><p><strong>计算：如果设置 spark.executor.memeory 大小为 2G，那么将启动 2 个 container，大小为 3G，1core，-Xmx2048M</strong></p><p>在 Spark 资源分配表述中，Executor 是 container 的同义词——即一个 Executor，一个 Container。因此，Spark 的 Executor 的分配转换成了 yarn 的 container 的分配。当 Spark 计算出所需要的 Executor 数量，其与 AM 交互，AM 进而与 YARN RM 交互，请求分配 container。</p><p>spark.executor.memeory 参数设置会影响下面两个地方：<br>–Spark 可以缓存的数据量；<br>–shuffle 操作可以使用的内存最大值。</p><p>下图展示了 spark executor 内存与 yarn 总内存 yarn.nodemanager.resource.memory-mb的关系：<br><img src="/images/2019/07/25/81920210-aea4-11e9-8908-0b6e2efa6f40.png" alt="图展示了.png"></p><p>下面的例子显示了在提交 Spark 应用程序时，如何设置目前为止讨论的属性：</p><blockquote><p>spark-submit –class org.apaches.spark.example.SparkPi \<br>–master yarn \<br>–deploy-mode cluster \<br>–driver-memory 4g \<br>–executor-memory 2g \<br>–executor-cores 1 \<br>–queue thequeue \<br>lib/spark-example*.jar \<br>10</p></blockquote><h2 id="5-10-Spark-内存使用"><a href="#5-10-Spark-内存使用" class="headerlink" title="5.10 Spark 内存使用"></a>5.10 Spark 内存使用</h2><h3 id="5-10-1-Spark-如何使用内存"><a href="#5-10-1-Spark-如何使用内存" class="headerlink" title="5.10.1 Spark 如何使用内存"></a>5.10.1 Spark 如何使用内存</h3><p>在前面的章节中，我解释了如何分配内存给 Spark：通过分配内存给 driver 和 executor。Spark 是如何使用分配到的内存呢？Spark 主要在两个方面使用内存：执行代码和存储数据（执行内存 execute memory 和存储内存 storage memory）。<br><strong>执行内存</strong>，用于执行 Spark 操作，包括 shuffle, joins 和 sorts。<br><strong>存储内存</strong>，用于 Spark 缓存数据和在集群间移动中间数据。<br>执行内存的增加，会导致内存中的数据对象被清除，直到存储内存值达到一个阈值。</p><h3 id="5-10-2-Spark-内存使用的配置"><a href="#5-10-2-Spark-内存使用的配置" class="headerlink" title="5.10.2 Spark 内存使用的配置"></a>5.10.2 Spark 内存使用的配置</h3><p>为了了解 Spark 如何分配执行内存和存储内存，我会用下列符号代表内存的各个组成部分。</p><ul><li><strong>M</strong>：表示执行内存和存储内存的总和。</li><li><strong>R</strong>：最小存储空间（阈值），低于这个值，不能将 RDD 从存储内存中清除出去。</li></ul><p>使用以下两个配置属性来调整执行内存和存储内存大小：</p><ul><li><p><strong>spark.memory.fraction</strong>：这个分数表示 M 是 JVM 堆空间的一部分。默认值为0.75（Spark1.6），0.6（Spark2.0）。意思是：分配给 executor 的内存，60% 用于 M，剩下的 40% 用于存储用户数据结构和内部 Spark 元数据。Apache Spark 建议保留该属性默认值。</p></li><li><p><strong>spark.memory.storageFraction</strong>：这个分数表示，保留存储内存 R 的大小占总内存 M 的百分比。默认值为 0.5。意思是：如果 Spark 应用程序缓存的 RDD 位于 M 总内存的 50% 之内，将禁止被清除出去。注意：这个属性值越大，用于执行代码的内存越小，task 会频繁溢出数据到磁盘。Apache Spark 建议保留该属性默认值。如果在执行 Spark 应用程序过程中，频繁发生 GC，此时可以适当调低这个属性值，如从 0.5 调低到 0.4，这样 M 中的 60% 内存用于代码执行。如：</p><blockquote><p>spark-shell –conf spark.memory.storageFraction=0.4</p></blockquote></li></ul><h3 id="5-10-3-一个-Executor-究竟能使用多少内存？"><a href="#5-10-3-一个-Executor-究竟能使用多少内存？" class="headerlink" title="5.10.3 一个 Executor 究竟能使用多少内存？"></a>5.10.3 一个 Executor 究竟能使用多少内存？</h3><p>分配给 Executor 的内存是 Spark 应用程序执行的关键。假设你设置了 spark.executor.memory 的值为 4GB，那么 4GB内存中，有多少内存真正用于 Spark 应用程序代码执行呢？下面的分析显示了 executor 实际上可以使用多少内存执行代码。</p><ol><li><p>首先，Spark 会从 4GB Java heap 中减去 300MB，作为”reserved memeory”（保留内存）。现在 Java heap 中只剩下 4096-300= 3796MB 内存。</p></li><li><p>spark.memory.fraction 属性值，按默认值 0.75，将会使用内存 0.75*3796=2847MB。</p></li><li><p>spark.memory.storageFraction 属性值，默认值 0.5，那么用于存储内存的大小为：0.5*2847MB=1423.5MB，这是初始存储内存区域大小。</p></li><li><p>executor 可以使用另外的 50%，即 1423.5MB 执行应用程序代码。</p></li></ol><p>好好看看你的的应用程序并为存储内存和执行内存找出近似比例。然后调整 spark.memory.fraction 和 spark.memory.storageFraction 的属性值。Spark 建议您保留默认位置，因为它们是用于大多数应用程序场景。但是，每个 Spark 应用程序都是不同的。</p><h3 id="5-10-4-发现当前内存使用量"><a href="#5-10-4-发现当前内存使用量" class="headerlink" title="5.10.4 发现当前内存使用量"></a>5.10.4 发现当前内存使用量</h3><p>如果您想要计算 RDD 需要多少内存，最好的办法是创建 RDD 并缓存它。然后您可以查看 Spark web UI 存储页面统计数据，以计算 RDD 的当前内存使用量。</p><p>使用 SizeEstimator 类的 estimate 方法计算某个对象占内存空间的大小。具体用法参见：<br><a href="https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/util/SizeEstimator.html" target="_blank" rel="noopener">https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/util/SizeEstimator.html</a><br>您可以尝试各种存储、执行内存设置来优化内存使用。</p><h3 id="5-10-5-需要记住的几点"><a href="#5-10-5-需要记住的几点" class="headerlink" title="5.10.5 需要记住的几点"></a>5.10.5 需要记住的几点</h3><p>以下是您在配置用于 Sparkdriver 程序和 Executor 的资源时可能需要考虑的一些重要事项。</p><p><strong>1. 使用尝试和错误来调整 executor 内存</strong><br>如果分配给 Executor 过多的内存，GC 会耗时较长。然而，在许多情况线下，您需要为 Executor 分配更高的内存，因为默认值只有 1024MB。在一个特定应用程序调优中，你可以尝试升高不同的内存值，以找到合适的数字。大多数情况下，分配给 Executor 的内存不需要超过 6GB，大约 4GB 的大小对于一个 Executor 来说是一个很好的内存分配。任何分配内存太多，垃圾回收可能会对应用程序性能产生负面影响，尽管如果使用新的 GC 方法（G1GC），可以减轻这种情况。</p><p><strong>2. 限制每个 Executor 的 task 数量</strong></p><h2 id="5-11-Client-模式还是-Cluster-模式"><a href="#5-11-Client-模式还是-Cluster-模式" class="headerlink" title="5.11 Client 模式还是 Cluster 模式"></a>5.11 Client 模式还是 Cluster 模式</h2><p>在client 模式下，spark-submit 脚本创建了 driver，driver 与任务提交脚本运行在同一个进程中，此时，你可以方便的跟踪调试应用程序，因为应用程序的输出信息都会打印到屏幕上。如果在你的笔记本电脑上面运行 spark-submit 命令提交应用程序，然后关闭了笔记本电脑，那么这个应用程序就死掉；如果是在 cluster 模式下，这种情况不会影响应用程序的执行，因为你提交应用程序后，应用程序 driver 会在集群节点上运行。在 cluster 模式下，ResourceManager 决定 driver 进程运行在哪个集群节点上，不能人为指定。因此，需要保证在所有的集群节点上提供了所有的依赖库，如 jar 文件和 py 文件等。</p><p><strong>spark.driver.maxResultSize</strong> 配置参数决定了 spark action 操作中，RDD 所有分区的记录总数大小，如 collect()。默认值为 1GB，如果数据集过大，你需要调高此参数值，因为一旦超过了此参数值，job 会被 kill 掉。如果你调高了这个参数值，请确认同时调高 <strong>spark.driver.memory</strong>参数值。<strong>spark.driver.maxResultSize</strong> 参数值为 0，无限大。</p><p><strong>spark.cleaner.ttl</strong> 参数，定时清理 spark 程序运行过程中产生的元数据（stages 元数据或 task 元数据），对于长时间运行的 Spark Streaming 程序可以设置这个参数，以避免耗尽 driver 内存。</p><p><strong>配置 spark 相关的网络参数</strong><br>spark 使用 akka 框架进行网络通信。在生产环境中，有两个重要的网络配置参数可能需要调整：</p><ol><li><p><strong>spark.akka.framesize</strong>: 默认值为128MB，设置了 driver 和 executors 传输消息的最大值。如果 job 运行了大量的 map/reduce task，可能需要调整这个参数。</p></li><li><p><strong>spark.akka.threads</strong>：默认值为 4，设置了用于交互的 akka actor 线程数。如果 driver 配置了多个 cpu core，你可能需要提升这个参数值。</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-Spark-共享变量实战&quot;&gt;&lt;a href=&quot;#1-Spark-共享变量实战&quot; class=&quot;headerlink&quot; title=&quot;1. Spark 共享变量实战&quot;&gt;&lt;/a&gt;1. Spark 共享变量实战&lt;/h1&gt;&lt;p&gt;通常，Spark 程序计算的时候，我们传递的函数时在远程集群节点上执行的，在函数中使用的所有&lt;strong&gt;变量副本&lt;/strong&gt;会传递到远程节点，计算任务使用&lt;strong&gt;变量副本&lt;/strong&gt;进行计算。这些变量被复制到每台机器上，对远程机器上的变量的更新不会返回 driver 程序。&lt;/p&gt;
&lt;p&gt;跨任务支持通用的读写共享变量将是低效的。但是，Spark 为两种常见的使用模式提供了两种有限 功能的共享变量：&lt;strong&gt;广播变量&lt;/strong&gt; 和 &lt;strong&gt;累加器&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="笔记" scheme="https://miracle-xing.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Spark" scheme="https://miracle-xing.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 编程核心抽象——RDD</title>
    <link href="https://miracle-xing.github.io/2019/07/24/Spark-%E7%BC%96%E7%A8%8B%E6%A0%B8%E5%BF%83%E6%8A%BD%E8%B1%A1%E2%80%94%E2%80%94RDD/"/>
    <id>https://miracle-xing.github.io/2019/07/24/Spark-编程核心抽象——RDD/</id>
    <published>2019-07-23T18:27:29.000Z</published>
    <updated>2019-08-19T03:47:26.923Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-什么是RDD？"><a href="#1-什么是RDD？" class="headerlink" title="1. 什么是RDD？"></a>1. 什么是RDD？</h1><p>RDD 是 Resilient Distributed Dataset（<strong>弹性分布式数据集</strong>） 的简称。它是 Apache Spark 的基本数据结构。它是一个不可变的对象集合，在集群的不同节点上进行计算。</p><ul><li>Resilient: 即在 RDD lineage(DAG) 的帮助下具有容错能力，能够重新计算由于节点故障而丢失或损坏的数据分区。</li><li>Distributed: 数据分布在多个节点上。</li><li>Dataset: 表示所操作的数据集。用户可以通过 JDBC 从外部加载数据集，数据集可以是 JSON 文件，CSV 文件，文本文件或数据库。<br><br><a id="more"></a></li></ul><h1 id="2-RDD-的特点"><a href="#2-RDD-的特点" class="headerlink" title="2. RDD 的特点"></a>2. RDD 的特点</h1><ol><li><p><strong>内存计算</strong>：它将中间计算结果存储在分布式内存（RAM）中，而不是磁盘中。</p></li><li><p><strong>延迟计算</strong>：Apache Spark 中的所有 transformation 都是惰性的，因为它们不会立即计算结果，它们会记住应用于数据集的那些 transformation。直到 action 出现时，才会真正开始计算。</p></li><li><p><strong>容错性</strong>：Spark RDDs 能够容错，因为它们跟踪数据<strong>沿袭</strong>（lineage）信息，以便在故障时自动重建丢失的数据。</p></li><li><p><strong>不可变性</strong>：跨进程共享数据是安全的。它也可以在任何时候创建或检索，这使得缓存、共享和复制变得容易。因此，它是一种在计算中达到一致性的方法。</p></li><li><p><strong>分区性</strong>：partition 是 Spark RDD 中并行性的基本单元，每个分区都是数据的逻辑分区。Partition—task 一 一对应。</p></li><li><p><strong>持久化</strong>：用户可以声明他们将重用哪些 RDDs，并为它们选择存储策略。</p></li><li><p><strong>数据本地性</strong>：RDDs 能够定义计算分区的位置首选项。位置首选项是关于 RDD 位置的信息。</p></li></ol><h1 id="3-Spark-RDD-的操作类型"><a href="#3-Spark-RDD-的操作类型" class="headerlink" title="3. Spark RDD 的操作类型"></a>3. Spark RDD 的操作类型</h1><p>Apache Spark 中的 RDD 支持两种操作：</p><ul><li><strong>Transformation</strong></li><li><strong>Action</strong></li></ul><h2 id="3-1-Transformation-操作："><a href="#3-1-Transformation-操作：" class="headerlink" title="3.1 Transformation 操作："></a>3.1 Transformation 操作：</h2><p>Spark RDD transformation 操作是一个从现有的 RDD 生成新 RDD 的函数（方法、算子）。如：map(), filter(), reduceByKey()。</p><p>Transformation 操作都是<strong>延迟计算</strong>的操作。</p><p>有两种类型：<strong>窄变换、宽变换</strong>（窄依赖、宽依赖）。</p><ol><li><strong>窄依赖</strong>：它是map、filter 这样数据来自一个单独分区的操作。即输出 RDD 分区中的数据，来自父 RDD 中的单个分区。不需要 shuffle 操作就能解决。<br><img src="/images/2019/07/24/75185340-ad78-11e9-bb89-83337f9b9a34.png" alt="窄依赖.png"></li></ol><p><strong>窄依赖算子</strong>：map(), flatMap(), mapPartition(), filter(), sample(), union()</p><ol start="2"><li><strong>宽依赖</strong>：在子 RDD 单个分区中计算结果所需的数据可能存在于父 RDD 的多个分区中。类似 groupByKey() 和 reduceBykey() 这样的 transformation。宽依赖也称为 shuffle transformation。<br><img src="/images/2019/07/24/7bda84f0-ad78-11e9-bb89-83337f9b9a34.png" alt="宽依赖.png"></li></ol><p><strong>宽依赖算子</strong>：intersection(), distinct(), reduceByKey(), groupByKey(), join(), cartesian(), repartition(), coalesce()。</p><h2 id="3-2-action-操作"><a href="#3-2-action-操作" class="headerlink" title="3.2 action 操作"></a>3.2 action 操作</h2><p>Spark 中的 action 操作 ，返回 RDD 计算 的最终结果，<strong>其结果是一个值，而不是一个 RDD</strong>。</p><p>Action 触发血缘关系中 RDD 上的 transformation  操作的真正计算，计算结果返回 Driver端或者写入数据库。</p><p>这种设计使 Spark 运行更加高效。例如：map操作返回的数据集用于 reduce 操作，返回到 driver 端的只是 reduce 的结果值，而不是 map操作的数据集。</p><p><strong>常见的 Action</strong>：first(), take(), reduce(), collect(), the count()。</p><h1 id="4-创建-RDD"><a href="#4-创建-RDD" class="headerlink" title="4. 创建 RDD"></a>4. 创建 RDD</h1><p>三种创建 RDD 的方法：</p><ol><li>使用集合创建 RDD（<strong>parallelize</strong>）</li><li>使用已有 RDD 创建 RDD（<strong>父生子</strong>）</li><li>从外部数据源创建 RDD（<strong>textFile</strong>）</li></ol><p>在我们学习 Spark 的初始阶段 ，RDD 通常由集合创建的，即在 Driver 程序中创建集合并将其传递给 SparkContext 的 paralize() 方法。这种方法很少在 正式环境中使用，因为这种方法的整个数据集位于一台主机上。</p><p>首先实例化 SparkContext 对象：</p><p><strong>Scala</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val sparkConf = new SparkConf().setAppName(&quot;sc_wordcount&quot;).setMaster(&quot;local[*]&quot;)</span><br><span class="line">val sc = new SparkContext(sparkConf)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);</span><br><span class="line">JavaSparkContext jsc = new JavaSparkContext(conf);</span><br></pre></td></tr></table></figure><p>其中 appName 用于显示在 Spark 集群的 webUI上面。master 是一个 spark、YARN、mesos 集群 URL，或者是一个 local 字符串。<strong>实际项目中，在集群上运行时，不会对 master 进行硬编码。而是用 spark-submit 启动应用程序，并传递 master 给应用程序。但是，对于本地测试和单元测试，可以使用 local 运行 Spark</strong>。<br><img src="/images/2019/07/24/938a83c0-ad78-11e9-bb89-83337f9b9a34.png" alt="sparksubmit示例.png"></p><h2 id="4-1-使用集合创建-RDD"><a href="#4-1-使用集合创建-RDD" class="headerlink" title="4.1 使用集合创建 RDD"></a>4.1 使用集合创建 RDD</h2><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val data = Array(1, 2, 3, 4, 5)</span><br><span class="line">val distData = sc.paralize(data)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; data = Arrays.asList(1, 2, 3, 4, 5);</span><br><span class="line">JavaRDD&lt;Integer&gt; distData = sc.paralize(data);</span><br></pre></td></tr></table></figure><h2 id="4-2-从外部数据源创建-RDD"><a href="#4-2-从外部数据源创建-RDD" class="headerlink" title="4.2 从外部数据源创建 RDD"></a>4.2 从外部数据源创建 RDD</h2><p>Spark 可以从 Hadoop 支持的任何存储源创建分布式数据集，包括本地文件系统、HDFS、Cassandra、HBase、Amazon S3等。<br>Spark 支持文本文件、SequenceFiles 和任何其他 Hadoop InputFormat。</p><h3 id="4-2-1-读取本地文件"><a href="#4-2-1-读取本地文件" class="headerlink" title="4.2.1 读取本地文件"></a>4.2.1 读取本地文件</h3><p>文本文件 RDDs 可以使用 SparkContext 的 textFile 方法创建。此方法接受文件的 URI（机器上的本地文件路径、hdfs://、s3a://等URI），并将其作为行集合读取。下面是一个示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val distFile = sc.textFile(&quot;data.txt&quot;)</span><br></pre></td></tr></table></figure><p>一旦创建，就可以对 distFile 进行相应操作。例如，我们 可以将所有行的长度相加，使用 map 和 reduce 操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distFile.map(line=&gt;line.length).reduce(_+_)</span><br></pre></td></tr></table></figure><p><strong>关于用 Spark 读取文件的一些注意事项</strong>：</p><ol><li><p>如果使用本地文件系统上的路径，则必须在 worker 节点上的同一路径上，此文件可访问。要么将文件复制到所有 worker 上，要么使用一个挂载网络的共享文件系统。</p></li><li><p>Spark 所有基于文件的输入方法（textFile 等），支持在目录、压缩文件和通配符上运行，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">textFile(&quot;/my/directory&quot;), textFile(&quot;/my/directory/*.txt&quot;), textFile(&quot;/my/directory/*.gz))</span><br></pre></td></tr></table></figure></li><li><p>textFile 方法还接受一个可选的第二个参数，用于控制文件的分区数量。默认情况下，<strong>Spark 为文件的每个块创建一个分区（HDFS 中的块默认 是 128MB）</strong>，但是 您也可以通过传递更大的值来要求更高数量的分区。注意，分区数不能少于块数。</p></li></ol><p><strong>除了文本文件，Spark 的 Scala API 还支持其他几种数据格式</strong>：</p><ol><li><p><strong>SparkContext.wholeTextFile</strong> 允许您读取包含多个小文本文件的目录，并将它们作为（filename, content）的键值对返回。这与 textFile 不同，textFile 将在每个文件中每行返回一条 记录。分区由数据本地性决定，在某些情况下，数据本地性可能导致分区太少。对于这些情况，wholeTextFile 提供了控制最小分区数量的第二个可选 参数。</p></li><li><p>对于** SequenceFiles**，使用 SparkContext 的 sequenceFile[K, V]方法，其中K和V是文件中的键和值的类型。这些应该是 Hadoop Writable 接口的子类，比如 IntWritable 和 Text。</p></li><li><p>对于其他 Hadoop inputformat，您可以使用  SparkContext.hadoopRDD方法，它接受任意的 JobConf 和 输入格式类、键类和值类。将这些设置为与使用输入源 Hadoop 作业相同的方式。还可以使用 SparkContext。基于“new”MapReduce API（org.apache.hadoop.mapreduce）的 inputformat 的 newAPIHadoopRDD。</p></li></ol><h3 id="4-2-2-读取-HDFS-上的数据"><a href="#4-2-2-读取-HDFS-上的数据" class="headerlink" title="4.2.2 读取 HDFS 上的数据"></a>4.2.2 读取 HDFS 上的数据</h3><ol><li>启动 HDFS</li><li>读取 HDFS 上的数据<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val textFileRDD = sc.textFile(&quot;hdfs://bigdata01:9000/textdata/order.txt&quot;)</span><br><span class="line">val count = textFileRDD.count()</span><br><span class="line">println(&quot;count:&quot; + count)</span><br></pre></td></tr></table></figure></li></ol><h3 id="4-2-3-提交应用程序到-Spark-集群"><a href="#4-2-3-提交应用程序到-Spark-集群" class="headerlink" title="4.2.3 提交应用程序到 Spark 集群"></a>4.2.3 提交应用程序到 Spark 集群</h3><ol><li><p>打包应用程序</p></li><li><p>上传 jar 包到服务器</p></li><li><p>运行 spark-submit 命令<br>./spark-submit –class sparkcore.learnTextFile –deploy-mode client /opt/sparkapp/learnTextFile.jar</p></li></ol><h3 id="4-2-4-配置并启动-Spark-History-Server"><a href="#4-2-4-配置并启动-Spark-History-Server" class="headerlink" title="4.2.4 配置并启动 Spark History Server"></a>4.2.4 配置并启动 Spark History Server</h3><ol><li><p>重命名 conf/spark-deaults.conf.template 为 conf/spark-defaults.conf</p></li><li><p>修改 spark-defaults.conf 配置文件，并同步到其他节点。<br>修改前：<br><img src="/images/2019/07/24/323ce350-ad79-11e9-bb89-83337f9b9a34.png" alt="spark historyserver 配置1.png"><br>修改后（注意：hdfs 目录要先创建）：<br><img src="/images/2019/07/24/36b43aa0-ad79-11e9-bb89-83337f9b9a34.png" alt="spark historyserver 配置2.png"></p></li><li><p>启动 ./start-history-server.sh<br><img src="/images/2019/07/24/3b44e650-ad79-11e9-bb89-83337f9b9a34.png" alt="spark historyserver 配置3.png"></p></li><li><p>访问 <a href="http://bigdata01:18080/" target="_blank" rel="noopener">http://bigdata01:18080/</a> webUI </p></li></ol><h1 id="5-向-Spark-算子传递函数"><a href="#5-向-Spark-算子传递函数" class="headerlink" title="5. 向 Spark 算子传递函数"></a>5. 向 Spark 算子传递函数</h1><p>Spark API 严重依赖于将 dirver 程序中的函数传递到集群上运行</p><p><strong>Scala</strong>:<br>推荐使用如下两种方式实现函数的传递：</p><ol><li>匿名函数语法，可用于短代码段。</li><li>全局单例对象中的静态方法。例如，您可以定义对象 MyFunctions，然后传递 MyFunctions.func1，如下所示：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">object MyFunctions&#123;</span><br><span class="line">    def func1(s:String): String = &#123;...&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">myRDD.map(MyFunctions.func1)</span><br></pre></td></tr></table></figure></li></ol><p><strong>Java</strong>:<br>在 Java 中，函数由实现 org.apacche.spark.api.java.function 包中的接口的类表示。<br><img src="/images/2019/07/24/a6005070-ad78-11e9-bb89-83337f9b9a34.png" alt="Java 函数编程1.png"></p><p><img src="/images/2019/07/24/ab566f00-ad78-11e9-bb89-83337f9b9a34.png" alt="Java 函数编程2.png"></p><p>有两种方法可以创建这样的函数：</p><ol><li>可以是匿名内部类</li><li>也可以是创建 类实现相应接口，并将其实例传递给 Spark</li></ol><p>使用<strong>lambda 表达式</strong>简洁的定义实现。<br>例如，可以这样编写代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;data.txt&quot;);</span><br><span class="line">JavaRDD&lt;Integer&gt; lineLengths = lines.map(new Function&lt;String, Integer&gt;()&#123;</span><br><span class="line">    public Integer call(String s) &#123;return s.length();&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">int totalLength = lineLengths.reduce(new Function2&lt;Integer,  Integer, Integer&gt;()&#123;</span><br><span class="line">    public Integer call(Integer a, Integer b)&#123;return a + b;&#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>或者 可以这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class GetLength implements Function&lt;String, Integer&gt;&#123;</span><br><span class="line">    public Integer call(String s)&#123; return s.length();&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Sum implements Function2&lt;Integer, Integer, Integer&gt; &#123;</span><br><span class="line"></span><br><span class="line">  public Integer call(Integer a, Integer b) &#123; return a + b; &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;data.txt&quot;);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; lineLengths = lines.map(new GetLength());</span><br><span class="line"></span><br><span class="line">int totalLength = lineLengths.reduce(new Sum());</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：Java 中的匿名内部类也可以访问闭包作用域中的变量，只要它们被标记为 final。 Spark 将把这些变量的副本发送到每个 worker 节点上。</p><h1 id="6-Spark-算子实战——transformation"><a href="#6-Spark-算子实战——transformation" class="headerlink" title="6. Spark 算子实战——transformation"></a>6. Spark 算子实战——transformation</h1><h2 id="6-1-map-和-flatMap-算子"><a href="#6-1-map-和-flatMap-算子" class="headerlink" title="6.1 map 和 flatMap 算子"></a>6.1 map 和 flatMap 算子</h2><p><strong>map()</strong>: 将传入的函数应用于RDD 中的每一条记录，返回由函数结果组成的新 RDD。函数的结果值是一个对象，不是一个集合。</p><p><strong>flatMap()</strong>: 与map() 操作类似。但是传入 flatMap() 的函数可以返回 0个、1个或多个结果值。即函数结果值是一个集合，而不是一个对象。</p><p><strong>map操作 Scala版本</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val textFileRDD = sc.textFile(&quot;in/README.md&quot;)</span><br><span class="line">val uppercaseRDD = textFileRDD.map(line=&gt;line.toUpperCase)</span><br><span class="line">for ( elem &lt;- uppercaseRDDD.take(3))&#123;</span><br><span class="line">    println(elem)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>map操作 Java版本</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; javaRDD = jsc.textFile(&quot;in/README.md&quot;);</span><br><span class="line">JavaRDD&lt;String&gt; map = javaRDD.map(line -&gt; line.toUpperCase());</span><br><span class="line">for (String line : map.take(3)) &#123;</span><br><span class="line">    System.out.println(line);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>flatMap操作 Scala版本</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val flatMapRDD = textFileRDD.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">flatMapRDD.take(3).foreach(println)</span><br></pre></td></tr></table></figure><p><strong>flatMap操作 Java版本</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; javaRDD = jsc.textFile(&quot;in/README.md&quot;);</span><br><span class="line">JavaRDD&lt;String&gt; wordsRDD = javaRDD.flatMap(line -&gt; (Arrays.asList(line.split(&quot; &quot;)).iterator()));</span><br><span class="line">wordsRDD.take(3).forEach(word-&gt; System.out.println(word));</span><br></pre></td></tr></table></figure><p><img src="/images/2019/07/24/b9d781e0-ad78-11e9-bb89-83337f9b9a34.png" alt="flatMap执行过程.png"><br>从Spark map() 和 flatMap() 的比较中可以看出，Spark map函数表达的是一对一的转换。它将集合的每个数据元素转换为结果集合的一个数据元素。而Spark flatMap 函数表示一对多的转换。它将每个元素转换为 0 或更多的元素。</p><h2 id="6-2-filter-算子"><a href="#6-2-filter-算子" class="headerlink" title="6.2 filter 算子"></a>6.2 filter 算子</h2><p>Spark RDD filter() 函数返回一个新的 RDD，<strong>只包含满足过滤条件的元素</strong>。这是一个<strong>窄依赖</strong>的操作，不会将数据从一个分区转移到其他分区。——不会发生shuffle。<br>例如，假设 RDD 包含5个整数（1, 2, 3, 4, 5），过滤条件是 判断是否偶数。过滤后得到的 RDD将只包含偶数，即 2 和 4。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val filterRDD = fileRDD.filter(line =&gt; line.contains(&quot;Spark&quot;))</span><br><span class="line">filterRDD.foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; filterRDD = javaRDD.filter(line -&gt; line.contains(&quot;Spark&quot;));</span><br><span class="line">filterRDD.foreach(line -&gt; System.out.println(line));</span><br></pre></td></tr></table></figure><h2 id="6-3-distinct-算子"><a href="#6-3-distinct-算子" class="headerlink" title="6.3 distinct 算子"></a>6.3 distinct 算子</h2><p>返回 RDD 中的非重复记录。注意：此操作是昂贵的，因为它需要对数据进行 shuffle。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val distintRDD = sc.parallelize(Seq(1,2,3,4,5,1,2,3,4)).distinct()</span><br><span class="line">distintRDD.foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; numsRDD = jsc.parallelize(Arrays.asList(1, 2, 3, 4, 5, 1, 2, 3, 4));</span><br><span class="line">JavaRDD&lt;Integer&gt; distinctRDD = numsRDD.distinct();</span><br><span class="line">distinctRDD.foreach(ele -&gt; System.out.println(ele));</span><br></pre></td></tr></table></figure><h2 id="6-4-mapPartitions"><a href="#6-4-mapPartitions" class="headerlink" title="6.4 mapPartitions"></a>6.4 mapPartitions</h2><p>在 mapPartition() 函数中，map() 函数同时应用于每个 partition 分区。对比学习 foreachPartition() 函数，foreachPartition() 是一个action 算子，操作方式与 mapPartition相同。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val mapPartitionRDD = fileRDD.mapPartitions(partition =&gt; &#123;</span><br><span class="line">  // map 每一个分区，然后再 map 分区中的每一个元素</span><br><span class="line">  partition.map(line =&gt; line.toUpperCase())</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">// foreach 是一个没有返回值的 action</span><br><span class="line">mapPartitionRDD.foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; mapPartitionsRDD = javaRDD.mapPartitions(stringIterator -&gt; &#123;</span><br><span class="line">    List&lt;String&gt; list = new ArrayList&lt;&gt;();</span><br><span class="line">    while (stringIterator.hasNext()) &#123;</span><br><span class="line">        list.add(stringIterator.next().toUpperCase());</span><br><span class="line">    &#125;</span><br><span class="line">    return list.iterator();</span><br><span class="line">&#125;);</span><br><span class="line">mapPartitionsRDD.foreach(line-&gt; System.out.println(line));</span><br></pre></td></tr></table></figure><h2 id="6-5-mapPartitionWithIndex"><a href="#6-5-mapPartitionWithIndex" class="headerlink" title="6.5 mapPartitionWithIndex()"></a>6.5 mapPartitionWithIndex()</h2><p>就像 mapPartition，除了 mapPartition外，它还为传入的函数提供了一个整数值，表示<strong>分区的索引</strong>，map()在分区索引上依次应用。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val mapPartitionRDD = fileRDD.mapPartitionsWithIndex((index, partition) =&gt; &#123;</span><br><span class="line">  partition.map(line =&gt; index + line.toUpperCase())</span><br><span class="line">&#125;)</span><br><span class="line">mapPartitionRDD.foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; mapPartitionsRDD = javaRDD.mapPartitionsWithIndex((index, stringIterator) -&gt; &#123;</span><br><span class="line">    List&lt;String&gt; list = new ArrayList&lt;&gt;();</span><br><span class="line">    while (stringIterator.hasNext()) &#123;</span><br><span class="line">        list.add(index + stringIterator.next().toUpperCase());</span><br><span class="line">    &#125;</span><br><span class="line">    return list.iterator();</span><br><span class="line">&#125;, false);</span><br><span class="line">mapPartitionsRDD.foreach(line -&gt; System.out.println(line));</span><br></pre></td></tr></table></figure><h2 id="6-6-union-并集"><a href="#6-6-union-并集" class="headerlink" title="6.6 union 并集"></a>6.6 union 并集</h2><p>使用 union() 函数，我们可以在新的 RDD 中获得两个 RDD 的元素。<strong>这个函数的关键规则是两个RDDs 属于同一类型</strong>。例如，RDD1 的元素是（Spark, Spark, Hadoop, Flink），而 RDD2 的元素是（Big data, Spark, Flink），所以结果 union(rdd1.union) 有元素（Spark, Spark, Spark, Hadoop, Flink, Flikn, Big data）</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val RDD1 = sc.parallelize(Seq(1,2,3,4))</span><br><span class="line">val RDD2 = sc.parallelize(Seq(1,2,3,4))</span><br><span class="line">val unionRDD = RDD1.union(RDD2)</span><br><span class="line">unionRDD.foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; RDD1 = jsc.parallelize(Arrays.asList(1, 2, 3, 4));</span><br><span class="line">JavaRDD&lt;Integer&gt; RDD2 = jsc.parallelize(Arrays.asList(1, 2, 3, 4));</span><br><span class="line">JavaRDD&lt;Integer&gt; unionRDD = RDD1.union(RDD2);</span><br><span class="line">unionRDD.foreach(num -&gt; System.out.println(num));</span><br></pre></td></tr></table></figure><h2 id="6-7-intersection（交集）"><a href="#6-7-intersection（交集）" class="headerlink" title="6.7 intersection（交集）"></a>6.7 intersection（交集）</h2><p>使用 intersection() 函数，我们只得到新 RDD 中的两个 RDD 的公共元素。<strong>这个函数的关键规则是这两个 RDDs 应该是同一类型的</strong>。</p><p>举个例子，RDD1 的元素是（Spark, Spark, Hadoop,  Flink)，RDD2的元素是（Big data, Spark, Flink) 交集（RDD1.intersection(RDD2))将包含元素（Spark）。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val RDD1 = sc.parallelize(Seq(1,2,3,4))</span><br><span class="line">val RDD2 = sc.parallelize(Seq(1,2,3,4))</span><br><span class="line">val intersectionRDD = RDD1.intersection(RDD2)</span><br><span class="line">intersectionRDD.foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; RDD1 = jsc.parallelize(Arrays.asList(1, 2, 3, 4));</span><br><span class="line">JavaRDD&lt;Integer&gt; RDD2 = jsc.parallelize(Arrays.asList(1, 2, 3, 4));</span><br><span class="line">JavaRDD&lt;Integer&gt; intersectionRDD = RDD1.intersection(RDD2);</span><br><span class="line">intersectionRDD.foreach(num -&gt; System.out.println(num));</span><br></pre></td></tr></table></figure><h2 id="6-8-PairRDD"><a href="#6-8-PairRDD" class="headerlink" title="6.8 PairRDD"></a>6.8 PairRDD</h2><p>现实生活中的许多数据集通常是<strong>键值对</strong>形式的。例如：包含课程名称和选修课程的学生名单的数据集。<br>这种数据集的典型模式是每一行都是一个key映射到一个或多个value。为此，Spark提供了一个名为 PairRDD 的数据结构，而不是常规的 RDD。这使得处理此类数据更加简单和高效。</p><p>PairRDD 是一种特殊类型的 RDD，可以存储 键-值对</p><p><strong>创建 PairRDD</strong>:</p><ol><li>通过键值数据结构列表构建 Pair RDD。键值数据结构称为 tuple2 元组。（Java 语言没有内置的 tuple类型，所以Spark 的 Java API 允许用户使用 scala.Tuple2 类创建元组）。</li></ol><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val tuple = List((&quot;张三&quot;, &quot;语文&quot;), (&quot;李四&quot;, &quot;数学&quot;), (&quot;王五&quot;, &quot;英语&quot;))</span><br><span class="line">val pairRDD = sc.parallelize(tuple)pairRDD.foreach(t =&gt; println(t._1 + &quot;: &quot; + t._2))</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Tuple2&lt;String, String&gt;&gt; tuple2s = Arrays.asList(new Tuple2&lt;&gt;(&quot;张三&quot;, &quot;语文&quot;),</span><br><span class="line">        new Tuple2&lt;&gt;(&quot;李四&quot;, &quot;数学&quot;),</span><br><span class="line">        new Tuple2&lt;&gt;(&quot;王五&quot;, &quot;英语&quot;));</span><br><span class="line">JavaPairRDD&lt;String, String&gt; pairRDD = jsc.parallelizePairs(tuple2s);</span><br><span class="line">pairRDD.foreach(t -&gt; System.out.println(t._1 + &quot;: &quot; + t._2));</span><br></pre></td></tr></table></figure><ol start="2"><li>将一个常规的 RDD 转换为 PairRDD</li></ol><p><strong>Scala</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val regularRDD = sc.parallelize(List(&quot;张三 语文&quot;, &quot;李四 数学&quot;, &quot;王五 英语&quot;))</span><br><span class="line">val pairRDD = regularRDD.map(item =&gt; (item.split(&quot; &quot;)(0), item.split(&quot; &quot;)(1)))</span><br><span class="line">pairRDD.foreach(item =&gt; println(item._1 + &quot;: &quot; + item._2))</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; parallelizeRDD = jsc.parallelize(Arrays.asList(&quot;张三 语文&quot;, &quot;李四 数学&quot;, &quot;王五 英语&quot;));</span><br><span class="line">JavaPairRDD&lt;String, String&gt; pairRDD = parallelizeRDD.mapToPair(item -&gt; new Tuple2&lt;&gt;(item.split(&quot; &quot;)[0], item.split(&quot; &quot;)[1]));</span><br><span class="line">pairRDD.foreach(t -&gt; System.out.println(t._1 + &quot;: &quot; + t._2));</span><br></pre></td></tr></table></figure><h3 id="6-8-1-PairRDD-上的-transformation-操作"><a href="#6-8-1-PairRDD-上的-transformation-操作" class="headerlink" title="6.8.1 PairRDD 上的 transformation 操作"></a>6.8.1 PairRDD 上的 transformation 操作</h3><p>PairRDDs 允许使用常规 RDDs 可用的所有转换，支持与常规 RDDs 相同功能。<br>由于 PairRDDs 包含元组，所以我们 需要传递操作元组而不是 单个元素的函数给 Spark。</p><p><strong>1.  filter</strong><br>在 pairRDD 上使用 filter transformation:</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val regularRDD = sc.parallelize(List(&quot;张三 语文&quot;, &quot;李四 数学&quot;, &quot;王五 英语&quot;))</span><br><span class="line">val pairRDD = regularRDD.map(item =&gt; (item.split(&quot; &quot;)(0), item.split(&quot; &quot;)(1)))</span><br><span class="line"></span><br><span class="line">val filterPairRDD = pairRDD.filter(t =&gt; t._2.equals(&quot;语文&quot;))</span><br><span class="line">filterPairRDD.foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; parallelizeRDD = jsc.parallelize(Arrays.asList(&quot;张三 语文&quot;, &quot;李四 数学&quot;, &quot;王五 英语&quot;));</span><br><span class="line">JavaPairRDD&lt;String, String&gt; pairRDD = parallelizeRDD.mapToPair(item -&gt; new Tuple2&lt;&gt;(item.split(&quot; &quot;)[0], item.split(&quot; &quot;)[1]));</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, String&gt; filterRDD = pairRDD.filter(t -&gt; t._2.equals(&quot;数学&quot;));</span><br><span class="line">filterRDD.foreach(t -&gt; System.out.println(t));</span><br></pre></td></tr></table></figure><p><strong>2. reduceByKey—另一个版本的 wordcount</strong></p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val fileRDD = sc.textFile(&quot;in/README.md&quot;)</span><br><span class="line">  .flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">  .map(word =&gt; (word, 1))</span><br><span class="line">  .reduceByKey(_ + _)</span><br><span class="line">  .sortBy(_._2, false)</span><br><span class="line">  .collect()</span><br><span class="line">  .foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; javaRDD = jsc.textFile(&quot;in/README.md&quot;);</span><br><span class="line">JavaRDD&lt;String&gt; wordsRDD = javaRDD.flatMap(line -&gt; Arrays.asList(line.split(&quot; &quot;)).iterator());</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD = wordsRDD.mapToPair(word -&gt; new Tuple2&lt;&gt;(word, 1));</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; wordCounts = javaPairRDD.reduceByKey((a, b) -&gt; (a + b));</span><br><span class="line">JavaRDD&lt;Tuple2&lt;Integer, String&gt;&gt; tuple2JavaRDD = wordCounts.map(t -&gt; new Tuple2&lt;&gt;(t._2, t._1));</span><br><span class="line">JavaRDD&lt;Tuple2&lt;Integer, String&gt;&gt; tuple2JavaRDD1 = tuple2JavaRDD.sortBy(t -&gt; t._1, false, 1);</span><br><span class="line">JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; sortedRDD = tuple2JavaRDD1.map(t -&gt; new Tuple2&lt;&gt;(t._2, t._1));</span><br><span class="line">sortedRDD.foreach(t-&gt; System.out.println(t));</span><br></pre></td></tr></table></figure><p><strong>3. combineByKey</strong><br>combineByKey 是 Spark 中一个核心的高级函数，其他一些 键值对函数底层都是用它实现的。如 groupByKey, reduceByKey 等。</p><p>例：计算平均分数（Scala）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">object learnCombineBeKey &#123;</span><br><span class="line"></span><br><span class="line">  case class ScoreDetail(studentName: String, subject: String, score: Float)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sparkSession = SparkSession.builder()</span><br><span class="line">      .master(&quot;local[*]&quot;)</span><br><span class="line">      .appName(&quot;learnCombineBeKey&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    val sc = sparkSession.sparkContext</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * https://www.edureka.co/blog/apache-spark-combinebykey-explained</span><br><span class="line">      *</span><br><span class="line">      * combineByKey transformation</span><br><span class="line">      * combineByKey API 有三个函数</span><br><span class="line">      * Create combiner function: x</span><br><span class="line">      * Merge value function: y</span><br><span class="line">      * Merger combiners function: z</span><br><span class="line">      *</span><br><span class="line">      * API 格式为 combineByKey(x, y, z)</span><br><span class="line">      * 让我们看一个例子（Scala语言）：本例的目标是找到每个学生的平均分数</span><br><span class="line">      */</span><br><span class="line"></span><br><span class="line">    val scores = List(</span><br><span class="line">      ScoreDetail(&quot;A&quot;, &quot;Math&quot;, 98),</span><br><span class="line">      ScoreDetail(&quot;A&quot;, &quot;English&quot;, 66),</span><br><span class="line">      ScoreDetail(&quot;B&quot;, &quot;Math&quot;, 74),</span><br><span class="line">      ScoreDetail(&quot;B&quot;, &quot;English&quot;, 80),</span><br><span class="line">      ScoreDetail(&quot;C&quot;, &quot;Math&quot;, 98),</span><br><span class="line">      ScoreDetail(&quot;C&quot;, &quot;English&quot;, 96),</span><br><span class="line">      ScoreDetail(&quot;D&quot;, &quot;Math&quot;, 100),</span><br><span class="line">      ScoreDetail(&quot;D&quot;, &quot;English&quot;, 95)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 将测试数据转换为键值对形式--键key为学生名称Student Name，值为ScoreDetail 实例对象</span><br><span class="line">      */</span><br><span class="line">    val scoreWithKey = for (i &lt;- scores) yield (i.studentName, i)</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 创建一个 pairRDD</span><br><span class="line">      */</span><br><span class="line">    val scoreWithKeyRDD = sc.parallelize(scoreWithKey)</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 计算 平均分数</span><br><span class="line">      */</span><br><span class="line">    val avgScoresRDD = scoreWithKeyRDD.combineByKey(</span><br><span class="line">      (x: ScoreDetail) =&gt; (x.score, 1),</span><br><span class="line">      (acc: (Float, Int), x: ScoreDetail) =&gt; (acc._1 + x.score, acc._2 + 1),</span><br><span class="line">      (acc1: (Float, Int), acc2: (Float, Int)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2)</span><br><span class="line">    ).map(&#123;</span><br><span class="line">      case (key, value) =&gt; (key, value._1 / value._2)</span><br><span class="line">    &#125;)</span><br><span class="line">    avgScoresRDD.foreach(println)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>4. sortByKey</strong><br>当我们在（K, V）数据集中应用 sortByKey() 函数时，数据是根据 RDD 中的键 K 排序的。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val data =sc.parallelize(Seq((&quot;maths&quot;,52), (&quot;english&quot;,75), (&quot;science&quot;,82), (&quot;computer&quot;,65), (&quot;maths&quot;,85)))</span><br><span class="line">val sorted = data.sortByKey()</span><br><span class="line">sorted.collect().foreach(println)</span><br></pre></td></tr></table></figure><p><strong>注</strong>：在上面的代码中，sortByKey() 将数据 RDD的 key(String) 按升序排序。</p><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2s = Arrays.asList(new Tuple2&lt;&gt;(&quot;maths&quot;, 52),</span><br><span class="line">        new Tuple2&lt;&gt;(&quot;english&quot;, 75),</span><br><span class="line">        new Tuple2&lt;&gt;(&quot;science&quot;, 82),</span><br><span class="line">        new Tuple2&lt;&gt;(&quot;computer&quot;, 65),</span><br><span class="line">        new Tuple2&lt;&gt;(&quot;maths&quot;, 85));</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD = jsc.parallelizePairs(tuple2s);</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD1 = javaPairRDD.sortByKey();</span><br><span class="line">javaPairRDD1.collect().forEach(t -&gt; System.out.println(t));</span><br></pre></td></tr></table></figure><p><strong>5. join</strong><br>join 是数据库术语。它使用公共值组合两个表中的字段。Spark 中的 join() 操作是在 pairRDD 上定义的。pairRDD 每个元素都以 tuple 的形式出现。tuple 第一个元素是 key，第二个元素是 value。join() 操作根据 key 组合两个数据集。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val data1 = sc.parallelize(Array((&apos;A&apos;, 1), (&apos;B&apos;, 2)))</span><br><span class="line">val data2 = sc.parallelize(Array((&apos;A&apos;, 4), (&apos;A&apos;, 6), (&apos;b&apos;, 7), (&apos;c&apos;, 3), (&apos;c&apos;, 8)))</span><br><span class="line">val result = data1.join(data2)</span><br><span class="line">println(result.collect().mkString(&quot;,&quot;)) // (A,(1,4)),(A,(1,6))</span><br></pre></td></tr></table></figure><h1 id="7-Spark-算子实战–action"><a href="#7-Spark-算子实战–action" class="headerlink" title="7. Spark 算子实战–action"></a>7. Spark 算子实战–action</h1><h2 id="7-1-count"><a href="#7-1-count" class="headerlink" title="7.1 count"></a>7.1 count</h2><p>count() 返回 RDD 中的元素数量。</p><h2 id="7-2-take"><a href="#7-2-take" class="headerlink" title="7.2 take"></a>7.2 take</h2><p>从 RDD 返回 n 个元素。它试图减少它访问的分区数量，不能使用此方法来控制访问元素的顺序。</p><h2 id="7-3-top"><a href="#7-3-top" class="headerlink" title="7.3 top"></a>7.3 top</h2><p>如果 RDD 中元素有序，那么可以使用 top() 从 RDD 中提取前几个元素。</p><p><strong>Scala</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val fileRDD = sc.textFile(&quot;in/README.md&quot;)</span><br><span class="line">val lengthRDD = fileRDD.map(line =&gt; (line,line.length))</span><br><span class="line">lengthRDD.top(3).foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; javaRDD = jsc.textFile(&quot;in/README.md&quot;);</span><br><span class="line">javaRDD.top(3).forEach(item -&gt; System.out.println(item));</span><br></pre></td></tr></table></figure><h2 id="7-4-countByValue"><a href="#7-4-countByValue" class="headerlink" title="7.4 countByValue"></a>7.4 countByValue</h2><p>countByValue()  返回，每个元素都出现在 RDD 中的次数。例如：<br>RDD 中的元素{1, 2, 2, 3, 4, 5, 5, 6}，“rdd.countByValue()” -&gt; {(1,1), (2,2), (3,1), (4,1), (5,2), (6,1)}，返回一个 HashMap(K, Int) ，包括每个 key 的计数。</p><p><strong>Scala</strong>: wordcount 另一种实现</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val fileRDD = sc.textFile(&quot;in/README.md&quot;)</span><br><span class="line">fileRDD.flatMap(line =&gt; line.split(&quot; &quot;)).countByValue().foreach(println)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; javaRDD = jsc.textFile(&quot;in/README.md&quot;);</span><br><span class="line">JavaRDD&lt;String&gt; javaRDD1 = javaRDD.flatMap(line -&gt; Arrays.asList(line.split(&quot; </span><br><span class="line">&quot;)).iterator());javaRDD1.countByValue().forEach((key, value) -&gt; System.out.println(key + &quot;,&quot; + </span><br><span class="line">value));</span><br></pre></td></tr></table></figure><h2 id="7-5-reduce"><a href="#7-5-reduce" class="headerlink" title="7.5 reduce"></a>7.5 reduce</h2><p>reduce() 函数将 RDD 的两个元素作为输入，然后生成与输入元素相同类型的输出。这种函数的简单形式就是一个加法。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List(12,13,14,43,53,65,34))</span><br><span class="line">val sum = rdd1.reduce(_+_)</span><br><span class="line">println(sum)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; parallelizeRDD = jsc.parallelize(Arrays.asList(12, 13, 14, 43, 53, 65, 34));</span><br><span class="line">int sum = parallelizeRDD.reduce((a, b) -&gt; a + b);</span><br><span class="line">System.out.println(sum);</span><br></pre></td></tr></table></figure><h2 id="7-6-collect"><a href="#7-6-collect" class="headerlink" title="7.6 collect"></a>7.6 collect</h2><p>collect() 是将整个 RDDs 内容返回给 driver 程序的常见且最简单的操作。collect() 的应用是<strong>单元测试</strong>，在单元测试中，期望整个 RDD 能够装入内存。如果使用了 collect 方法，但是 driver 内存不够，则会内存溢出。</p><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val data1 = sc.parallelize(Array((&apos;A&apos;, 1), (&apos;B&apos;, 2)))</span><br><span class="line">val data2 = sc.parallelize(Array((&apos;A&apos;, 4), (&apos;A&apos;, 6), (&apos;b&apos;, 7), (&apos;c&apos;, 3), (&apos;c&apos;, 8)))</span><br><span class="line">val result = data1.join(data2)</span><br><span class="line">println(result.collect().mkString(&quot;,&quot;)) // (A,(1,4)),(A,(1,6))</span><br></pre></td></tr></table></figure><h2 id="7-7-foreach-（无返回值）"><a href="#7-7-foreach-（无返回值）" class="headerlink" title="7.7 foreach （无返回值）"></a>7.7 foreach （无返回值）</h2><p>当我们希望对 RDD 的每个元素应用操作，但它不应该返回值给 driver 程序时。在这种情况下，foreach() 函数是非常合适的。例如，向输入库插入一条记录。</p><h2 id="7-8-foreachParitition（无返回值）"><a href="#7-8-foreachParitition（无返回值）" class="headerlink" title="7.8 foreachParitition（无返回值）"></a>7.8 foreachParitition（无返回值）</h2><p>类似 mapPartitions, 区别在于：1、foreachPartition 是 action 操作 2、foreachPartition 函数没有返回值（返回值是unit）。</p><h2 id="7-9-作业："><a href="#7-9-作业：" class="headerlink" title="7.9 作业："></a>7.9 作业：</h2><ol><li>航班数最多的航空公司，算出前 6 名。</li><li>北京飞往重庆的航空公司，有多少个？</li></ol><p><strong>数据格式</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">阿克苏,41.188341,80.293842,北京,39.92998578,116.395645,3049,CA1276,中国国航,JET,15:40,21:40,阿克苏机场,41.26940127,80.30091874,首都机场,40.06248537,116.5992671,63%,42分钟,1,0,1,0,1,0,1</span><br></pre></td></tr></table></figure><ol><li>航班数最多的航空公司，算出前 6 名。</li></ol><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val textFileRDD = sc.textFile(&quot;in/Flight.csv&quot;)</span><br><span class="line">val airlinesRDD = textFileRDD.map(line =&gt; (line.split(&quot;,&quot;)(8), 1))</span><br><span class="line">val resRDD = airlinesRDD.reduceByKey(_ + _)</span><br><span class="line">val list1 = resRDD.sortBy(_._2, false)</span><br><span class="line">  .collect()</span><br><span class="line">// 取前6名</span><br><span class="line">for (i &lt;- 0 to 5) &#123;</span><br><span class="line">  println(list1(i))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; textFileRDD = jsc.textFile(&quot;in/Flight.csv&quot;);</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD = textFileRDD.mapToPair(line -&gt; new Tuple2&lt;&gt;(line.split(&quot;,&quot;)[8], 1));</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD1 = javaPairRDD.reduceByKey((a, b) -&gt; (a + b));</span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; integerStringJavaPairRDD = javaPairRDD1.mapToPair(t -&gt; new Tuple2&lt;&gt;(t._2, t._1));</span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; integerStringJavaPairRDD1 = integerStringJavaPairRDD.sortByKey(false);</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD2 = integerStringJavaPairRDD1.mapToPair(t -&gt; new Tuple2&lt;&gt;(t._2, t._1));</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;String, Integer&gt;&gt; collect = javaPairRDD2.collect();</span><br><span class="line">for (int i = 0; i &lt; 5; i++) &#123;</span><br><span class="line">    System.out.println(collect.get(i));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>北京飞往重庆的航空公司，有多少个？</li></ol><p><strong>Scala</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val BJ_CQ_RDD = textFileRDD.filter(line =&gt; (line.split(&quot;,&quot;)(0).equals(&quot;北京&quot;) &amp;&amp; line.split(&quot;,&quot;)(3).equals(&quot;重庆&quot;)))</span><br><span class="line">val count = BJ_CQ_RDD.map(line =&gt; (line.split(&quot;,&quot;)(8), 1)).countByKey().size</span><br><span class="line">println(count)</span><br></pre></td></tr></table></figure><p><strong>Java</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; filterRDD = textFileRDD.filter(line -&gt; (line.split(&quot;,&quot;)[0].equals(&quot;北京&quot;) &amp;&amp; line.split(&quot;,&quot;)[3].equals(&quot;重庆&quot;)));</span><br><span class="line">int size = filterRDD.mapToPair(line -&gt; new Tuple2&lt;&gt;(line.split(&quot;,&quot;)[8], 1)).countByKey().size();</span><br><span class="line">System.out.println(size);</span><br></pre></td></tr></table></figure><h1 id="8-Spark-RDD-分区实战"><a href="#8-Spark-RDD-分区实战" class="headerlink" title="8. Spark RDD 分区实战"></a>8. Spark RDD 分区实战</h1><h2 id="8-1-RDD-partition-概念"><a href="#8-1-RDD-partition-概念" class="headerlink" title="8.1 RDD partition 概念"></a>8.1 RDD partition 概念</h2><p>我们处理大数据时，由于数据量太大，以至于单个节点无法完全存储、计算。所以这些数据需要分割成多个数据块 block，以利用多个集群节点的存储、计算资源。Spark 自动对 RDDs 中的大量数据元素进行分区，并在 worker 节点之间分配分区，计算。分区是逻辑上。</p><h2 id="8-2-RDD-partition-的相关属性"><a href="#8-2-RDD-partition-的相关属性" class="headerlink" title="8.2 RDD partition 的相关属性"></a>8.2 RDD partition 的相关属性</h2><table><thead><tr><th>属性</th><th>描述</th></tr></thead><tbody><tr><td>partitions</td><td>返回包含 RDD 所有分区引用 的一个数组</td></tr><tr><td>partitions.size</td><td>返回 RDD 的分区数量</td></tr><tr><td>partitioner</td><td>返回下列分区器之一：<br>NONE<br>HashPartitioner<br>RangePartitioner<br>自定义分区器</td></tr></tbody></table><p>Spark 使用 partitioner 属性来确定分区算法，以此来确定哪些 worker 需要存储特定的 RDD记录。如果 partitoner 的值为 NONE，意思是分区不是基于数据的特性 ，但是分布是随机的，并且保证在节点之间是均匀地。</p><h2 id="8-3-查看-RDD-partition-信息"><a href="#8-3-查看-RDD-partition-信息" class="headerlink" title="8.3 查看 RDD partition 信息"></a>8.3 查看 RDD partition 信息</h2><p><strong>textfile 方法的 partition size 查看</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">println(textFileRDD.partitions.size)</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：使用 textFile 方法读取数据，可以设置 partition 大小：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val textFileRDD = sc.textFile(&quot;in/Flight.csv&quot;, 2)</span><br></pre></td></tr></table></figure><p>在集群环境中，读取本地文件、HDFS数据，由数据的 block 个数决定，最小为2。</p><p><strong>特殊情况</strong>：如果 local 模式，单线程运行，默认 partitions.size 为1。（项目中不会使用此情况）</p><p><strong>注意</strong>：每个 partition 会运行一个task 来处理其中的数据元素。</p><h2 id="8-4-RDD-的初始分区"><a href="#8-4-RDD-的初始分区" class="headerlink" title="8.4 RDD 的初始分区"></a>8.4 RDD 的初始分区</h2><blockquote><p>conf.set(“spark.default.parallelism”,”3”)   // 设置默认的并行度</p></blockquote><p>local: 一个线程———–sc.defaultParallelism 值为1<br>local[*]: 服务器core 数量——–sc. defaultParallelism 的值为8<br>local[4]: 4个线程———–sc.defaultParallelism 的值为4</p><p><strong>spark.default.parallelism 参数值的说明</strong>：<br>如果 spark-default.conf 或 SparkConf 中设置了 spark.default.parallelism 参数值，那么 spark.default.parallelism = 设置值；<br>如果 spark-default.conf 或 SparkConf 中没有设置，那么：</p><p><strong>local 模式</strong>：<br>local: spark.default.parallelism =1<br>local[4]: spark.default.parallelism = 4</p><p><strong>yarn 和 standalone 模式</strong>：<br>spark.default.parallelism = max(所有 executor 使用的core 总数, 2)</p><p><strong>由上述规则，确定 spark.default.parallelism 的默认值</strong><br>当Spark 程序执行时，会生成个 SparkContext 对象，同时会生成以下两个参数值：<br>sc.defaultParallelism = spark.default.parallelism<br>sc.defaultMinPartitions = min(spark.default.parallelism, 2)</p><p>当sc.defaultParallelism 和 sc.defaultMinPartitions 确认了，就可以推算出RDD 的分区数了。</p><p><strong>有三种产生 RDD 的方式</strong>：</p><ol><li>通过集合创建<blockquote><p>val rdd = sc.parallelize(1 to 100)</p></blockquote></li></ol><p>没有指定分区数，则rdd的分区数 = sc.defaultParallelism</p><ol start="2"><li>通过外部存储创建<blockquote><p>val rdd = sc.textFile(filePath)</p></blockquote></li></ol><p>2.1 从本地 文件生成 RDD，没有指定分区数，则默认分区规则为: rdd 的分区数 = max(本地 file 的分片数, sc.defaultMinPartitions)<br>2.2 从 HDFS 读取数据生成 RDD，没有指定分区数，则默认 分区规则为：rdd 的分区数 = max(HDFS文件的 block 数, sc.defaultMinPartitions)</p><ol start="3"><li>通过已有 RDD 产生新的 RDD，新 RDD的分区数遵循<strong>遗传</strong>特性。见下节。</li></ol><p><strong>注</strong>：项目中，在 spark-default.conf 文件中，spark.default.parallelism 属性值设置为 executor-cores * executors 个数 * 3</p><h2 id="8-5-Transformation-操作对分区的影响"><a href="#8-5-Transformation-操作对分区的影响" class="headerlink" title="8.5 Transformation 操作对分区的影响"></a>8.5 Transformation 操作对分区的影响</h2><h3 id="8-5-1-普通-RDD-操作"><a href="#8-5-1-普通-RDD-操作" class="headerlink" title="8.5.1 普通 RDD 操作"></a>8.5.1 普通 RDD 操作</h3><table><thead><tr><th>API调用</th><th>RDD 分区属性值<br>partition.size</th><th>RDD 分区属性值<br>partitioner</th></tr></thead><tbody><tr><td>map(),  flatMap(), distinct()</td><td>与父RDD相同</td><td>NONE</td></tr><tr><td>filter()</td><td>与父RDD相同</td><td>与父RDD相同</td></tr><tr><td>rdd.union(otherRDD)</td><td>rdd.partitions.size + otherRDD.partitions.size</td><td>NONE</td></tr><tr><td>rdd.intersection(otherRDD)</td><td>max(rdd.partitions.size, otherRDD.partitions.size)</td><td>NONE</td></tr><tr><td>rdd.subtract(otherRDD)</td><td>rdd.partitions.size</td><td>NONE</td></tr><tr><td>rdd.cartesian(otherRDD)</td><td>rdd.partitions.size * otherRDD.partitions.size</td><td>NONE</td></tr></tbody></table><h3 id="8-5-2-Key-value-RDD-操作"><a href="#8-5-2-Key-value-RDD-操作" class="headerlink" title="8.5.2 Key-value RDD 操作"></a>8.5.2 Key-value RDD 操作</h3><table><thead><tr><th>API调用</th><th>RDD 分区属性值<br>partition.size</th><th>RDD 分区属性值<br>partitioner</th></tr></thead><tbody><tr><td>reduceByKey(),foldByKey(),combineByKey(),groupByKey()</td><td>与父RDD相同</td><td>HashPartitioner</td></tr><tr><td>sortByKey</td><td>与父RDD相同</td><td>RangePartitioner</td></tr><tr><td>mapValues(), flatMapValues()</td><td>与父RDD相同</td><td>与父RDD相同</td></tr><tr><td>cogroup(),join(),leftOuterJoin(), rightOuterJoin()</td><td>取决于所涉及的两个 RDDs 的某些输入属性</td><td>HashPartitioner</td></tr></tbody></table><h2 id="8-6-有多少分区是合适的（重点！！）"><a href="#8-6-有多少分区是合适的（重点！！）" class="headerlink" title="8.6 有多少分区是合适的（重点！！）"></a>8.6 有多少分区是合适的（重点！！）</h2><p>分区数量太少、太多都有一定的优点和缺点。因此，建议根据集群配置和需求进行明智的分区。<br>Core-partition-task</p><h3 id="8-6-1-分区太少的缺点"><a href="#8-6-1-分区太少的缺点" class="headerlink" title="8.6.1 分区太少的缺点"></a>8.6.1 分区太少的缺点</h3><p>减少并发性——您没有使用并行性的优点。可能存在空闲的 wroker 节点。<br>数据倾斜和不恰当的资源利用——数据可能在一个分区上倾斜，因此一个 worker 可能比其他 worker 做的更多，因此可能会出现资源问题。</p><h3 id="8-6-2-分区太多的缺点"><a href="#8-6-2-分区太多的缺点" class="headerlink" title="8.6.2 分区太多的缺点"></a>8.6.2 分区太多的缺点</h3><p>任务调度可能比实际执行时间花费更多的时间。</p><p><strong>因此，在分区的数量之间存在权衡。推荐如下</strong>：</p><ol><li><p>可用 core 数量的2-3 倍。Spark 只为 RDD 的每个分区运行一个并发任务，最多可以同时运行集群中的核心数量个 task，分区数量至少与可用 core 数量相等。可以通过调用 sc.defaultParallelism 获得可用 core 值。单个分区的数据量大小最终取决于执行程序的可用内存。</p></li><li><p>WebUI 上查看任务执行，至少需要 100+ ms 时间。如果所用 时间少于 100ms，那么应用程序可能会花更多的时间来调度任务。此时就要减少 partition 的数量。</p></li></ol><h2 id="8-7-Spark-中的分区器"><a href="#8-7-Spark-中的分区器" class="headerlink" title="8.7 Spark 中的分区器"></a>8.7 Spark 中的分区器</h2><p>要使用分区器，首先要<strong>创建 PairRDD类型的 RDD</strong>。<br>Spark 有两种类型的分区器。一个是 HashPartitioner，另一个是 RangePartitioner。</p><h3 id="8-7-1-HashPartitioner"><a href="#8-7-1-HashPartitioner" class="headerlink" title="8.7.1 HashPartitioner"></a>8.7.1 HashPartitioner</h3><p>HashPartitioner 基于 Java 的 Object.hashcode() 方法进行分区。</p><h3 id="8-7-2-RangePartitioner"><a href="#8-7-2-RangePartitioner" class="headerlink" title="8.7.2 RangePartitioner"></a>8.7.2 RangePartitioner</h3><p>如果有可排序的记录，那么范围分区将几乎在相同的范围内划分记录。范围 Range 是 通过采样传入 RDD的数据内容来确定的。首先，RangePartitioner 将根据 key 对记录进行排序，然后根据给定的值将记录划分为 若干个分区。</p><h3 id="8-7-3-自定义分区器"><a href="#8-7-3-自定义分区器" class="headerlink" title="8.7.3 自定义分区器"></a>8.7.3 自定义分区器</h3><p>还可以通过扩展 Spark 中的默认分区器类来定制 需要的分区数量和应该存储在这些分区中的内容。</p><p><strong>代码示例</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">val listRDD = sc.parallelize((1 to 10).toList)</span><br><span class="line">val pairRDD = listRDD.map(num =&gt; (num, num))</span><br><span class="line">println(&quot;NumPartitions: &quot; + pairRDD.getNumPartitions) // NumPartitions: 8</span><br><span class="line">println(&quot;Partitioner: &quot; + pairRDD.partitioner)  // Partitioner: None</span><br><span class="line">pairRDD.saveAsTextFile(&quot;out/None&quot;)</span><br><span class="line"></span><br><span class="line">// 使用 HashPartitioner 并 设置分区个数</span><br><span class="line">val hashPartitionerRDD = pairRDD.partitionBy(new HashPartitioner(4))</span><br><span class="line">hashPartitionerRDD.saveAsTextFile(&quot;out/hashPartition4&quot;)</span><br><span class="line"></span><br><span class="line">// coalesce 方法只能用来减少 分区数量，不能用来增加分区数量</span><br><span class="line">// partitionBy 方法可以减少，也可以增加</span><br><span class="line">hashPartitionerRDD.coalesce(2).saveAsTextFile(&quot;out/hashPartition2&quot;)</span><br><span class="line">println(hashPartitionerRDD.partitioner) // Some(org.apache.spark.HashPartitioner@4)</span><br></pre></td></tr></table></figure><h1 id="9-Spark-RDD-数据保存实战"><a href="#9-Spark-RDD-数据保存实战" class="headerlink" title="9. Spark RDD 数据保存实战"></a>9. Spark RDD 数据保存实战</h1><h2 id="9-1-保存数据到-HDFS"><a href="#9-1-保存数据到-HDFS" class="headerlink" title="9.1 保存数据到 HDFS"></a>9.1 保存数据到 HDFS</h2><p><strong>代码示例</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val textFileRDD = sc.textFile(&quot;hdfs://master01:8020/in/README.txt&quot;, 2)</span><br><span class="line">val wordsRDD = textFileRDD.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">val wordcountPairRDD = wordsRDD.map(w =&gt; (w, 1))</span><br><span class="line">val wordcountRDD = wordcountPairRDD.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">wordcountRDD.saveAsTextFile(&quot;hdfs://master01:8020/out/wordcount&quot;)</span><br></pre></td></tr></table></figure><h2 id="9-2-保存数据到-mysql-数据库"><a href="#9-2-保存数据到-mysql-数据库" class="headerlink" title="9.2 保存数据到 mysql 数据库"></a>9.2 保存数据到 mysql 数据库</h2><h3 id="9-2-1-读"><a href="#9-2-1-读" class="headerlink" title="9.2.1 读"></a>9.2.1 读</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">//mysql 读</span><br><span class="line">val jdbcDF = sparkSession.read</span><br><span class="line">  .format(&quot;jdbc&quot;)</span><br><span class="line">  .option(&quot;url&quot;, &quot;jdbc:mysql://master01:3306/test&quot;)</span><br><span class="line">  .option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">  .option(&quot;password&quot;, &quot;Mysql123!&quot;)</span><br><span class="line">  .option(&quot;dbtable&quot;, &quot;flight&quot;)</span><br><span class="line">  .load()</span><br><span class="line">jdbcDF.printSchema()</span><br></pre></td></tr></table></figure><h3 id="9-2-2-写"><a href="#9-2-2-写" class="headerlink" title="9.2.2 写"></a>9.2.2 写</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">val schema = StructType(List(</span><br><span class="line">  StructField(&quot;name&quot;, StringType, nullable = false),</span><br><span class="line">  StructField(&quot;age&quot;, IntegerType, nullable = false),</span><br><span class="line">  StructField(&quot;gender&quot;, StringType, nullable = false)</span><br><span class="line">))</span><br><span class="line"></span><br><span class="line">val rowRDD = sc.parallelize(Seq(</span><br><span class="line">  Row(&quot;张1&quot;, 18, &quot;男&quot;),</span><br><span class="line">  Row(&quot;张2&quot;, 19, &quot;女&quot;),</span><br><span class="line">  Row(&quot;张3&quot;, 10, &quot;男&quot;),</span><br><span class="line">  Row(&quot;张4&quot;, 48, &quot;女&quot;),</span><br><span class="line">  Row(&quot;张5&quot;, 68, &quot;男&quot;),</span><br><span class="line">  Row(&quot;张6&quot;, 16, &quot;男&quot;)</span><br><span class="line">))</span><br><span class="line">val df = sparkSession.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line">// mysql 写</span><br><span class="line">df.write</span><br><span class="line">  .format(&quot;jdbc&quot;)</span><br><span class="line">  .option(&quot;url&quot;, &quot;jdbc:mysql://master01:3306/test&quot;)</span><br><span class="line">  .option(&quot;dbtable&quot;, &quot;user&quot;)</span><br><span class="line">  .option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">  .option(&quot;password&quot;, &quot;Mysql123!&quot;)</span><br><span class="line">  .mode(SaveMode.Overwrite)</span><br><span class="line">  .save()</span><br></pre></td></tr></table></figure><h2 id="9-3-保存数据到-kafka"><a href="#9-3-保存数据到-kafka" class="headerlink" title="9.3 保存数据到 kafka"></a>9.3 保存数据到 kafka</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// producer 配置</span><br><span class="line">val props = new Properties()</span><br><span class="line">props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;master01:9092&quot;)</span><br><span class="line">props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName)</span><br><span class="line">props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName)</span><br><span class="line"></span><br><span class="line">// producer 发送 RDD 数据</span><br><span class="line">val textFileRDD = sc.textFile(&quot;in/Flight1.csv&quot;)</span><br><span class="line">textFileRDD.foreach(line =&gt; &#123;</span><br><span class="line">  val producer = new KafkaProducer[String, String](props)</span><br><span class="line">  val message = new ProducerRecord[String, String](&quot;myTopic&quot;, line)</span><br><span class="line">  println(message)</span><br><span class="line">  producer.send(message)</span><br><span class="line">  Thread.sleep(3000)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h1 id="10-Spark-RDD-缓存实战"><a href="#10-Spark-RDD-缓存实战" class="headerlink" title="10. Spark RDD 缓存实战"></a>10. Spark RDD 缓存实战</h1><h2 id="10-1-前言"><a href="#10-1-前言" class="headerlink" title="10.1 前言"></a>10.1 前言</h2><p>一个 action 会启动一个 job， 一个 job 里面有一个或多个 stage，一个 stage 里面有一个或者多个 task。</p><p>Repartiton 引起 shuffle 操作，shuffle 操作发生的时候，stage 会一分为二。</p><p>窄依赖， 宽依赖<br>一种性能调优的方式。</p><h2 id="10-2-要点"><a href="#10-2-要点" class="headerlink" title="10.2 要点"></a>10.2 要点</h2><ol><li><p><strong>缓存</strong> 和 <strong>持久化</strong>是 Spark 计算过程中的调优技术。缓存和持久化可以保存中间计算结果，以便在后续的 stage 中重用，而不需要再次从头计算。这些中间结果以 RDD 的形式保存在内存（默认）中，或者磁盘中。</p></li><li><p>StorageLevel 描述了 RDD 是如何被<strong>持久化</strong>（persist）的，可以提供如下相关信息：</p></li></ol><ul><li>RDD 持久化磁盘存储还是内存存储</li><li>RDD 持久化是否使用了 off-heap</li><li>RDD 是否需要被序列化</li><li>缓存的副本是多少（默认是 1）</li></ul><ol start="3"><li><strong>StorageLevel</strong> 的值包括：</li></ol><ul><li>NONE（默认）</li><li>DISK_ONLY: RDD 只是存储在磁盘，内存消耗低，CPU 密集型。</li><li>DISK_ONLY_2</li><li>MEMORY_ONLY（cache 操作）：RDD 以非序列化的 Java 对象存储在 JVM中。如果 RDD 的大小超过了内存大小，那么某些 partition 将会不缓存，下次使用时重新计算。这种存储级别比较耗内存，但是不耗 CPU。数据只存储在内存，不存储在磁盘。</li><li>MEMORY_ONLY_2</li><li>MEMORY_ONLY_SER: RDD 以序列化 Java 对象（每个 partition 一个字节数组）的形式存储。在这个级别，内存空间 使用很低，CPU计算时间高。</li><li>MEMORY_ONLY_SER_2</li><li>MEMORY_AND_DISK: RDD 以非序列化的 Java 对象存储在 JVM 中。当 RDD 的大小超过了内存 大小，多出的 partition  会缓存在磁盘上，后续计算如果用到这些多出的 partiton，会从磁盘获取。这种存储级别比较耗内存，CPU消耗一般。</li><li>MEMORY_AND_DISK_2</li><li>MEMORY_AND_DISK_SER: 与 MEMORY_ONLY_SER 类似，只是将大于内存的partition 数据序列化到磁盘，而不是重新计算。内存消耗低，CPU密集型。</li><li>MEMORY_AND_DISK_SER_2</li><li>OFF_HEAP</li></ul><p>可以使用 <strong>getStorageLevel</strong> 方法查看 RDD 的 StorageLevel:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val textFileRDD = sc.textFile(&quot;\in\README.txt&quot;)</span><br><span class="line">println(textFileRDD.getStorageLevel)</span><br><span class="line">println(textFileRDD.getStorageLevel)</span><br></pre></td></tr></table></figure><p><strong>输出</strong>：</p><blockquote><p>StorageLevel(1 replicas)</p></blockquote><ol start="4"><li><p>RDD 可以被缓存（cache）到内存，使用 cache() 方法，也可以被持久化（persist），使用 persist() 方法。</p></li><li><p>cache() 和 persist() 方法的<strong>区别</strong>在于：cache() 等价于 persist(MEMEORY_ONLY)，即 cache() 仅仅是 persist() 使用默认存储级别 MEMORY_ONLY 的一种情况。使用 persist() 方法可以设置不同的 StorageLevel值。</p></li><li><p>对于<strong>迭代算法</strong>，缓存和持久化是一个重要的工具。因为，当我们在一个节点上缓存了 RDD 的某个 partiton 到内存中，其就可以在下面的计算中重复使用，而不需要从头计算，可以使计算性能提高 <strong>10</strong>倍。如果缓存中的某个 partiton 丢失或者不可用，根据 Spark RDD 的容错特性，Spark 会从头计算这个 partition。</p></li><li><p>什么时候需要对 RDD 进行持久化？在 Spark 中，我们可以多次使用同一个 RDD，如：使用 RDD 计算 count()、max()、min()等 action 操作。而且这些操作可能<strong>很耗内存</strong>，尤其是迭代算法（机器学习）。为了解决<strong>频繁重复计算</strong>的问题，此时就需要对 RDD 进行持久化。</p></li><li><p>Spark 自动监控每个节点的<strong>缓存</strong>和以 <strong>LRU</strong>（最近最少使用）方式删除旧数据分区。LRU算法，保证了最常用的数据被缓存。我们 也可以使用 <strong>RDD.unpersist()</strong> 方法手动删除缓存。</p></li><li><p>Spark 会在 <strong>shuffle</strong> 操作中<strong>自动持久化</strong>一些中间数据（例如 redueByKey），即使没有调用 persist 方法。这样做是为了避免在 shuffle 期间节点故障时重新计算整个输入。如果用户准备重用生成的 RDD，推荐显式调用持久化。</p></li></ol><h2 id="10-3-RDD-持久化存储级别如何选择"><a href="#10-3-RDD-持久化存储级别如何选择" class="headerlink" title="10.3 RDD 持久化存储级别如何选择"></a>10.3 RDD 持久化存储级别如何选择</h2><p>Spark 的存储级别是为了在<strong>内存使用</strong>和 <strong>CPU 效率</strong>之间 提供不同的权衡，具体选择哪个存储级别，可以从以下方面考虑：</p><ul><li>如果 RDDs 数据适合默认存储级别（MEMORY_ONLY），那么就使用默认。此时，RDD 的运算速度最快。</li><li>如果没有，请尝试使用 MEMORY_ONLY_SER 并选择一个快速序列化库，以使对象更节省空间，但访问速度仍然相当快。</li><li>不要持久化到磁盘，除非计算 数据集的函数很耗时，或者 过滤了大量 数据。因为，从磁盘读取分区，可能没有重新计算快。</li><li>如果需要快速的故障恢复，则使用副本存储级别。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-什么是RDD？&quot;&gt;&lt;a href=&quot;#1-什么是RDD？&quot; class=&quot;headerlink&quot; title=&quot;1. 什么是RDD？&quot;&gt;&lt;/a&gt;1. 什么是RDD？&lt;/h1&gt;&lt;p&gt;RDD 是 Resilient Distributed Dataset（&lt;strong&gt;弹性分布式数据集&lt;/strong&gt;） 的简称。它是 Apache Spark 的基本数据结构。它是一个不可变的对象集合，在集群的不同节点上进行计算。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Resilient: 即在 RDD lineage(DAG) 的帮助下具有容错能力，能够重新计算由于节点故障而丢失或损坏的数据分区。&lt;/li&gt;
&lt;li&gt;Distributed: 数据分布在多个节点上。&lt;/li&gt;
&lt;li&gt;Dataset: 表示所操作的数据集。用户可以通过 JDBC 从外部加载数据集，数据集可以是 JSON 文件，CSV 文件，文本文件或数据库。&lt;br&gt;
&lt;br&gt;
    
    </summary>
    
    
      <category term="笔记" scheme="https://miracle-xing.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Spark" scheme="https://miracle-xing.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop 集群 HA 架构配置</title>
    <link href="https://miracle-xing.github.io/2019/07/23/Hadoop-%E9%9B%86%E7%BE%A4-HA-%E6%9E%B6%E6%9E%84%E9%85%8D%E7%BD%AE/"/>
    <id>https://miracle-xing.github.io/2019/07/23/Hadoop-集群-HA-架构配置/</id>
    <published>2019-07-22T20:42:04.000Z</published>
    <updated>2019-07-23T18:27:43.653Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Linux-基础配置"><a href="#1-Linux-基础配置" class="headerlink" title="1. Linux 基础配置"></a>1. Linux 基础配置</h1><h2 id="1-1-设置静态IP："><a href="#1-1-设置静态IP：" class="headerlink" title="1.1 设置静态IP："></a>1.1 设置静态IP：</h2><p>宿主机配置：<br>VMware: NAT模式，不使用DHCP<br>VMnet8: IPv4使用固定ip，子网掩码</p><a id="more"></a><h2 id="1-2-虚拟机配置："><a href="#1-2-虚拟机配置：" class="headerlink" title="1.2 虚拟机配置："></a>1.2 虚拟机配置：</h2><blockquote><p>vim /etc/sysconfig/network-scripts/ifcfg-ens33</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">TYPE=&quot;Ethernet&quot;</span><br><span class="line">PROXY_METHOD=&quot;none&quot;</span><br><span class="line">BROWSER_ONLY=&quot;no&quot;</span><br><span class="line">BOOTPROTO=&quot;static&quot;</span><br><span class="line">DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV4_FAILURE_FATAL=&quot;yes&quot;</span><br><span class="line">NAME=&quot;ens33&quot;</span><br><span class="line">UUID=&quot;a428bf24-b245-408a-88b6-d0934885c452&quot;</span><br><span class="line">DEVICE=&quot;ens33&quot;</span><br><span class="line">ONBOOT=&quot;yes&quot;</span><br><span class="line">IPADDR=&quot;192.168.12.130&quot;</span><br><span class="line">GATEWAY=&quot;192.168.12.2&quot;</span><br><span class="line">DNS1=&quot;192.168.12.2&quot;</span><br></pre></td></tr></table></figure><h2 id="1-3-修改主机名："><a href="#1-3-修改主机名：" class="headerlink" title="1.3 修改主机名："></a>1.3 修改主机名：</h2><blockquote><p>vim /etc/sysconfig/network</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NETWORKING=yes</span><br><span class="line">HOSTNAME=master01</span><br></pre></td></tr></table></figure><blockquote><p>vim /etc/hostname</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">master01</span><br></pre></td></tr></table></figure><h2 id="1-4-设置IP和主机名映射"><a href="#1-4-设置IP和主机名映射" class="headerlink" title="1.4 设置IP和主机名映射:"></a>1.4 设置IP和主机名映射:</h2><blockquote><p>vim /etc/hosts</p></blockquote><h2 id="1-5-设置免密登录"><a href="#1-5-设置免密登录" class="headerlink" title="1.5 设置免密登录"></a>1.5 设置免密登录</h2><blockquote><p>ssh-keygen</p></blockquote><p>生成公钥和私钥</p><blockquote><p>ssh-copy-id -i ~/.ssh/id_rsa.pub 目标机器用户名@目标机器名</p></blockquote><p>将公钥拷贝到目标机器</p><blockquote><p>ssh 目标机器用户名@目标机器名 </p></blockquote><h2 id="1-6-禁用selinux与防火墙"><a href="#1-6-禁用selinux与防火墙" class="headerlink" title="1.6 禁用selinux与防火墙"></a>1.6 禁用selinux与防火墙</h2><h3 id="1-6-1-selinux"><a href="#1-6-1-selinux" class="headerlink" title="1.6.1 selinux"></a>1.6.1 selinux</h3><blockquote><p>vim /etc/sysconfig/selinux</p></blockquote><p>将SELINUX改成disabled</p><h3 id="1-6-2-防火墙"><a href="#1-6-2-防火墙" class="headerlink" title="1.6.2 防火墙"></a>1.6.2 防火墙</h3><blockquote><p>systemctl stop firewalld<br>systemctl disable firewalld</p></blockquote><h2 id="1-7-卸载-Openjdk"><a href="#1-7-卸载-Openjdk" class="headerlink" title="1.7 卸载 Openjdk"></a>1.7 卸载 Openjdk</h2><p>查看jdk情况</p><blockquote><p>rpm -qa | grep java</p></blockquote><p>若使用openjdk则卸载</p><blockquote><p>rpm -e –nodeps ‘查询到的openjdk，多个文件用空格隔开’</p></blockquote><p>解压JDK<br>配置环境变量</p><h1 id="2-Hadoop-配置"><a href="#2-Hadoop-配置" class="headerlink" title="2. Hadoop 配置"></a>2. Hadoop 配置</h1><h2 id="2-1-集群规划："><a href="#2-1-集群规划：" class="headerlink" title="2.1 集群规划："></a>2.1 集群规划：</h2><p>有3台虚拟机，分别是 master01, master02, slave01, slave02, slave03。</p><table><thead><tr><th>节点</th><th>master01</th><th>master02</th><th>slave01</th><th>slave02</th><th>slave03</th></tr></thead><tbody><tr><td><strong>组件</strong></td><td>Namenode<br>DFSZKFailoverController<br>ResourceManager<br>Jobhistory</td><td>Namenode<br>DFSZKFailoverController</td><td>Datanode<br>NodeManager<br>JournalNode</td><td>Datanode<br>NodeManager<br>JournalNode</td><td>Datanode<br>NodeManager<br>JournalNode</td></tr></tbody></table><h2 id="2-2-解压-hadoop-并清理文档"><a href="#2-2-解压-hadoop-并清理文档" class="headerlink" title="2.2 解压 hadoop 并清理文档"></a>2.2 解压 hadoop 并清理文档</h2><blockquote><p>tar -zxvf hadoop-2.7.7.tar.gz -C /opt/modules/<br>mv hadoop-2.7.7/ hadoop277</p></blockquote><p>清理hadoop-2.5.0/share/doc</p><h2 id="2-3-指定-Java-路径"><a href="#2-3-指定-Java-路径" class="headerlink" title="2.3 指定 Java 路径"></a>2.3 指定 Java 路径</h2><p>文件：hadoop-env.sh / mapred-env.sh / yarn-env.sh</p><h2 id="2-4-HDFS-相关修改"><a href="#2-4-HDFS-相关修改" class="headerlink" title="2.4 HDFS 相关修改"></a>2.4 HDFS 相关修改</h2><h3 id="2-4-1-修改-core-site-xml"><a href="#2-4-1-修改-core-site-xml" class="headerlink" title="2.4.1 修改 core-site.xml"></a>2.4.1 修改 core-site.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hdfs://ns&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/opt/modules/hadoop277/data/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;master01:2181,master02:2181,slave01:2181,slave02:2181,slave03:2181&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="2-4-2-修改hdfs-site-xml"><a href="#2-4-2-修改hdfs-site-xml" class="headerlink" title="2.4.2 修改hdfs-site.xml"></a>2.4.2 修改hdfs-site.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;ns&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.ha.namenodes.ns&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;nn1,nn2&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.rpc-address.ns.nn1&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;master01:8020&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.rpc-address.ns.nn2&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;master02:8020&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.http-address.ns.nn1&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;master01:50070&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.http-address.ns.nn2&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;master02:50070&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">       &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;qjournal://slave01:8485;slave02:8485;slave03:8485/ns&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.client.failover.proxy.provider.ns&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;/opt/modules/hadoop277/data/dfs/jn&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;sshfence&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="2-4-3-修改slaves"><a href="#2-4-3-修改slaves" class="headerlink" title="2.4.3 修改slaves"></a>2.4.3 修改slaves</h3><p>从节点主机名</p><blockquote><p>slave01<br>slave02<br>slave03</p></blockquote><h2 id="2-5-MapReduce-与-YARN-修改"><a href="#2-5-MapReduce-与-YARN-修改" class="headerlink" title="2.5 MapReduce 与 YARN 修改"></a>2.5 MapReduce 与 YARN 修改</h2><h3 id="2-5-1-修改-mapred-site-xml"><a href="#2-5-1-修改-mapred-site-xml" class="headerlink" title="2.5.1 修改 mapred-site.xml"></a>2.5.1 修改 mapred-site.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master01:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master01:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="2-5-2-修改-yarn-site-xml"><a href="#2-5-2-修改-yarn-site-xml" class="headerlink" title="2.5.2 修改 yarn-site.xml"></a>2.5.2 修改 yarn-site.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">       &lt;!-- 开启RM高可靠 --&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;!-- 指定RM的cluster id --&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;RM_HA_ID&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;!-- 指定RM的名字 --&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;rm1,rm2&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;!-- 分别指定RM的地址 --&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;master01&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;master02&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;!-- 指定zk集群地址 --&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;master01:2181,master02:2181,slave01:2181,slave02:2181,slave03:2181&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p><strong>分发 hadoop 文件到其他节点</strong></p><h2 id="2-6-集群启动"><a href="#2-6-集群启动" class="headerlink" title="2.6 集群启动"></a>2.6 集群启动</h2><h3 id="2-6-1-启动-zookeeper，再启动-journalnode"><a href="#2-6-1-启动-zookeeper，再启动-journalnode" class="headerlink" title="2.6.1 启动 zookeeper，再启动 journalnode"></a>2.6.1 启动 zookeeper，再启动 journalnode</h3><p>slave01,  slave02, slave03</p><blockquote><p>sbin/hadoop-daemon.sh start journalnode</p></blockquote><h3 id="2-6-2-格式化-namenode"><a href="#2-6-2-格式化-namenode" class="headerlink" title="2.6.2 格式化 namenode"></a>2.6.2 格式化 namenode</h3><p>master01</p><blockquote><p>bin/hdfs namenode -format</p></blockquote><h3 id="2-6-3-同步元数据"><a href="#2-6-3-同步元数据" class="headerlink" title="2.6.3 同步元数据"></a>2.6.3 同步元数据</h3><p>master01 上启动 namenode </p><p>master02：</p><blockquote><p>bin/hdfs namenode -bootstrapStandby</p></blockquote><h3 id="2-6-4-初始化-ZKFC"><a href="#2-6-4-初始化-ZKFC" class="headerlink" title="2.6.4 初始化 ZKFC"></a>2.6.4 初始化 ZKFC</h3><p>master01</p><blockquote><p>bin/hdfs zkfc -formatZK<br>//zk下生成hadoop-ha目录表示成功</p></blockquote><h3 id="2-6-5-启动-HDFS-相关进程"><a href="#2-6-5-启动-HDFS-相关进程" class="headerlink" title="2.6.5 启动 HDFS 相关进程"></a>2.6.5 启动 HDFS 相关进程</h3><p>master01</p><blockquote><p>sbin/start-dfs.sh</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Starting namenodes on [master01 master02]</span><br><span class="line">master02: starting namenode, logging to /opt/modules/hadoop277/logs/hadoop-root-namenode-master02.out</span><br><span class="line">master01: starting namenode, logging to /opt/modules/hadoop277/logs/hadoop-root-namenode-master01.out</span><br><span class="line">slave01: starting datanode, logging to /opt/modules/hadoop277/logs/hadoop-root-datanode-slave01.out</span><br><span class="line">slave02: starting datanode, logging to /opt/modules/hadoop277/logs/hadoop-root-datanode-slave02.out</span><br><span class="line">slave03: starting datanode, logging to /opt/modules/hadoop277/logs/hadoop-root-datanode-slave03.out</span><br><span class="line">Starting journal nodes [slave01 slave02 slave03]</span><br><span class="line">slave02: starting journalnode, logging to /opt/modules/hadoop277/logs/hadoop-root-journalnode-slave02.out</span><br><span class="line">slave03: starting journalnode, logging to /opt/modules/hadoop277/logs/hadoop-root-journalnode-slave03.out</span><br><span class="line">slave01: starting journalnode, logging to /opt/modules/hadoop277/logs/hadoop-root-journalnode-slave01.out</span><br><span class="line">Starting ZK Failover Controllers on NN hosts [master01 master02]</span><br><span class="line">master02: starting zkfc, logging to /opt/modules/hadoop277/logs/hadoop-root-zkfc-master02.out</span><br><span class="line">master01: starting zkfc, logging to /opt/modules/hadoop277/logs/hadoop-root-zkfc-master01.out</span><br></pre></td></tr></table></figure><h3 id="2-6-6-查看进程"><a href="#2-6-6-查看进程" class="headerlink" title="2.6.6 查看进程"></a>2.6.6 查看进程</h3><p><strong>master01 进程</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">6392 Jps</span><br><span class="line">6026 DFSZKFailoverController</span><br><span class="line">2027 QuorumPeerMain</span><br><span class="line">5711 NameNode</span><br><span class="line">6191 ResourceManager</span><br></pre></td></tr></table></figure><p><strong>master02 进程</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">3635 NameNode</span><br><span class="line">3752 DFSZKFailoverController</span><br><span class="line">3884 Jps</span><br><span class="line">1805 QuorumPeerMain</span><br></pre></td></tr></table></figure><p><strong>slave01, slave02, slave03 进程</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">3507 DataNode</span><br><span class="line">2085 QuorumPeerMain</span><br><span class="line">3606 JournalNode</span><br><span class="line">3863 Jps</span><br><span class="line">3759 NodeManager</span><br></pre></td></tr></table></figure><h3 id="2-6-7-验证测试"><a href="#2-6-7-验证测试" class="headerlink" title="2.6.7 验证测试"></a>2.6.7 验证测试</h3><p><strong>HDFS webUI</strong>: <a href="http://master02:50070" target="_blank" rel="noopener">http://master02:50070</a><br><strong>YARN webUI</strong>: <a href="http://master01:8088" target="_blank" rel="noopener">http://master01:8088</a></p><p><strong>测试 YARN</strong>：</p><blockquote><p>hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount /in/README.txt /out</p></blockquote><hr><p><strong>后记</strong>：写完博客，天也快亮了。日拱一卒，功不唐捐。加油！！</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-Linux-基础配置&quot;&gt;&lt;a href=&quot;#1-Linux-基础配置&quot; class=&quot;headerlink&quot; title=&quot;1. Linux 基础配置&quot;&gt;&lt;/a&gt;1. Linux 基础配置&lt;/h1&gt;&lt;h2 id=&quot;1-1-设置静态IP：&quot;&gt;&lt;a href=&quot;#1-1-设置静态IP：&quot; class=&quot;headerlink&quot; title=&quot;1.1 设置静态IP：&quot;&gt;&lt;/a&gt;1.1 设置静态IP：&lt;/h2&gt;&lt;p&gt;宿主机配置：&lt;br&gt;VMware: NAT模式，不使用DHCP&lt;br&gt;VMnet8: IPv4使用固定ip，子网掩码&lt;/p&gt;
    
    </summary>
    
    
      <category term="笔记" scheme="https://miracle-xing.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Hadoop" scheme="https://miracle-xing.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Spark 概念及应用程序架构</title>
    <link href="https://miracle-xing.github.io/2019/07/22/Spark-%E6%A6%82%E5%BF%B5%E5%8F%8A%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E6%9E%B6%E6%9E%84/"/>
    <id>https://miracle-xing.github.io/2019/07/22/Spark-概念及应用程序架构/</id>
    <published>2019-07-21T16:47:55.000Z</published>
    <updated>2019-07-23T18:28:11.467Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><ol><li><p>Spark是计算框架，不是存储框架。类似Hadoop中的MR</p></li><li><p>Spark是分布式的内存计算框架，Spark在计算的时候，内存不够用，数据会写到磁盘。</p><a id="more"></a></li><li><p>Spark和Hadoop没有必然联系，两者是独立的。</p></li><li><p>Spark可以读取HDFS / fileSystem / DB / Kafka / Flume上的数据，可以把数据写到HDFS / fileSystem / DB / Kafka / Flume中。</p></li><li><p>Spark可以使用YARN做资源调度管理器 Spark on YARN。</p></li><li><p><strong>数据不动代码动</strong>。</p></li><li><p>Spark架构：Master Slave架构，主从架构，一主多从。</p></li><li><p>主从架构的突出问题是 <strong>单点故障</strong>，HA（高可用）架构就是为了解决单点故障，心跳消息。</p></li><li><p>主主架构：Flume，Kafka</p></li><li><p>数据本地性：计算时从最近的节点读取数据。</p></li><li><p>粗粒度、细粒度<br>指的是资源分配方式。<br>粗粒度：应用启动，资源就分配给你，你用不用都是你的。<br>细粒度：不提前分配资源，你需要的时候再给你。</p></li><li><p>Spark两种算子 <strong>Transformation</strong> 和 <strong>Action</strong><br>Transformation算子返回值是 RDD，Action算子返回值是计算结果，不是RDD。</p></li><li><p>Spark 有四种部署方式：Standalone / Spark on YARN / Apache Mesos / Kubernetes</p></li><li><p>Spark Shell 是Spark提供的本地交互式脚本，默认启动时Local模式，使用Scala语言。</p></li></ol><p><strong>Scala 一行代码实现 wordcount</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">textFile.flatMap(line=&gt;line.split(&quot; &quot;)).map(word=&gt;(word,1)).reduceByKey(_+_).sortBy(_._2,false).collect().foreach(println)</span><br></pre></td></tr></table></figure><hr><h1 id="Spark-应用程序架构"><a href="#Spark-应用程序架构" class="headerlink" title="Spark 应用程序架构"></a>Spark 应用程序架构</h1><ol><li><p>Spark 应用程序组件：driver, the master, the cluster manager 和运行在worker节点的executor(s)<br><img src="/images/2019/07/22/42616bf0-abd7-11e9-87f2-3dee39091945.png" alt="Spark 应用程序架构.png"><br>所有Spark组件，包括 driver, master 和 executor进程，都在JVM中运行。使用Scala编写的Spark程序编译为Java字节码在JVM上运行。</p></li><li><p>区分Spark运行时应用程序组件 和运行它们的位置和节点类型是很重要的。使用不同的部署模式，这些组件可能运行在不同的位置，所以不要以物理节点或实例的形式考虑这些组件。</p></li></ol><h2 id="1-Spark-Driver"><a href="#1-Spark-Driver" class="headerlink" title="1.  Spark Driver"></a>1.  Spark Driver</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;Spark 应用程序由一个 driver 进程（驱动程序）和一组 executor 进程组成。Spark 应用程序的生命周期从 Spark Driver 程序开始（和结束）。driver 进程负责运行你的 main 函数，此进程位于集群中的一个 节点上。主要负责三件事：</p><ol><li>维护有关 Spark 应用程序的信息；</li><li>响应用户的程序或输入；</li><li>分配和调度 executor 的 task 和资源。</li></ol><p>executors 进程实际执行 driver 分配给他们的工作。这意味着每个 executor 主要负责两件事：</p><ol><li>执行由驱动程序分配给它的代码。</li><li>将执行器 executor 的计算状态报告给 driver 节点。</li></ol><h3 id="1-1-SparkContext"><a href="#1-1-SparkContext" class="headerlink" title="1.1 SparkContext"></a>1.1 SparkContext</h3><p>Spark Driver 程序负责创建 SparkContext。 SparkContext 在 Spark Shell 中对应的变量名为 sc，用于连接 Spark 集群，是与 Spark 集群交互的入口。SparkContext 在 Spark 应用程序（包括 Spark Shell）的开始实例化，并用于整个程序。</p><h3 id="1-2-应用程序执行计划"><a href="#1-2-应用程序执行计划" class="headerlink" title="1.2 应用程序执行计划"></a>1.2 应用程序执行计划</h3><p>Driver 程序的主要功能之一是规划应用程序的执行。驱动程序接受所有请求的 transformation 和 action 操作，并创建一个有向无环图（DAG）。<br><strong>注</strong>：DAG  是计算机科学中常用的表示数据流及其依赖关系的数学结构。DAGs 包含节点和边，节点表示执行计划中的步骤。DAG中的边以定向的方式将一个节点连接到另一个顶点，这样就不会出现 循环引用。</p><p>DAG 由 task 和 stages 组成。task 是 Spark 程序中可调度工作的最小单位。stage是一组可以一起运行的task。多个stage之间是相互依存的 。shuffle是划分stage的依据。</p><p>在进程调度意义上，DAGs 并不是 Spark 独有的。例如，它们被用于其他大数据生态系统项目，如 Tez、Drill 和 Presto 的任务调度。DAGs 是 Spark 的基础！！！</p><h3 id="1-3-应用程序的调度"><a href="#1-3-应用程序的调度" class="headerlink" title="1.3 应用程序的调度"></a>1.3 应用程序的调度</h3><p>driver 程序还协调 DAG 中定义的 stage 和 task 的运行。在调度和运行 task 时涉及的主要driver 程序活动包括：</p><ul><li>跟踪可用资源以执行 task</li><li>调度任务，以便在可能的情况下 “接近”数据运行——数据本地性</li><li>协调数据在 stages 之间的移动</li></ul><h3 id="1-4-其他功能"><a href="#1-4-其他功能" class="headerlink" title="1.4 其他功能"></a>1.4 其他功能</h3><p>除了计划和编排 Spark 程序的执行之外，驱动程序还负责从应用程序返回结果。<br>driver 程序在4040端口上自动创建了应用程序 UI。如果在同一个主机上启动后续应用程序，则会为应用程序 UI 使用连续的端口（例如 4041, 4042 等）。</p><h2 id="2-Executor-和-worker"><a href="#2-Executor-和-worker" class="headerlink" title="2. Executor 和 worker"></a>2. Executor 和 worker</h2><p>Spark executor 是运行来自 Spark DAG 的task 进程。executor 在 Spark 集群中的worker 节点上获取 CPU和内存等计算资源。executor 专用于特定的 Spark 应用程序，并在 应用程序完成时终止。在 Spark 程序中，Spark executor 可以运行成百上千个 task。</p><p>通常情况下，worker 节点（承载 executor 进程）具有有限或固定数量的 executor。因此，一个 spark 集群（包括一定数量的服务器节点）具有有限数量的 executor，可以分配它们来运行 Spark 任务。</p><p>Spark executor 驻留在 JVM 中。executor 的 JVM 分配了一个堆内存，这是一个用于存储和管理对象的专用内存空间。堆内存的大小由 spark 配置文件 spark-default.xml 中的 spark.executor.memory 属性确定，或者 由提交应用程序时 spark-submit 的参数 –executor-memroy  确定。</p><p>worker 和 executor 只知道分配给他们的 task，而 driver 程序负责理解组成应用程序的完整 task 集合它们各自的依赖关系。</p><h2 id="3-Master-和-Cluster-Manager"><a href="#3-Master-和-Cluster-Manager" class="headerlink" title="3. Master 和 Cluster Manager"></a>3. Master 和 Cluster Manager</h2><p>Spark driver 程序计划并协调运行 Spark 应用程序所需的 task 集。task 本身在 executor 中运行，executor 驻留在 worker 节点上。</p><p>Master 和 Cluster Manager 是监控、分配、回收集群（Executor 运行的节点）资源的核心进程，Master 和 Cluster Manager 可以是各自独立的进程（Spark On YARN），也可以组合成一个进程（Standalone 运行模式）。</p><h3 id="3-1-Master"><a href="#3-1-Master" class="headerlink" title="3.1 Master"></a>3.1 Master</h3><p>Spark master 是用于请求集群中的资源并将这些资源提供给 Spark driver 程序的进程。在两种部署模式中，master 节点都与 worker 节点或slave 节点协商资源或容器，并跟踪 它们的状态并监视它们的进展。</p><p>Spark master 进程在 master 进程所在主机上的端口 8080 上，提供 web 用户界面。</p><p><strong>注</strong>：要区分 driver 进程和 master 进程在 Spark 程序运行时的作用。master 只是请求 资源，并使这些资源在应用程序的生命周期内对驱动程序可用。尽管 master 监控这些资源的状态和健康状况，但是它不参与应用程序的执行以及 task 和 stage 的协调。</p><h3 id="3-2-Cluster-Manager（集群管理器）"><a href="#3-2-Cluster-Manager（集群管理器）" class="headerlink" title="3.2 Cluster Manager（集群管理器）"></a>3.2 Cluster Manager（集群管理器）</h3><p>Cluster Manager 进程负责监控分配给 worker 节点上的资源，这些资源是 master 进程请求分配的。然后，master 以 Executor 的形式将这些集群资源提供给 driver 程序。如前所述，Cluster Manager可以独立于 master 进程（Spark On YARN），也可以组合成一个进程（Standalone 运行模式）。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概念&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Spark是计算框架，不是存储框架。类似Hadoop中的MR&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Spark是分布式的内存计算框架，Spark在计算的时候，内存不够用，数据会写到磁盘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="笔记" scheme="https://miracle-xing.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Spark" scheme="https://miracle-xing.github.io/tags/Spark/"/>
    
  </entry>
  
</feed>

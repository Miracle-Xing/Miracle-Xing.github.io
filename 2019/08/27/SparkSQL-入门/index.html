<!DOCTYPE html>


  <html class="light page-post">


<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>SparkSQL 入门 | 邢大强的blog</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="Spark,SparkSQL,">
  

  <meta name="description" content="1. 概览1.1 简述SparkSQL 是 Spark 计算框架的一个模块。与基础的 Spark RDD API 不同，SparkSQL 为 Spark 提供了更多的数据结构 schema 信息。在内部，SparkSQL 使用这些额外的数据结构信息做进一步的优化操作。与 SparkSQL 交互的方式有多种，包括SQL 语句交互、Dataset API 交互。当使用 SparkSQL 获取数据处理结">
<meta name="keywords" content="Spark,SparkSQL">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkSQL 入门">
<meta property="og:url" content="https://miracle-xing.github.io/2019/08/27/SparkSQL-入门/index.html">
<meta property="og:site_name" content="邢大强的blog">
<meta property="og:description" content="1. 概览1.1 简述SparkSQL 是 Spark 计算框架的一个模块。与基础的 Spark RDD API 不同，SparkSQL 为 Spark 提供了更多的数据结构 schema 信息。在内部，SparkSQL 使用这些额外的数据结构信息做进一步的优化操作。与 SparkSQL 交互的方式有多种，包括SQL 语句交互、Dataset API 交互。当使用 SparkSQL 获取数据处理结">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/08/27/6e0cbcb0-c878-11e9-a36a-35ce21c8b830.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/08/27/8f63f9d0-c87a-11e9-a36a-35ce21c8b830.png">
<meta property="og:updated_time" content="2019-08-29T18:44:26.067Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SparkSQL 入门">
<meta name="twitter:description" content="1. 概览1.1 简述SparkSQL 是 Spark 计算框架的一个模块。与基础的 Spark RDD API 不同，SparkSQL 为 Spark 提供了更多的数据结构 schema 信息。在内部，SparkSQL 使用这些额外的数据结构信息做进一步的优化操作。与 SparkSQL 交互的方式有多种，包括SQL 语句交互、Dataset API 交互。当使用 SparkSQL 获取数据处理结">
<meta name="twitter:image" content="https://miracle-xing.github.io/images/2019/08/27/6e0cbcb0-c878-11e9-a36a-35ce21c8b830.png">

  

  
    <link rel="icon" href="/assets/img/m.png">
  

  <link href="/css/styles.css?v=c114cbeddx" rel="stylesheet">


  
    <link rel="stylesheet" href="/css/personal-style.css">
  

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-38189205-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?57e94d016e201fba3603a8a2b0263af0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  
  <script type="text/javascript">
	(function(){
	    var bp = document.createElement('script');
	    var curProtocol = window.location.protocol.split(':')[0];
	    if (curProtocol === 'https') {
	        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
	    }
	    else {
	        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
	    }
	    var s = document.getElementsByTagName("script")[0];
	    s.parentNode.insertBefore(bp, s);
	})();
  </script>



  
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">
  

</head>
</html>
<body>


  
    <span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/archives/"
            rel="noopener noreferrer"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/category/"
            rel="noopener noreferrer"
            target="_self"
            >
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tag/"
            rel="noopener noreferrer"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/about/"
            rel="noopener noreferrer"
            target="_self"
            >
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/atom.xml"
            rel="noopener noreferrer"
            target="_blank"
            >
            RSS
          </a>
        </li>
      
    </ul>
  </div>


</div>


  <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-概览"><span class="toc-text">1. 概览</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-简述"><span class="toc-text">1.1 简述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-SparkSQL-的用法之一：SQL-语句交互"><span class="toc-text">1.2 SparkSQL 的用法之一：SQL 语句交互</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-SparkSQL-的用法之二：Dataset-API-交互"><span class="toc-text">1.3 SparkSQL 的用法之二：Dataset API 交互</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-SparkSession"><span class="toc-text">1.4 SparkSession</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-5-Partition-分区"><span class="toc-text">1.5 Partition 分区</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-6-Transformation"><span class="toc-text">1.6 Transformation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-7-延迟计算"><span class="toc-text">1.7 延迟计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-8-Action"><span class="toc-text">1.8 Action</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-9-注册为表或视图"><span class="toc-text">1.9 注册为表或视图</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-重要概念"><span class="toc-text">2. 重要概念</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-DataFrame"><span class="toc-text">2.1 DataFrame</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-Dataset"><span class="toc-text">2.2 Dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-DataFrame-和-Dataset"><span class="toc-text">2.3 DataFrame 和 Dataset</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-代码实战"><span class="toc-text">3. 代码实战</span></a></li></ol>
  </div>



<div class="content content-post CENTER">
   <article id="post-SparkSQL-入门" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">SparkSQL 入门</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2019.08.27</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>Miracle</span>
        </span>
      

      
  <span class="article-category">
    <i class="icon-list"></i>
    <a class="article-category-link" href="/categories/笔记/">笔记</a>
  </span>



      

      
      <i class="fa fa-eye"></i> 
        <span id="busuanzi_container_page_pv">
           &nbsp热度 <span id="busuanzi_value_page_pv">
           <i class="fa fa-spinner fa-spin"></i></span>℃
        </span>
      
      
    </div>
  </header>

  <div class="article-content">
    
      <h1 id="1-概览"><a href="#1-概览" class="headerlink" title="1. 概览"></a>1. 概览</h1><h2 id="1-1-简述"><a href="#1-1-简述" class="headerlink" title="1.1 简述"></a>1.1 简述</h2><p>SparkSQL 是 Spark 计算框架的一个模块。与基础的 Spark RDD API 不同，SparkSQL 为 Spark 提供了更多的<strong>数据结构 schema</strong> 信息。在内部，SparkSQL 使用这些额外的数据结构信息做进一步的<strong>优化操作</strong>。与 SparkSQL 交互的方式有多种，包括<strong>SQL 语句交互</strong>、<strong>Dataset API 交互</strong>。当使用 SparkSQL 获取数据处理结果时，无论使用什么样的交互方式，无论什么样的语言编写的程序，其底层的执行引擎都是相同的。这就意味着，Spark 开发人员可以很容易在不同 API 之间来回切换，而不同担心性能方面的问题。</p>
<a id="more"></a>
<p><img src="/images/2019/08/27/6e0cbcb0-c878-11e9-a36a-35ce21c8b830.png" alt="spark框架.png"></p>
<h2 id="1-2-SparkSQL-的用法之一：SQL-语句交互"><a href="#1-2-SparkSQL-的用法之一：SQL-语句交互" class="headerlink" title="1.2 SparkSQL 的用法之一：SQL 语句交互"></a>1.2 SparkSQL 的用法之一：SQL 语句交互</h2><p>SparkSQL 的用途之一是执行 SQL 查询。SparkSQL 也可以从已有 Hive数据仓库中读取数据(Spark on Hive)。SparkSQL 语句的执行结果是一个 <strong>DataFrame或Dataset</strong> 对象。同时支持命令行执行 SQL 和 JDBC/ODBC 连接。</p>
<h2 id="1-3-SparkSQL-的用法之二：Dataset-API-交互"><a href="#1-3-SparkSQL-的用法之二：Dataset-API-交互" class="headerlink" title="1.3 SparkSQL 的用法之二：Dataset API 交互"></a>1.3 SparkSQL 的用法之二：Dataset API 交互</h2><p>Dataset 是一个<strong>分布式的数据集合</strong>。在 Spark 1.6 版本中，Dataset 作为一个新接口添加进来，兼具 RDD 的优点（强类型、使用强大的 lambda 函数的能力）和 SparkSQL 优化引擎的优势。Dataset 可以从 JVM 对象来构建，然后使用一系列转换函数（map、flatmap、filter等）进行计算。Dataset API 在 Scala 和 Java 语言中是支持的，Python 语言不支持 Dataset API。但是，由于 Python 的动态特性，Dataset API 的许多优势，Python 语言已经具备了。R 语言类似。</p>
<p>DataFrame(Dataset&lt;Row&gt;)是一个由<strong>命令列</strong>组成的 Dataset。<strong>概念上相当于关系型数据库中的一个表</strong>，但底层提供了更丰富的优化操作。DataFrames 可以从一系列广泛的数据来源中构建，如结构化数据文件，Hive 中的表、外部数据库、或者已有 RDD。</p>
<p>DataFrame API 在 Scala、Java、Python、R 语言中都支持。在 Scala API 中，DataFrame 是 Dataset[Row] 的类型别名。但是在 Java API 中，开发人员需要使用 Dataset&lt;Row&gt; 来表示一个 DataFrame。</p>
<h2 id="1-4-SparkSession"><a href="#1-4-SparkSession" class="headerlink" title="1.4 SparkSession"></a>1.4 SparkSession</h2><p>SparkSession 提供了与底层 Spark 功能交互的入口，允许使用 DataFrame 和 Dataset API 对 Spark 进行编程。最重要的是，<strong>它限制了概念的数量</strong>（SparkContext、SQLContext 和 HiveContext），并且构建了开发人员在与 Spark 交互时必须兼顾的结构。</p>
<p>启动 spark-shell 控制台时，SparkSession 被实例化为 Spark 变量，可以直接使用。<br><img src="/images/2019/08/27/8f63f9d0-c87a-11e9-a36a-35ce21c8b830.png" alt="sparkshell.png"></p>
<p>运行代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val myRange = spark.range(1000).toDF(&quot;number&quot;)</span><br></pre></td></tr></table></figure>

<p>我们创建了一个 DataFrame(Dataset&lt;Row&gt;)，其中一个列包含 1000 行，值从 0 到 999。这一系列数字代表一个分布式集合。</p>
<p><strong>注意</strong>：<br>DataFrame 的概念并不是 Spark 特有的。R 和 Python 都有类似的概念。然而，Python/R DataFrames（有一些列外）存在于一台机器上，而不是多台机器上。这限制了给定的 DataFrame 只能使用某一台特定机器上存在的资源 。但是，因为 Spark 具有 Python 和 R 的语言接口，很容易将 Pandas(Python) 的 DataFrames、R DataFrames 转换为 Spark DataFrames。</p>
<p>DataFrame 和 Dataset 具有和 RDD 相同的概念。</p>
<h2 id="1-5-Partition-分区"><a href="#1-5-Partition-分区" class="headerlink" title="1.5 Partition 分区"></a>1.5 Partition 分区</h2><p>为了使每个 executor 执行器并行执行任务，Spark 将数据分为 partition（分区）。每个分区是集群中的一个物理机器上的<strong>行集合</strong>。DataFrame 的分区表示了在执行过程中数据是如何在集群中物理分布的。</p>
<p>需要注意的是，对于 DataFrames 操作，<strong>大多数情况下不需要手动或单独操作分区</strong>，因为使用 DataFrame 的高级 transformation 操作，底层会做一些优化操作，然后转化为 RDD 进行计算。</p>
<h2 id="1-6-Transformation"><a href="#1-6-Transformation" class="headerlink" title="1.6 Transformation"></a>1.6 Transformation</h2><p>执行一个简单的转换，以在当前的 DataFrame 中找到所有偶数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 使用 Dataset API</span><br><span class="line">Dataset&lt;Row&gt; where = number.where(&quot;number % 2 = 0&quot;);</span><br><span class="line">// where.show();</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：这些操作没有输出。这是因为我们只声明了一个抽象转换（Transformation) where。Spark 将不会对转换进行操作，知道我们调用一个 Action 操作。</p>
<h2 id="1-7-延迟计算"><a href="#1-7-延迟计算" class="headerlink" title="1.7 延迟计算"></a>1.7 延迟计算</h2><p>延迟计算意味着 Spark 将等到最后一刻才执行一系列 Transformation。在 SparkSQL 中，不会在执行某个 Transformation 操作时立即修改数据，Spark 会构建一个应用于源数据的 Plan。直到最后 Action 时执行代码，Spark 将这个计划从原始的 DataFrame 转换为 Physical Plan，该计划将在整个集群中高效地运行，因为 Spark 可以从端到端优化整个数据流。</p>
<p>例如 DataFrame 的<strong>谓词下推</strong> pushdown 优化方式：如果我们构建一个大型的 Spark 作业，在最后指定一个过滤器 where，只需要从源数据中获取一行。最有效的执行方式从数据源过滤所需的单个记录。Spark 实际上是通过自动将过滤器下推来优化的。</p>
<h2 id="1-8-Action"><a href="#1-8-Action" class="headerlink" title="1.8 Action"></a>1.8 Action</h2><p>为了触发计算，需要运行一个 Action 操作。Action 操作使 Spark 通过执行一系列 Transformation 转换，得到计算结果。最简单的 Action 操作是 <strong>count</strong>，它给出了 DataFrame 中记录的总数（行数）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">System.out.println(where.count());</span><br></pre></td></tr></table></figure>

<p>输出：500</p>
<p>有三种类型的 Action：</p>
<ul>
<li>在控制台中查看数据的 Action</li>
<li>数据收集的 Action 操作</li>
<li>输出到第三方存储系统的 Action 操作</li>
</ul>
<p>在执行这个 count 操作时，启动了一个 Spark job，运行过滤器 where 转换（一个窄依赖转换），然后是一个聚合（一个宽依赖转换），它在每个分区基础上执行计数，然后是一个收集 Action，它将我们的结果返回到 driver 端。通过检查 Spark UI，可以看到所有这一切。</p>
<h2 id="1-9-注册为表或视图"><a href="#1-9-注册为表或视图" class="headerlink" title="1.9 注册为表或视图"></a>1.9 注册为表或视图</h2><p>可以通过一个简单的方法 将任何 DataFrame 转换为一个表或视图：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">number.createOrReplaceTempView(&quot;number&quot;);</span><br><span class="line">Dataset&lt;Row&gt; sql = spark.sql(&quot;select * from number where number % 2 =0&quot;);</span><br></pre></td></tr></table></figure>

<p>现在我们可以用 SQL 查询我们的数据了。使用 spark.sql(sqlString)，spark 是我们的 SparkSession 变量，返回一个新的 DataFrame。</p>
<p>Spark 中的 DataFrames（和 SQL ) 已经有大量可用的操作。您可以使用和导入数百个函数来帮助您更快地解决大数据问题。</p>
<h1 id="2-重要概念"><a href="#2-重要概念" class="headerlink" title="2. 重要概念"></a>2. 重要概念</h1><h2 id="2-1-DataFrame"><a href="#2-1-DataFrame" class="headerlink" title="2.1 DataFrame"></a>2.1 DataFrame</h2><ul>
<li>SparkSQL 从 Spark 1.3 开始引入了一个名为 DataFrame 的表格式数据抽象。</li>
<li>DataFrame 是用于处理结构化和半结构化的数据抽象。</li>
<li>DataFrame 利用其 <strong>Schema</strong> 以比原始 RDDs 更有效的方式存储数据。</li>
<li>DataFrame 利用 RDD 的不可变的、内存计算的、弹性的、分布式的和并行的特性，并对数据应用一个称为 Schema 的数据结构，允许 Spark 管理 Schema，以比 Java 序列化更有效的方式在集群节点之间传递数据。</li>
<li>与 RDD 不同，DataFrame 中的数据被组织到指定的 columns 中，就像关系数据库中的表一样。</li>
</ul>
<h2 id="2-2-Dataset"><a href="#2-2-Dataset" class="headerlink" title="2.2 Dataset"></a>2.2 Dataset</h2><ul>
<li>从 Spark 1.6 版本开始提供 Dataset API, Dataset API提供了：<ul>
<li>面向对象的编程风格</li>
<li>像 RDD API 一样的<strong>编译时类型安全，编译时异常，运行时异常</strong></li>
<li>利用 Schema 处理结构化数据的优势</li>
</ul>
</li>
<li>Dataset 是结构化数据集，数据集泛型可以是 Row(DataFrame)，也可以是特定的数据类型。</li>
<li>Java 和 Spark 在编译时将指导数据集中数据的类型。</li>
</ul>
<h2 id="2-3-DataFrame-和-Dataset"><a href="#2-3-DataFrame-和-Dataset" class="headerlink" title="2.3 DataFrame 和 Dataset"></a>2.3 DataFrame 和 Dataset</h2><ul>
<li>从 Spark 2.0 开始，DataFrame API 和 Dataset API 合并。</li>
<li>Dataset 提供了两个截然不同的 API 特性：strong typed API 和 untyped API。</li>
<li>可以将 DataFrame 看作是 Dataset 的 untyped 类型：Dataset&lt;Row&gt;，Row 是一个 untyped 的 JVM 对象。</li>
<li>Dataset 是强类型 JVM 对象的集合。</li>
</ul>
<h1 id="3-代码实战"><a href="#3-代码实战" class="headerlink" title="3. 代码实战"></a>3. 代码实战</h1><p>创建 DataFrame 方式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// 集合创建 DataFrame</span><br><span class="line">List&lt;Person&gt; personList = new ArrayList&lt;&gt;();</span><br><span class="line">personList.add(new Person(&quot;张三&quot;, 20, &quot;深圳&quot;));</span><br><span class="line">personList.add(new Person(&quot;李四&quot;, 22, &quot;天津&quot;));</span><br><span class="line">Dataset&lt;Row&gt; dataFrame = spark.createDataFrame(personList, Person.class);</span><br><span class="line">dataFrame.printSchema();</span><br><span class="line">dataFrame.show();</span><br><span class="line"></span><br><span class="line">// RDD 创建 DataFrame</span><br><span class="line">JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());</span><br><span class="line">JavaRDD&lt;Person&gt; parallelize = jsc.parallelize(personList);</span><br><span class="line">Dataset&lt;Row&gt; dataFrame1 = spark.createDataFrame(parallelize, Person.class);</span><br><span class="line">dataFrame1.printSchema();</span><br><span class="line">dataFrame1.show();</span><br><span class="line"></span><br><span class="line">// Json/CSV 文件创建 DataFrame</span><br><span class="line">Dataset&lt;Row&gt; json = spark.read().json(&quot;E:\\Java\\IntelliJ IDEA\\spark\\in\\2015-summary.json&quot;);</span><br><span class="line">json.createOrReplaceTempView(&quot;summary&quot;);</span><br><span class="line">Dataset&lt;Row&gt; sql = spark.sql(&quot;select * from summary where count &gt; 50 order by count desc limit 10&quot;);</span><br><span class="line">sql.show();</span><br></pre></td></tr></table></figure>
    
  </div>

</article>


   
  <div class="text-center donation">
    <div class="inner-donation">
      <span class="btn-donation">支持一下</span>
      <div class="donation-body">
        <div class="tip text-center">扫一扫，支持forsigner</div>
        <ul>
        
          <li class="item">
            
              <span>微信扫一扫</span>
            
            <img src="/images/qr-wechat.png" alt="">
          </li>
        
          <li class="item">
            
              <span>支付宝扫一扫</span>
            
            <img src="/images/qr-alipay.png" alt="">
          </li>
        
        </ul>
      </div>
    </div>
  </div>


   
  <div class="box-prev-next clearfix">
    <a class="show pull-left" href="/2019/08/18/自定义分区器-二次排序/">
        <i class="icon icon-angle-left"></i>
    </a>
    <a class="show pull-right" href="/2019/08/28/DataFrame-操作详解/">
        <i class="icon icon-angle-right"></i>
    </a>
  </div>




</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">关闭</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              rel="noopener noreferrer"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/category/"
              rel="noopener noreferrer"
              target="_self"
              >
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              rel="noopener noreferrer"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              rel="noopener noreferrer"
              target="_self"
              >
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/atom.xml"
              rel="noopener noreferrer"
              target="_blank"
              >
              RSS
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    

    

    
    

    

    
    

    

<!-- Gitalk评论插件通用代码 -->
<div id="gitalk-container"></div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>

<script>
const gitalk = new Gitalk({
  clientID: 'cffabda338955fb33e72',
  clientSecret: '27685d32607acc9c76041016860f5434fa1d65d0',
  repo: 'gitalk_comment',
  owner: 'Miracle-Xing',
  // 在这里设置一下截取前50个字符串, 这是因为 github 对 label 的长度有了要求, 如果超过
  // 50个字符串则会报错.
  id: location.pathname.split('/').pop().substring(0, 49),
  // id: location.pathname,
  //id: title,
  admin: ['Miracle-Xing'],
  // facebook-like distraction free mode
  distractionFreeMode: false
})
gitalk.render('gitalk-container')
</script>
<!-- Gitalk代码结束 -->



  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

</body>
</html>

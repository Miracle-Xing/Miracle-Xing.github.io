<!DOCTYPE html>


  <html class="light page-post">


<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>SparkSQL 聚合函数、UDF 自定义函数、Spark on Hive | 邢大强的blog</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="Spark,SparkSQL,">
  

  <meta name="description" content="1. 聚合函数123456// 聚合函数// 4. 计算名为 CA 的 state，每个 city 的 zip 总数、人口总量// APIds.where(&quot;state = &apos;CA&apos;&quot;).groupBy(&quot;city&quot;).agg(functions.count(&quot;zip&quot;).alias(&quot;zip_count&quot;), functions.sum(&quot;pop&quot;).alias(&quot;total_pop&quot;)).sh">
<meta name="keywords" content="Spark,SparkSQL">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkSQL 聚合函数、UDF 自定义函数、Spark on Hive">
<meta property="og:url" content="https://miracle-xing.github.io/2019/08/30/SparkSQL-聚合函数、UDF-自定义函数、Spark-on-Hive/index.html">
<meta property="og:site_name" content="邢大强的blog">
<meta property="og:description" content="1. 聚合函数123456// 聚合函数// 4. 计算名为 CA 的 state，每个 city 的 zip 总数、人口总量// APIds.where(&quot;state = &apos;CA&apos;&quot;).groupBy(&quot;city&quot;).agg(functions.count(&quot;zip&quot;).alias(&quot;zip_count&quot;), functions.sum(&quot;pop&quot;).alias(&quot;total_pop&quot;)).sh">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-09-02T08:07:10.092Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SparkSQL 聚合函数、UDF 自定义函数、Spark on Hive">
<meta name="twitter:description" content="1. 聚合函数123456// 聚合函数// 4. 计算名为 CA 的 state，每个 city 的 zip 总数、人口总量// APIds.where(&quot;state = &apos;CA&apos;&quot;).groupBy(&quot;city&quot;).agg(functions.count(&quot;zip&quot;).alias(&quot;zip_count&quot;), functions.sum(&quot;pop&quot;).alias(&quot;total_pop&quot;)).sh">

  

  
    <link rel="icon" href="/assets/img/m.png">
  

  <link href="/css/styles.css?v=c114cbeddx" rel="stylesheet">


  
    <link rel="stylesheet" href="/css/personal-style.css">
  

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-38189205-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?57e94d016e201fba3603a8a2b0263af0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  
  <script type="text/javascript">
	(function(){
	    var bp = document.createElement('script');
	    var curProtocol = window.location.protocol.split(':')[0];
	    if (curProtocol === 'https') {
	        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
	    }
	    else {
	        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
	    }
	    var s = document.getElementsByTagName("script")[0];
	    s.parentNode.insertBefore(bp, s);
	})();
  </script>



  
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">
  

</head>
</html>
<body>


  
    <span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/archives/"
            rel="noopener noreferrer"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/category/"
            rel="noopener noreferrer"
            target="_self"
            >
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tag/"
            rel="noopener noreferrer"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/about/"
            rel="noopener noreferrer"
            target="_self"
            >
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/atom.xml"
            rel="noopener noreferrer"
            target="_blank"
            >
            RSS
          </a>
        </li>
      
    </ul>
  </div>


</div>


  <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-聚合函数"><span class="toc-text">1. 聚合函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-UDF-自定义函数"><span class="toc-text">2. UDF 自定义函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Spark-on-Hive"><span class="toc-text">3. Spark on Hive</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-Spark-on-Hive-与-Hive-on-Spark-区别？"><span class="toc-text">3.1 Spark on Hive 与 Hive on Spark 区别？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-Spark-on-Hive"><span class="toc-text">3.2 Spark on Hive</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-相关组件"><span class="toc-text">3.2.1 相关组件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-如何配置-Spark-on-Hive"><span class="toc-text">3.2.2 如何配置 Spark on Hive?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-23-Spark-on-Hive-练习"><span class="toc-text">3.23 Spark on Hive 练习</span></a></li></ol></li></ol></li></ol>
  </div>



<div class="content content-post CENTER">
   <article id="post-SparkSQL-聚合函数、UDF-自定义函数、Spark-on-Hive" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">SparkSQL 聚合函数、UDF 自定义函数、Spark on Hive</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2019.08.30</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>Miracle</span>
        </span>
      

      
  <span class="article-category">
    <i class="icon-list"></i>
    <a class="article-category-link" href="/categories/笔记/">笔记</a>
  </span>



      

      
      <i class="fa fa-eye"></i> 
        <span id="busuanzi_container_page_pv">
           &nbsp热度 <span id="busuanzi_value_page_pv">
           <i class="fa fa-spinner fa-spin"></i></span>℃
        </span>
      
      
    </div>
  </header>

  <div class="article-content">
    
      <h1 id="1-聚合函数"><a href="#1-聚合函数" class="headerlink" title="1. 聚合函数"></a>1. 聚合函数</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 聚合函数</span></span><br><span class="line"><span class="comment">// 4. 计算名为 CA 的 state，每个 city 的 zip 总数、人口总量</span></span><br><span class="line"><span class="comment">// API</span></span><br><span class="line">ds.where(<span class="string">"state = 'CA'"</span>).groupBy(<span class="string">"city"</span>).agg(functions.count(<span class="string">"zip"</span>).alias(<span class="string">"zip_count"</span>), functions.sum(<span class="string">"pop"</span>).alias(<span class="string">"total_pop"</span>)).show();</span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">spark.sql(<span class="string">"select city, count(zip) as zip_count, sum(pop) as total_pop from zips where state = 'CA' group by city"</span>).show();</span><br></pre></td></tr></table></figure>

<h1 id="2-UDF-自定义函数"><a href="#2-UDF-自定义函数" class="headerlink" title="2. UDF 自定义函数"></a>2. UDF 自定义函数</h1><p>如果 Spark 内置函数不够用，那么可以自定义函数。<br>注册一个 UDF，用于将 String 类型的数据转为 Long 类型。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// API</span></span><br><span class="line">UserDefinedFunction zipToLong = functions.udf((String zip) -&gt; Long.valueOf(zip), DataTypes.LongType);</span><br><span class="line">Dataset&lt;Row&gt; zip = ds.select(zipToLong.apply(functions.col(<span class="string">"zip"</span>)));</span><br><span class="line">zip.printSchema();</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">spark.udf().register(<span class="string">"zipToLong"</span>, (String zip1) -&gt; Long.valueOf(zip1), DataTypes.LongType);</span><br><span class="line">spark.sql(<span class="string">"select zipToLong(zip) from zips"</span>).printSchema();</span><br></pre></td></tr></table></figure>

<p>以上是两种不同的定义方式，① 定义 UDF 函数，只能在 Dataset API 中使用；② 将 UDF 函数注册到 SparkSession 中，在 SQL 中调用。<br>UDF 是一个函数，但是 UDF 不仅仅是一个函数，有自己特殊性：需要将 UDF 的参数看作数据表的某个列。<br>在使用 UDF 时，不一定非要传入列，还可以传入常量。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// API</span></span><br><span class="line">UserDefinedFunction largerThan = functions.udf((String zip2, Long number) -&gt; Long.valueOf(zip2) &gt; number, DataTypes.BooleanType);</span><br><span class="line">ds.select(</span><br><span class="line">        zipToLong.apply(functions.col(<span class="string">"zip"</span>)).as(<span class="string">"zipToLong"</span>),</span><br><span class="line">        functions.col(<span class="string">"city"</span>),</span><br><span class="line">        largerThan.apply(functions.col(<span class="string">"zip"</span>), functions.lit(<span class="number">99923L</span>)).as(<span class="string">"largerThanConst"</span>)</span><br><span class="line">)</span><br><span class="line">        .orderBy(functions.desc(<span class="string">"zipToLong"</span>))</span><br><span class="line">        .show();</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">spark.udf().register(<span class="string">"largerThan"</span>,(String zip3, Long number)-&gt; Long.valueOf(zip3) &gt; number, DataTypes.BooleanType);</span><br><span class="line">spark.sql(<span class="string">"select zipToLong(zip) as zipToLong, city, largerThan(zip,99923L) as largerThanConst from zips order by zipToLong desc"</span>).show();</span><br></pre></td></tr></table></figure>

<h1 id="3-Spark-on-Hive"><a href="#3-Spark-on-Hive" class="headerlink" title="3. Spark on Hive"></a>3. Spark on Hive</h1><h2 id="3-1-Spark-on-Hive-与-Hive-on-Spark-区别？"><a href="#3-1-Spark-on-Hive-与-Hive-on-Spark-区别？" class="headerlink" title="3.1 Spark on Hive 与 Hive on Spark 区别？"></a>3.1 Spark on Hive 与 Hive on Spark 区别？</h2><p>Spark on Hive 是将 Hive 作为数据库，Spark 读取 Hive 的数据做计算；<br>Hive on Spark 是 Hive 使用 Spark 作为计算工具，相对于 Hive on MapReduce。</p>
<h2 id="3-2-Spark-on-Hive"><a href="#3-2-Spark-on-Hive" class="headerlink" title="3.2 Spark on Hive"></a>3.2 Spark on Hive</h2><h3 id="3-2-1-相关组件"><a href="#3-2-1-相关组件" class="headerlink" title="3.2.1 相关组件"></a>3.2.1 相关组件</h3><ul>
<li><p>是否需要启动 Hive?<br>不需要</p>
</li>
<li><p>是否需要启动 HDFS?<br>需要</p>
</li>
<li><p>是否需要启动 YARN?<br>不需要</p>
</li>
</ul>
<h3 id="3-2-2-如何配置-Spark-on-Hive"><a href="#3-2-2-如何配置-Spark-on-Hive" class="headerlink" title="3.2.2 如何配置 Spark on Hive?"></a>3.2.2 如何配置 Spark on Hive?</h3><p><strong>服务器环境</strong>：</p>
<ul>
<li>将 hive-site.xml、core-site.xml、hdfs-site.xml 拷贝到 /opt/modules/spark243/conf 文件夹下</li>
<li>在拷贝到 spark_home/conf 所在节点上以 local 模式启动 spark-sql，验证连接</li>
<li>如果 hive 的 metastore 是 mysql 数据库，需要将 mysql 驱动放到 spark_home/jars 目录下面</li>
</ul>
<p><strong>开发环境</strong>：</p>
<ul>
<li>项目根目录创建 conf 文件夹，标记为 Resources Root，将 hive-site.xml、core-site.xml、hdfs-site.xml 复制到该目录</li>
<li>pom.xml 添加 mysql 连接工具包依赖<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.47<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<p><strong>示例代码</strong>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SparkSession spark = SparkSession.builder()</span><br><span class="line">        .master(<span class="string">"local[*]"</span>)</span><br><span class="line">        .appName(<span class="string">"SparkSQL_d3"</span>)</span><br><span class="line">        .enableHiveSupport()</span><br><span class="line">        .getOrCreate();</span><br><span class="line">spark.sparkContext().setLogLevel(<span class="string">"ERROR"</span>);</span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">"create table if not exists student(num int, name string) row format delimited fields terminated by ','"</span>);</span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">"load data inpath 'hdfs://ns/in/student' into table student"</span>);</span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">"select * from student"</span>).show();</span><br></pre></td></tr></table></figure>

<h3 id="3-23-Spark-on-Hive-练习"><a href="#3-23-Spark-on-Hive-练习" class="headerlink" title="3.23 Spark on Hive 练习"></a>3.23 Spark on Hive 练习</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 最贵的10类手机</span></span><br><span class="line"><span class="comment">         * 销量最好的10类手机</span></span><br><span class="line"><span class="comment">         * "dpmc","cpmc","cpjg","cpxl","splj","cppj"</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        spark.sql(<span class="string">"create table if not exists phone (dpmc string, cpmc string, cpjg string, cpxl string, splj string, cppj string) tblproperties('skip.header.line.count'=1) row format delimited fields terminated by ','"</span>);</span><br><span class="line">        spark.sql(<span class="string">"load data inpath 'hdfs://ns/in/phone.csv' into table phone"</span>);</span><br><span class="line"></span><br><span class="line">        spark.sql(<span class="string">"select * from phone"</span>).show();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 最贵的 10 类手机</span></span><br><span class="line">        spark.udf().register(<span class="string">"getPrice"</span>, (String p) -&gt; Double.parseDouble(p.substring(<span class="number">2</span>, p.length() - <span class="number">1</span>)), DataTypes.DoubleType);</span><br><span class="line">        spark.sql(<span class="string">"select cpmc, getPrice(cpjg) as price from phone order by price desc limit 10"</span>).show();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 销量最好的 10 类手机</span></span><br><span class="line">        spark.udf().register(<span class="string">"getSales"</span>, (String str) -&gt; &#123;</span><br><span class="line">            <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">if</span> (str != <span class="keyword">null</span> &amp;&amp; !str.equals(<span class="string">""</span>)) &#123;</span><br><span class="line">                String numStr = str.substring(<span class="number">7</span>, str.length() - <span class="number">1</span>).replaceAll(<span class="string">"笔"</span>, <span class="string">""</span>);</span><br><span class="line">                <span class="keyword">if</span> (numStr.contains(<span class="string">"万"</span>)) &#123;</span><br><span class="line">                    result = (<span class="keyword">int</span>) (Double.valueOf(numStr.replaceAll(<span class="string">"万"</span>, <span class="string">""</span>)) * <span class="number">10000</span>);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    result = Integer.valueOf(numStr);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> result;</span><br><span class="line">        &#125;, DataTypes.IntegerType);</span><br><span class="line"></span><br><span class="line">        Dataset&lt;Row&gt; df = spark.sql(<span class="string">"select cpmc, getSales(cpxl) as sales from phone order by sales desc limit 10"</span>);</span><br><span class="line">        <span class="comment">// 存入 Hive 仓库</span></span><br><span class="line">        df.write().saveAsTable(<span class="string">"sales"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 验证</span></span><br><span class="line">        spark.sql(<span class="string">"select * from sales"</span>).show();</span><br></pre></td></tr></table></figure>


    
  </div>

</article>


   
  <div class="text-center donation">
    <div class="inner-donation">
      <span class="btn-donation">支持一下</span>
      <div class="donation-body">
        <div class="tip text-center">扫一扫，支持forsigner</div>
        <ul>
        
          <li class="item">
            
              <span>微信扫一扫</span>
            
            <img src="/images/qr-wechat.png" alt="">
          </li>
        
          <li class="item">
            
              <span>支付宝扫一扫</span>
            
            <img src="/images/qr-alipay.png" alt="">
          </li>
        
        </ul>
      </div>
    </div>
  </div>


   
  <div class="box-prev-next clearfix">
    <a class="show pull-left" href="/2019/08/28/DataFrame-操作详解/">
        <i class="icon icon-angle-left"></i>
    </a>
    <a class="show pull-right" href="/2019/09/01/Hive-安装及使用/">
        <i class="icon icon-angle-right"></i>
    </a>
  </div>




</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">关闭</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              rel="noopener noreferrer"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/category/"
              rel="noopener noreferrer"
              target="_self"
              >
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              rel="noopener noreferrer"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              rel="noopener noreferrer"
              target="_self"
              >
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/atom.xml"
              rel="noopener noreferrer"
              target="_blank"
              >
              RSS
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    

    

    
    

    

    
    

    

<!-- Gitalk评论插件通用代码 -->
<div id="gitalk-container"></div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="https://cdn.bootcss.com/blueimp-md5/2.11.1/js/md5.js"></script>
<script>
const gitalk = new Gitalk({
  clientID: 'cffabda338955fb33e72',
  clientSecret: '27685d32607acc9c76041016860f5434fa1d65d0',
  repo: 'gitalk_comment',
  owner: 'Miracle-Xing',
  // 在这里设置一下截取前50个字符串, 这是因为 github 对 label 的长度有了要求, 如果超过
  // 50个字符串则会报错.
  //id: location.pathname.split('/').pop().substring(0, 49),
   id: md5(location.pathname),
  // id: title,
  admin: ['Miracle-Xing'],
  // facebook-like distraction free mode
  distractionFreeMode: false
})
gitalk.render('gitalk-container')
</script>
<!-- Gitalk代码结束 -->



  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

</body>
</html>

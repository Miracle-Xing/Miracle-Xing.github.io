<!DOCTYPE html>


  <html class="light page-post">


<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>DataFrame 操作详解 | 邢大强的blog</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="Spark,SparkSQL,">
  

  <meta name="description" content="DataFrame 和 Dataset 是（分布式的）类似于表的集合，具有定义好的行和列。每个列必须具有与所有其他列相同的行数（尽管您可以使用 null 来指定值的缺失），并且每个列都有类型信息，这些信息必须与集合中的每一行一致。这是因为内部存在一个 schema 的概念，其定义了分布式集合中存储的数据的类型。">
<meta name="keywords" content="Spark,SparkSQL">
<meta property="og:type" content="article">
<meta property="og:title" content="DataFrame 操作详解">
<meta property="og:url" content="https://miracle-xing.github.io/2019/08/28/DataFrame-操作详解/index.html">
<meta property="og:site_name" content="邢大强的blog">
<meta property="og:description" content="DataFrame 和 Dataset 是（分布式的）类似于表的集合，具有定义好的行和列。每个列必须具有与所有其他列相同的行数（尽管您可以使用 null 来指定值的缺失），并且每个列都有类型信息，这些信息必须与集合中的每一行一致。这是因为内部存在一个 schema 的概念，其定义了分布式集合中存储的数据的类型。">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-08-29T18:27:58.534Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DataFrame 操作详解">
<meta name="twitter:description" content="DataFrame 和 Dataset 是（分布式的）类似于表的集合，具有定义好的行和列。每个列必须具有与所有其他列相同的行数（尽管您可以使用 null 来指定值的缺失），并且每个列都有类型信息，这些信息必须与集合中的每一行一致。这是因为内部存在一个 schema 的概念，其定义了分布式集合中存储的数据的类型。">

  

  
    <link rel="icon" href="/assets/img/m.png">
  

  <link href="/css/styles.css?v=c114cbeddx" rel="stylesheet">


  
    <link rel="stylesheet" href="/css/personal-style.css">
  

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-38189205-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?57e94d016e201fba3603a8a2b0263af0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  
  <script type="text/javascript">
	(function(){
	    var bp = document.createElement('script');
	    var curProtocol = window.location.protocol.split(':')[0];
	    if (curProtocol === 'https') {
	        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
	    }
	    else {
	        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
	    }
	    var s = document.getElementsByTagName("script")[0];
	    s.parentNode.insertBefore(bp, s);
	})();
  </script>



  
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">
  

</head>
</html>
<body>


  
    <span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/archives/"
            rel="noopener noreferrer"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/category/"
            rel="noopener noreferrer"
            target="_self"
            >
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tag/"
            rel="noopener noreferrer"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/about/"
            rel="noopener noreferrer"
            target="_self"
            >
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/atom.xml"
            rel="noopener noreferrer"
            target="_blank"
            >
            RSS
          </a>
        </li>
      
    </ul>
  </div>


</div>


  <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Schema"><span class="toc-text">1. Schema</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Rows"><span class="toc-text">2. Rows</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Column"><span class="toc-text">3. Column</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-Spark-DataType"><span class="toc-text">4. Spark DataType</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-DataFrame-基本操作"><span class="toc-text">5. DataFrame 基本操作</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-Columns-操作"><span class="toc-text">6. Columns 操作</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-字面常量转换为-Spark-类型（Literals）"><span class="toc-text">7. 字面常量转换为 Spark 类型（Literals）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-添加列"><span class="toc-text">8. 添加列</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-重命名列"><span class="toc-text">9. 重命名列</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-删除列"><span class="toc-text">10. 删除列</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-更改列类型"><span class="toc-text">11. 更改列类型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#12-过滤行"><span class="toc-text">12. 过滤行</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13-行去重"><span class="toc-text">13. 行去重</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#14-DataFrame-union"><span class="toc-text">14. DataFrame union</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#15-行排序"><span class="toc-text">15. 行排序</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#16-Limit"><span class="toc-text">16. Limit</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#17-Repartition-和-Coalesce"><span class="toc-text">17. Repartition 和 Coalesce</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#18-收集数据到-driver"><span class="toc-text">18. 收集数据到 driver</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#19-作业"><span class="toc-text">19. 作业</span></a></li></ol>
  </div>



<div class="content content-post CENTER">
   <article id="post-DataFrame-操作详解" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">DataFrame 操作详解</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2019.08.28</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>Miracle</span>
        </span>
      

      
  <span class="article-category">
    <i class="icon-list"></i>
    <a class="article-category-link" href="/categories/笔记/">笔记</a>
  </span>



      

      
      <i class="fa fa-eye"></i> 
        <span id="busuanzi_container_page_pv">
           &nbsp热度 <span id="busuanzi_value_page_pv">
           <i class="fa fa-spinner fa-spin"></i></span>℃
        </span>
      
      
    </div>
  </header>

  <div class="article-content">
    
      <p>DataFrame 和 Dataset 是（分布式的）类似于表的集合，具有定义好的行和列。每个列必须具有与所有其他列相同的行数（尽管您可以使用 null 来指定值的缺失），并且每个列都有类型信息，这些信息必须与集合中的每一行一致。这是因为内部存在一个 schema 的概念，其定义了分布式集合中存储的数据的类型。</p>
<a id="more"></a>
<h1 id="1-Schema"><a href="#1-Schema" class="headerlink" title="1. Schema"></a>1. Schema</h1><p>Schema 定义了 DataFrame 的列名和类型。您可以手动定义 Schema 模式或从数据源读取 Schema 模式（通常称为读模式）。Schema 包含列类型，用于声明<strong>什么列存储了什么类型的数据</strong>。</p>
<h1 id="2-Rows"><a href="#2-Rows" class="headerlink" title="2. Rows"></a>2. Rows</h1><p>一行（Row）只是表示数据的一条记录。DataFrame 中的每条数据记录必须是 Row 类型。我们可以从 SQL 、弹性分布式数据集（RDDs）、数据源或手动创建这些 Rows。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 返回 Row 对象数组</span></span><br><span class="line">Row[] collect = (Row[])spark.range(<span class="number">2</span>).toDF().collect();</span><br><span class="line"><span class="keyword">for</span> (Row r :</span><br><span class="line">        collect) &#123;</span><br><span class="line">    System.out.println(r);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="3-Column"><a href="#3-Column" class="headerlink" title="3. Column"></a>3. Column</h1><p>Column 表示一个简单的类型，如 Integer 或 String，复杂类型，如 Array 或 Map，或 Null。Spark 将跟踪所有这些类型的信息，并提供多种方式对 Column 进行转换。</p>
<h1 id="4-Spark-DataType"><a href="#4-Spark-DataType" class="headerlink" title="4. Spark DataType"></a>4. Spark DataType</h1><p>Spark 有大量的内部类型表示，这样就可以很容易地引用在特定语言（Java、Scala）中，与 Spark 类型相匹配的类型。import org.apache.spark.sql.types.DataTypes 类中。</p>
<p><strong>Scala type reference</strong>:</p>
<table>
<thead>
<tr>
<th>Data type</th>
<th>Value type in Scala</th>
<th>API to access or create a data type</th>
</tr>
</thead>
<tbody><tr>
<td>ByteType</td>
<td>Byte</td>
<td>ByteType</td>
</tr>
<tr>
<td>ShortType</td>
<td>Short</td>
<td>ShortType</td>
</tr>
<tr>
<td>IntegerType</td>
<td>Int</td>
<td>IntegerType</td>
</tr>
<tr>
<td>LongType</td>
<td>Long</td>
<td>LongType</td>
</tr>
<tr>
<td>FloatType</td>
<td>Float</td>
<td>FloatType</td>
</tr>
<tr>
<td>DoubleType</td>
<td>Double</td>
<td>DoubleType</td>
</tr>
<tr>
<td>DecimalType</td>
<td>java.math.BigDecimal</td>
<td>DecimalType</td>
</tr>
<tr>
<td>StringType</td>
<td>String</td>
<td>StringType</td>
</tr>
<tr>
<td>BinaryType</td>
<td>Array[Byte]</td>
<td>BinaryType</td>
</tr>
<tr>
<td>BooleanType</td>
<td>Boolean</td>
<td>BooleanType</td>
</tr>
<tr>
<td>TimestampType</td>
<td>java.sql.Timestamp</td>
<td>TimestampType</td>
</tr>
<tr>
<td>DateType</td>
<td>java.sql.Date</td>
<td>DateType</td>
</tr>
<tr>
<td>ArrayType</td>
<td>scala.collection.Seq</td>
<td>ArrayType(elementType, [containsNull]. Note: The default value of containsNull is true.</td>
</tr>
<tr>
<td>MapType</td>
<td>scala.collection.Map</td>
<td>MapType(keyType, valueType, [valueContainsNull). Note: The default value of valueContainsNull is true.</td>
</tr>
<tr>
<td>StructType</td>
<td>org.apache.spark.sql.Row</td>
<td>StructType(fields). Note: fields is an Array of StructFields. Also, fields with the same name are not allowed.</td>
</tr>
<tr>
<td>StructField</td>
<td>The value type in Scala of the data type of this field(for example, Int for a StructField  with the data type IntegerType)</td>
<td>StructField(name, dataType, [nullable]). Note: The default value of nullable is true.</td>
</tr>
</tbody></table>
<p><strong>Java type reference</strong>:</p>
<table>
<thead>
<tr>
<th>Data type</th>
<th>Value type in Java</th>
<th>API to access or create a data type</th>
</tr>
</thead>
<tbody><tr>
<td>ByteType</td>
<td>byte or Byte</td>
<td>DataTypes.ByteType</td>
</tr>
<tr>
<td>ShortType</td>
<td>short or Short</td>
<td>DataTypes.ShortType</td>
</tr>
<tr>
<td>IntegerType</td>
<td>int or Integer</td>
<td>DataTypes.IntegerType</td>
</tr>
<tr>
<td>LongType</td>
<td>long or Long</td>
<td>DataTypes.LongType</td>
</tr>
<tr>
<td>FloatType</td>
<td>float or Float</td>
<td>DataTypes.FloatType</td>
</tr>
<tr>
<td>DoubleType</td>
<td>double or Double</td>
<td>DataTypes.DoubleType</td>
</tr>
<tr>
<td>DecimalType</td>
<td>java.math.BigDecimal</td>
<td>DataTypes.createDecimalType(); DataTypes.createDecimalType(precision, scale).</td>
</tr>
<tr>
<td>StringType</td>
<td>String</td>
<td>DataTypes.StringType</td>
</tr>
<tr>
<td>BinaryType</td>
<td>byte[]</td>
<td>DataTypes.BinaryType</td>
</tr>
<tr>
<td>TimestmapType</td>
<td>java.sql.Timestamp</td>
<td>DataTypes.TimestampType</td>
</tr>
<tr>
<td>DateType</td>
<td>java.sql.Date</td>
<td>DataTypes.DateType</td>
</tr>
<tr>
<td>ArrayType</td>
<td>java.util.List</td>
<td>DataTypes.createArrayType(elementType). Note: The value of containsNull will be true; DataTypes.createArrayType(elementType, containsNull).</td>
</tr>
<tr>
<td>MapType</td>
<td>java.util.Map</td>
<td>DataTypes.createMapType(keyType, valueType). Note: The value of valueContainsNull will be true. DataTypes.createMapType(keyType, valueType, valueContainsNull)</td>
</tr>
<tr>
<td>StructType</td>
<td>org.apache.spark.sql.Row</td>
<td>DataTypes.createStructType(fields). Note: fields is a List or an array of StructFields. Also, two fields with the same name are not allowed.</td>
</tr>
<tr>
<td>StructField</td>
<td>The value type in Java of the data type of this field(for example, int for a StructField with the data typee IntegerType)</td>
<td>DataTypes.createStructField(name, dataType, nullable)</td>
</tr>
</tbody></table>
<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用 List&lt;Row&gt;、Schema 创建 DataFrame</span></span><br><span class="line"><span class="comment"> * 注意： 创建 StructField、StructType，要使用 DataTypes 的工厂方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">List&lt;Row&gt; rows = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">rows.add(RowFactory.create(<span class="string">"张三"</span>, <span class="number">20</span>, <span class="string">"北京"</span>));</span><br><span class="line">rows.add(RowFactory.create(<span class="string">"李四"</span>, <span class="number">22</span>, <span class="string">"上海"</span>));</span><br><span class="line">StructField[] fields = <span class="keyword">new</span> StructField[]&#123;</span><br><span class="line">        DataTypes.createStructField(<span class="string">"name"</span>, DataTypes.StringType, <span class="keyword">false</span>),</span><br><span class="line">        DataTypes.createStructField(<span class="string">"age"</span>, DataTypes.IntegerType, <span class="keyword">false</span>),</span><br><span class="line">        DataTypes.createStructField(<span class="string">"address"</span>, DataTypes.StringType, <span class="keyword">false</span>)</span><br><span class="line">&#125;;</span><br><span class="line">StructType schema = DataTypes.createStructType(fields);</span><br><span class="line">Dataset&lt;Row&gt; listSchemaDF = spark.createDataFrame(rows, schema);</span><br><span class="line">listSchemaDF.show();</span><br><span class="line">listSchemaDF.printSchema();</span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrame 转 Dataset</span></span><br><span class="line">Encoder&lt;Person&gt; bean = Encoders.bean(Person.class);</span><br><span class="line">Dataset&lt;Person&gt; ds = listSchemaDF.as(bean);</span><br><span class="line"></span><br><span class="line">listSchemaDF.foreach(row -&gt; &#123;</span><br><span class="line">    <span class="comment">// untyped</span></span><br><span class="line">    System.out.println(row.getAs(<span class="string">"name"</span>).toString());</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">System.out.println(<span class="string">"---------------------------"</span>);</span><br><span class="line"></span><br><span class="line">ds.foreach(person -&gt; &#123;</span><br><span class="line">    <span class="comment">// typed</span></span><br><span class="line">    System.out.println(person.getName());</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<h1 id="5-DataFrame-基本操作"><a href="#5-DataFrame-基本操作" class="headerlink" title="5. DataFrame 基本操作"></a>5. DataFrame 基本操作</h1><p>从定义上看，一个 DataFrame 包括一系列的 records（记录，就像 table 中的 rows），这些行的类型是 Row 类型，包括一系列的 columns（就像表格中的列）。Schema 定义了每一列的列名和数据类型。DataFrame 的分区定义了 DataFrame 或 Dataset 在整个集群中的物理分布情况。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建 DataFrame</span></span><br><span class="line">Dataset&lt;Row&gt; df = spark.read().json(<span class="string">"E:\\Java\\IntelliJ IDEA\\spark\\in\\2015-summary.json"</span>);</span><br><span class="line"><span class="comment">// 查看 Schema</span></span><br><span class="line">df.printSchema();</span><br><span class="line"><span class="comment">// 查看 schema</span></span><br><span class="line">System.out.println(df.schema());</span><br></pre></td></tr></table></figure>

<p>一个 Schema 就是一个 StructType，由多个 StructField 类型的 fields 组成，每个 field 包括一个列名称、一个列类型、一个布尔型的标识（是否可以有缺失值和 Null 值）。</p>
<h1 id="6-Columns-操作"><a href="#6-Columns-操作" class="headerlink" title="6. Columns 操作"></a>6. Columns 操作</h1><p>Spark 中的列类似于表格中的列。可以从 DataFrame 中选择列、操作列和删除列。对 Spark 来说，列是逻辑结构，它仅仅表示通过一个表达式按每条记录计算出的一个值。这意味着，要得到一个 column 列的真实值，我们需要一行 row 数据，为了得到一行数据，我们需要有一个 DataFrame。<strong>不能在 DataFrame 的上下文之外操作单个列</strong>。必须在 DataFrame 内使用 Spark 转换来操作列。</p>
<p>有许多不同的方法来构造和引用列，但最简单的两种方法是使用 <strong>col()</strong> 或 <strong>column()</strong>函数。要使用这些函数中的任何一个，需要传入一个列名：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;col, column&#125;</span><br><span class="line">col(<span class="string">"someColumnName"</span>)</span><br><span class="line">column(<span class="string">"someColumnName"</span>)</span><br></pre></td></tr></table></figure>

<p>如前所述，这个列可能存在于我们的 DataFrames 中，也可能不存在。在将列名称与我们 Catalog 中维护的列进行比较之前，列不会被解析，即列是 <strong>unresolved</strong>。</p>
<p><strong>注意</strong>：<br>我们刚才提到的两种不同的方法引用列。Scala 有一些独特的语言特性，允许使用更多的简写方式来引用列。以下的语法糖执行完全相同的事情，即创建一个列，但不提供性能改进：<br><strong>$”myColumn”</strong><br><strong>‘myColumn</strong><br>$允许我们将一个字符串指定为一个特殊的字符串，该字符串应该引用一个表达式。标记(‘)是一种特殊的东西，称为符号；这是一个特定于 Scala 语言的，指向某个标识符。它们都执行相同的操作，是按名称引用列的简写方法。当您阅读不同对的人的 Spark 代码时，可能会看到前面提到的所有引用。</p>
<p><strong>表达式 expression</strong><br><strong>列是表达式</strong>。表达式是什么？表达式是在 DataFrame 中数据记录的一个或多个值上的一组转换。把它想象成一个函数，它将一个或多个列名作为输入，表达式会解析它们，为数据集中的每个记录返回一个单一值。</p>
<p>在最简单的情况下，expr(“someCol”)等价于 col(“someCol”)。<br><strong>列操作是表达式功能的一个子集</strong>。<br>expr(“someCol - 5”) 与执行 col(“someCol”) - 5，或甚至 expr(“someCol”) - 5 的转换相同。这是因为 Spark 将它们编译为一个逻辑树，逻辑树指定了操作的顺序。</p>
<p><strong>一些 DataFrames 操作列的示例</strong>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建 DataFrame</span></span><br><span class="line">Dataset&lt;Row&gt; df = spark.read().json(<span class="string">"E:\\Java\\IntelliJ IDEA\\spark\\in\\2015-summary.json"</span>);</span><br><span class="line"><span class="comment">// Dataset API</span></span><br><span class="line">df.select(<span class="string">"DEST_COUNTRY_NAME"</span>).show(<span class="number">5</span>);</span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"flight"</span>);</span><br><span class="line">spark.sql(<span class="string">"select DEST_COUNTRY_NAME from flight limit 5"</span>).show();</span><br></pre></td></tr></table></figure>

<p><strong>可以使用相同的查询样式选择多个列，只需在 select 方法调用中添加更多的列名字符串参数</strong>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Dataset API</span></span><br><span class="line">df.select(<span class="string">"DEST_COUNTRY_NAME"</span>,<span class="string">"ORIGIN_COUNTRY_NAME"</span>).show(<span class="number">5</span>);</span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"flight"</span>);</span><br><span class="line">spark.sql(<span class="string">"select DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME from flight limit 5"</span>).show();</span><br></pre></td></tr></table></figure>

<p><strong>可以用许多不同的方式引用列，可以交替使用它们</strong>：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;expr, col, column&#125;</span><br><span class="line">df.select(</span><br><span class="line">    df.col(<span class="string">"DEST_COUNTRY_NAME"</span>),</span><br><span class="line">    col(<span class="string">"DEST_COUNTRY_NAME"</span>),</span><br><span class="line">    column(<span class="string">"DEST_COUNTRY_NAME"</span>),</span><br><span class="line">    <span class="symbol">'DEST_COUNTRY_NAME</span>,</span><br><span class="line">    $<span class="string">"DEST_COUNTRY_NAME"</span>,</span><br><span class="line">    expr(<span class="string">"DEST_COUNTRY_NAME"</span>))</span><br><span class="line">  .show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p><strong>一个常见的错误是混合使用列对象和列字符串。例如，下列代码将导致编译错误</strong>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(col(<span class="string">"DEST_COUNTRY_NAME"</span>), <span class="string">"EST_COUNTRY_NAME"</span>)</span><br></pre></td></tr></table></figure>

<p><strong>expr 是我们可以使用的最灵活的引用。它可以引用一个简单的列或一个列字符串操作。<br>为了说明这一点，让我们更改列名，然后通过 AS 关键字来更改它</strong>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Dataset API</span></span><br><span class="line">df.select(functions.expr(<span class="string">"DEST_COUNTRY_NAME as destination"</span>)).show(<span class="number">2</span>);</span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"flight"</span>);</span><br><span class="line">spark.sql(<span class="string">"select DEST_COUNTRY_NAME as dest from flight limit 2"</span>).show();</span><br></pre></td></tr></table></figure>

<h1 id="7-字面常量转换为-Spark-类型（Literals）"><a href="#7-字面常量转换为-Spark-类型（Literals）" class="headerlink" title="7. 字面常量转换为 Spark 类型（Literals）"></a>7. 字面常量转换为 Spark 类型（Literals）</h1><p>有时，我们需要将显式字面常量值传递给 Spark，它只是一个值（而不是一个新列）。这可能是一个常数值或者我们以后需要比较的值。我们的方法是通过 Literals，将给定编程语言的字面值转换为 Spark 能够理解的值：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Dataset API</span></span><br><span class="line">df.select(functions.expr(<span class="string">"*"</span>), functions.lit(<span class="number">1</span>).as(<span class="string">"One"</span>)).show();</span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"flight"</span>);</span><br><span class="line">spark.sql(<span class="string">"select *, 1 as One from flight"</span>).show();</span><br></pre></td></tr></table></figure>

<h1 id="8-添加列"><a href="#8-添加列" class="headerlink" title="8. 添加列"></a>8. 添加列</h1><p>将新列添加到 DataFrame 中，这是通过在 DataFrame 上使用 withColumn 方法来实现的，例如，让我们添加一个列，将数字 1 添加为一个列，列名为 numberOne：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  Dataset API</span></span><br><span class="line">df.withColumn(<span class="string">"numberOne"</span>, functions.lit(<span class="number">1</span>)).show();</span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"flight"</span>);</span><br><span class="line">spark.sql(<span class="string">"select *, 1 as One from flight"</span>).show();</span><br></pre></td></tr></table></figure>

<p>另一个例子：设置一个布尔标志，表示源国与目标国相同：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.withColumn(<span class="string">"ifSame"</span>, functions.expr(<span class="string">"DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME"</span>)).filter(<span class="string">"ifSame = true"</span>).show();</span><br></pre></td></tr></table></figure>

<p>注意：withColumn 函数由两个参数：<strong>列名</strong> 和 <strong>为 DataFrame 中的给定行创建值的表达式</strong>。</p>
<p>我们也可以这样 重命名列：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 复制某列但重新定义列名</span></span><br><span class="line">df.withColumn(<span class="string">"destination"</span>, functions.expr(<span class="string">"DEST_COUNTRY_NAME"</span>)).show();</span><br></pre></td></tr></table></figure>

<h1 id="9-重命名列"><a href="#9-重命名列" class="headerlink" title="9. 重命名列"></a>9. 重命名列</h1><p>虽然我们可以按照刚才描述的方式重命名列，但是另一种方法是使用 withcolumnrename 方法。这会将第一个参数中的字符串的名称重命名为第二个参数中的字符串：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 重命名列</span></span><br><span class="line">df.withColumnRenamed(<span class="string">"DEST_COUNTRY_NAME"</span>,<span class="string">"dest"</span>).show();</span><br></pre></td></tr></table></figure>

<h1 id="10-删除列"><a href="#10-删除列" class="headerlink" title="10. 删除列"></a>10. 删除列</h1><p>可能已经注意到我们可以通过使用 select 来实现这一点。然而，还有一个专门的方法叫做 drop：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 删除列</span></span><br><span class="line">df.drop(<span class="string">"DEST_COUNTRY_NAME"</span>).show();</span><br></pre></td></tr></table></figure>

<h1 id="11-更改列类型"><a href="#11-更改列类型" class="headerlink" title="11. 更改列类型"></a>11. 更改列类型</h1><p>有时，可能性需要列从一种类型转换为另一种类型，例如，如果有一组 StringType 应该是整数。我们可以将列从一种类型转换为另一种类型，例如，让我们将 count 列从整数转换为类型 Long：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 更改列类型</span></span><br><span class="line">df.withColumn(<span class="string">"count"</span>, functions.col(<span class="string">"count"</span>).cast(<span class="string">"int"</span>)).printSchema();</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"flight"</span>);</span><br><span class="line">spark.sql(<span class="string">"select *, cast(count as int) as count2 from flight"</span>).printSchema();</span><br></pre></td></tr></table></figure>

<h1 id="12-过滤行"><a href="#12-过滤行" class="headerlink" title="12. 过滤行"></a>12. 过滤行</h1><p>为了过滤行，我们创建一个计算值为 true 或 false 的表达式。然后用一个等于 false 的表达式过滤掉这些行。使用 DataFrames 执行此操作的最常见方法是将表达式创建为字符串，或者使用一组列操作构建表达式。执行此操作有两种方法：您可以使用 where 或 filter，它们都将执行相同的操作，并在使用 DataFrames 时接受相同的参数类型。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// API</span></span><br><span class="line">df.filter(functions.col(<span class="string">"count"</span>).$less(<span class="number">2</span>)).show(<span class="number">2</span>);</span><br><span class="line">df.where(<span class="string">"count &lt; 2"</span>).show(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"flight"</span>);</span><br><span class="line">spark.sql(<span class="string">"select * from flight where count &lt; 2"</span>).show(<span class="number">2</span>);</span><br></pre></td></tr></table></figure>

<p>你可能希望将多个过滤器放入相同的表达式中。尽管这是可能的，但它并不总是有用的，因为 Spark 会自动执行所有的过滤操作，而不考虑过滤器的排序。这意味着，如果你想指定多个过滤器，只需将它们按顺序连接起来，让 Spark 处理其余部分：</p>
<p>多重过滤：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// API</span></span><br><span class="line">df.where(<span class="string">"count &lt; 2"</span>).where(<span class="string">"DEST_COUNTRY_NAME != 'United States'"</span>).show();</span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"flight"</span>);</span><br><span class="line">spark.sql(<span class="string">"select * from flight where count &lt; 2 and DEST_COUNTRY_NAME != 'United States'"</span>).show();</span><br></pre></td></tr></table></figure>

<h1 id="13-行去重"><a href="#13-行去重" class="headerlink" title="13. 行去重"></a>13. 行去重</h1><p>一个常见的用例是在一个 DataFrame 中提取唯一的或不同的值。这些值可以在一个或多个列中。我们这样做的方法是在 DataFrame 上使用不同的方法，它允许我们对该 DataFrame 中的任何行进行删除重复行。例如，让我们在数据集中获取唯一的起源地。当然，这是一个转换，它将返回一个新的 DataFrame，只有唯一的行：</p>
<p>去除重复航线：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// API</span></span><br><span class="line">System.out.println(df.select(<span class="string">"DEST_COUNTRY_NAME"</span>, <span class="string">"ORIGIN_COUNTRY_NAME"</span>).distinct().count());</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"flight"</span>);</span><br><span class="line">spark.sql(<span class="string">"select count(distinct(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME)) as count from flight"</span>).show();</span><br></pre></td></tr></table></figure>

<h1 id="14-DataFrame-union"><a href="#14-DataFrame-union" class="headerlink" title="14. DataFrame union"></a>14. DataFrame union</h1><p>DataFrame 是<strong>不可变</strong>的。这意味着用户不能向 DataFrame 追加，因为这会改变它。要附加到 DataFrame，必须将原始的 DataFrame 与新的 DataFrame 结合起来 。这只是连接了两个 DataFrames。对于 Union 2 DataFrames，<strong>必须确保他们具有相同的模式和列数（Schema）</strong>，否则，union 将会失败。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Person&gt; p1 = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">p1.add(<span class="keyword">new</span> Person(<span class="string">"张三"</span>, <span class="number">20</span>, <span class="string">"北京"</span>));</span><br><span class="line">p1.add(<span class="keyword">new</span> Person(<span class="string">"李四"</span>, <span class="number">22</span>, <span class="string">"上海"</span>));</span><br><span class="line">Dataset&lt;Row&gt; df1 = spark.createDataFrame(p1, Person.class);</span><br><span class="line"></span><br><span class="line">List&lt;Person&gt; p2 = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">p2.add(<span class="keyword">new</span> Person(<span class="string">"wangwu"</span>, <span class="number">20</span>, <span class="string">"北京"</span>));</span><br><span class="line">p2.add(<span class="keyword">new</span> Person(<span class="string">"qianliu"</span>, <span class="number">22</span>, <span class="string">"上海"</span>));</span><br><span class="line">Dataset&lt;Row&gt; df2 = spark.createDataFrame(p2, Person.class);</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; union = df1.union(df2);</span><br><span class="line">union.show();</span><br></pre></td></tr></table></figure>

<h1 id="15-行排序"><a href="#15-行排序" class="headerlink" title="15. 行排序"></a>15. 行排序</h1><p>在对 DataFrame 中的值进行排序时，我们总是希望对 DataFrame 顶部的最大或最小值进行排序。有两个相同的操作可以实现：sort 和 orderBy。它们接受列表达式，字符串以及多个列。默认 是按升序排序：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df.sort(<span class="string">"count"</span>).show(<span class="number">5</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 二次排序</span></span><br><span class="line">df.orderBy(<span class="string">"count"</span>, <span class="string">"DEST_COUNTRY_NAME"</span>).show(<span class="number">5</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果要指定升序、降序，需要用asc 和 desc 函数</span></span><br><span class="line">df.orderBy(functions.desc(<span class="string">"ORIGIN_COUNTRY_NAME"</span>),functions.desc(<span class="string">"count"</span>)).show();</span><br></pre></td></tr></table></figure>

<p>出于优化的目的，有时建议在另一组转换之前对每个分区进行排序。可以使用 sortWithinPartitons 方法来执行以下操作：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.sortWithinPartitions(<span class="string">"count"</span>).show();</span><br></pre></td></tr></table></figure>

<h1 id="16-Limit"><a href="#16-Limit" class="headerlink" title="16. Limit"></a>16. Limit</h1><p>通常，你可能想要限制从 DataFrame 中提取的内容；例如，您可能只想要一些 DataFrame 的前十位。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// API</span></span><br><span class="line">df.sortWithinPartitions(<span class="string">"count"</span>).limit(<span class="number">10</span>).show();</span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">df.sortWithinPartitions(<span class="string">"count"</span>).createOrReplaceTempView(<span class="string">"flight"</span>);</span><br><span class="line">spark.sql(<span class="string">"select * from flight limit 10"</span>).show();</span><br></pre></td></tr></table></figure>

<h1 id="17-Repartition-和-Coalesce"><a href="#17-Repartition-和-Coalesce" class="headerlink" title="17. Repartition 和 Coalesce"></a>17. Repartition 和 Coalesce</h1><p>一个重要的优化方式是根据一些经常过滤的列对数据进行分区，它控制跨集群的数据的物理布局，包括分区计划和分区数量。</p>
<p><strong>Repartition</strong> 将导致数据的完全 shuffle，无论是否需要重新 shuffle。这意味着<strong>只有当将来的分区数目大于当前的分区数目时，或者当你希望通过一组列进行分区时</strong>，你才应该使用 <strong>Repartition</strong>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取当前 分区数量</span></span><br><span class="line">System.out.println(df.rdd().getNumPartitions());</span><br><span class="line"><span class="comment">// 重分区</span></span><br><span class="line">Dataset&lt;Row&gt; repartition = df.repartition(<span class="number">2</span>);</span><br><span class="line">System.out.println(repartition.rdd().getNumPartitions());</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果需要经常对某个列进行过滤，那么基于该列进行重新分区是值得的</span></span><br><span class="line">Dataset&lt;Row&gt; dest_country_name = df.repartition(<span class="number">5</span>, functions.col(<span class="string">"DEST_COUNTRY_NAME"</span>));</span><br></pre></td></tr></table></figure>

<p>另一方面，<strong>Coalesce</strong> 不会导致完全 shuffle，并尝试合并分区。<br>下面操作将根据目标国家的名称将你的数据转移到 5 个分区中，然后合并它们（没有完全shuffle）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 合并成 2 个分区</span></span><br><span class="line">Dataset&lt;Row&gt; coalesce = dest_country_name.coalesce(<span class="number">2</span>);</span><br></pre></td></tr></table></figure>

<h1 id="18-收集数据到-driver"><a href="#18-收集数据到-driver" class="headerlink" title="18. 收集数据到 driver"></a>18. 收集数据到 driver</h1><ul>
<li>collect： 从整个 DataFrame 中获取所有数据</li>
<li>take: 选取 DataFrame 的前几行</li>
<li>show: 打印出几行数据</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.take(<span class="number">2</span>);</span><br><span class="line">df.collect();</span><br><span class="line"><span class="comment">// Whether truncate long strings. If true, strings more than 20 characters will</span></span><br><span class="line"><span class="comment">// be truncated and all cells will be aligned right</span></span><br><span class="line">df.show(<span class="number">5</span>, <span class="keyword">false</span>);</span><br></pre></td></tr></table></figure>

<p>更多相关操作参考官网：<br><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset</a></p>
<h1 id="19-作业"><a href="#19-作业" class="headerlink" title="19. 作业"></a>19. 作业</h1><ol>
<li>读取zips.json文件为DataFrame，并将列名_id重命名为zip</li>
<li>创建名为zips的case class或者javaBean，用于将第一步创建的DF转换为DS</li>
<li>显示DS中的数据<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取 json 文件并修改列名</span></span><br><span class="line">Dataset&lt;Row&gt; df = spark.read().json(<span class="string">"E:\\Java\\IntelliJ IDEA\\spark\\in\\zips.json"</span>);</span><br><span class="line">Dataset&lt;Row&gt; rowDataset = df.withColumnRenamed(<span class="string">"_id"</span>, <span class="string">"zip"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义 Java bean 类，自定义 encoder 将 df 转为 ds</span></span><br><span class="line">Encoder&lt;Zips&gt; zipsBeanEncoder = Encoders.bean(Zips.class);</span><br><span class="line">Dataset&lt;Zips&gt; ds = rowDataset.as(zipsBeanEncoder);</span><br><span class="line">ds.show();</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>数据分析：</p>
<ol>
<li>以降序显示人口超过40000的state, zip, city, pop</li>
<li>显示名为CA的states中人口最多的三个城市</li>
<li>把所有州states的人口加起来，按降序排列，显示前10名</li>
</ol>
<p>分别使用Dataset API和sql实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">ds.createOrReplaceTempView(<span class="string">"zips"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. 以降序显示人口超过40000的state, zip, city, pop</span></span><br><span class="line"><span class="comment">// API</span></span><br><span class="line">ds.where(<span class="string">"pop &gt; 40000"</span>).select(<span class="string">"state"</span>, <span class="string">"zip"</span>, <span class="string">"city"</span>, <span class="string">"pop"</span>).orderBy(functions.desc(<span class="string">"pop"</span>)).show();</span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">spark.sql(<span class="string">"select state, zip, city, pop from zips where pop &gt;40000 order by pop desc"</span>).show();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 显示名为CA的states中人口最多的三个城市</span></span><br><span class="line"><span class="comment">// API</span></span><br><span class="line">ds.where(<span class="string">"state = 'CA'"</span>).orderBy(functions.desc(<span class="string">"pop"</span>)).select(<span class="string">"city"</span>).show(<span class="number">3</span>);</span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">spark.sql(<span class="string">"select city from zips where state = 'CA' order by pop desc limit 3"</span>).show();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 把所有州states的人口加起来，按降序排列，显示前10名</span></span><br><span class="line"><span class="comment">// API</span></span><br><span class="line">ds.groupBy(<span class="string">"state"</span>).sum(<span class="string">"pop"</span>).withColumnRenamed(<span class="string">"sum(pop)"</span>, <span class="string">"total"</span>).orderBy(functions.desc(<span class="string">"total"</span>)).limit(<span class="number">10</span>).show();</span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">spark.sql(<span class="string">"select state, sum(pop) as total from zips group by state order by total desc limit 10"</span>).show();</span><br></pre></td></tr></table></figure>
    
  </div>

</article>


   
  <div class="text-center donation">
    <div class="inner-donation">
      <span class="btn-donation">支持一下</span>
      <div class="donation-body">
        <div class="tip text-center">扫一扫，支持forsigner</div>
        <ul>
        
          <li class="item">
            
              <span>微信扫一扫</span>
            
            <img src="/images/qr-wechat.png" alt="">
          </li>
        
          <li class="item">
            
              <span>支付宝扫一扫</span>
            
            <img src="/images/qr-alipay.png" alt="">
          </li>
        
        </ul>
      </div>
    </div>
  </div>


   
  <div class="box-prev-next clearfix">
    <a class="show pull-left" href="/2019/08/27/SparkSQL-入门/">
        <i class="icon icon-angle-left"></i>
    </a>
    <a class="hide pull-right" href="/">
        <i class="icon icon-angle-right"></i>
    </a>
  </div>




</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">关闭</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              rel="noopener noreferrer"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/category/"
              rel="noopener noreferrer"
              target="_self"
              >
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              rel="noopener noreferrer"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              rel="noopener noreferrer"
              target="_self"
              >
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/atom.xml"
              rel="noopener noreferrer"
              target="_blank"
              >
              RSS
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    

    

    
    

    

    
    

    

<!-- Gitalk评论插件通用代码 -->
<div id="gitalk-container"></div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script>
const gitalk = new Gitalk({
  clientID: 'cffabda338955fb33e72',
  clientSecret: '27685d32607acc9c76041016860f5434fa1d65d0',
  repo: 'gitalk_comment',
  owner: 'Miracle-Xing',
  // 在这里设置一下截取前50个字符串, 这是因为 github 对 label 的长度有了要求, 如果超过
  // 50个字符串则会报错.
  // id: location.pathname.split('/').pop().substring(0, 49),
  id: location.pathname,
  admin: ['Miracle-Xing'],
  // facebook-like distraction free mode
  distractionFreeMode: false
})
gitalk.render('gitalk-container')
</script>
<!-- Gitalk代码结束 -->



  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

</body>
</html>

<!DOCTYPE html>


  <html class="light page-post">


<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>Spark 编程核心抽象——RDD | 邢大强的blog</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="Spark,SparkCore,">
  

  <meta name="description" content="1. 什么是RDD？RDD 是 Resilient Distributed Dataset（弹性分布式数据集） 的简称。它是 Apache Spark 的基本数据结构。它是一个不可变的对象集合，在集群的不同节点上进行计算。  Resilient: 即在 RDD lineage(DAG) 的帮助下具有容错能力，能够重新计算由于节点故障而丢失或损坏的数据分区。 Distributed: 数据分布在多个">
<meta name="keywords" content="Spark,SparkCore">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark 编程核心抽象——RDD">
<meta property="og:url" content="https://miracle-xing.github.io/2019/07/24/Spark-编程核心抽象——RDD/index.html">
<meta property="og:site_name" content="邢大强的blog">
<meta property="og:description" content="1. 什么是RDD？RDD 是 Resilient Distributed Dataset（弹性分布式数据集） 的简称。它是 Apache Spark 的基本数据结构。它是一个不可变的对象集合，在集群的不同节点上进行计算。  Resilient: 即在 RDD lineage(DAG) 的帮助下具有容错能力，能够重新计算由于节点故障而丢失或损坏的数据分区。 Distributed: 数据分布在多个">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/07/24/75185340-ad78-11e9-bb89-83337f9b9a34.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/07/24/7bda84f0-ad78-11e9-bb89-83337f9b9a34.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/07/24/938a83c0-ad78-11e9-bb89-83337f9b9a34.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/07/24/323ce350-ad79-11e9-bb89-83337f9b9a34.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/07/24/36b43aa0-ad79-11e9-bb89-83337f9b9a34.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/07/24/3b44e650-ad79-11e9-bb89-83337f9b9a34.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/07/24/a6005070-ad78-11e9-bb89-83337f9b9a34.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/07/24/ab566f00-ad78-11e9-bb89-83337f9b9a34.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/07/24/b9d781e0-ad78-11e9-bb89-83337f9b9a34.png">
<meta property="og:updated_time" content="2019-08-30T10:39:22.059Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark 编程核心抽象——RDD">
<meta name="twitter:description" content="1. 什么是RDD？RDD 是 Resilient Distributed Dataset（弹性分布式数据集） 的简称。它是 Apache Spark 的基本数据结构。它是一个不可变的对象集合，在集群的不同节点上进行计算。  Resilient: 即在 RDD lineage(DAG) 的帮助下具有容错能力，能够重新计算由于节点故障而丢失或损坏的数据分区。 Distributed: 数据分布在多个">
<meta name="twitter:image" content="https://miracle-xing.github.io/images/2019/07/24/75185340-ad78-11e9-bb89-83337f9b9a34.png">

  

  
    <link rel="icon" href="/assets/img/m.png">
  

  <link href="/css/styles.css?v=c114cbeddx" rel="stylesheet">


  
    <link rel="stylesheet" href="/css/personal-style.css">
  

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-38189205-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?57e94d016e201fba3603a8a2b0263af0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  
  <script type="text/javascript">
	(function(){
	    var bp = document.createElement('script');
	    var curProtocol = window.location.protocol.split(':')[0];
	    if (curProtocol === 'https') {
	        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
	    }
	    else {
	        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
	    }
	    var s = document.getElementsByTagName("script")[0];
	    s.parentNode.insertBefore(bp, s);
	})();
  </script>



  
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">
  

</head>
</html>
<body>


  
    <span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/archives/"
            rel="noopener noreferrer"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/category/"
            rel="noopener noreferrer"
            target="_self"
            >
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tag/"
            rel="noopener noreferrer"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/about/"
            rel="noopener noreferrer"
            target="_self"
            >
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/atom.xml"
            rel="noopener noreferrer"
            target="_blank"
            >
            RSS
          </a>
        </li>
      
    </ul>
  </div>


</div>


  <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-什么是RDD？"><span class="toc-text">1. 什么是RDD？</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-RDD-的特点"><span class="toc-text">2. RDD 的特点</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Spark-RDD-的操作类型"><span class="toc-text">3. Spark RDD 的操作类型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-Transformation-操作："><span class="toc-text">3.1 Transformation 操作：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-action-操作"><span class="toc-text">3.2 action 操作</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-创建-RDD"><span class="toc-text">4. 创建 RDD</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-使用集合创建-RDD"><span class="toc-text">4.1 使用集合创建 RDD</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-从外部数据源创建-RDD"><span class="toc-text">4.2 从外部数据源创建 RDD</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1-读取本地文件"><span class="toc-text">4.2.1 读取本地文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2-读取-HDFS-上的数据"><span class="toc-text">4.2.2 读取 HDFS 上的数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-3-提交应用程序到-Spark-集群"><span class="toc-text">4.2.3 提交应用程序到 Spark 集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-4-配置并启动-Spark-History-Server"><span class="toc-text">4.2.4 配置并启动 Spark History Server</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-向-Spark-算子传递函数"><span class="toc-text">5. 向 Spark 算子传递函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-Spark-算子实战——transformation"><span class="toc-text">6. Spark 算子实战——transformation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-map-和-flatMap-算子"><span class="toc-text">6.1 map 和 flatMap 算子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-filter-算子"><span class="toc-text">6.2 filter 算子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-distinct-算子"><span class="toc-text">6.3 distinct 算子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-mapPartitions"><span class="toc-text">6.4 mapPartitions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-5-mapPartitionWithIndex"><span class="toc-text">6.5 mapPartitionWithIndex()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-6-union-并集"><span class="toc-text">6.6 union 并集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-7-intersection（交集）"><span class="toc-text">6.7 intersection（交集）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-8-PairRDD"><span class="toc-text">6.8 PairRDD</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-8-1-PairRDD-上的-transformation-操作"><span class="toc-text">6.8.1 PairRDD 上的 transformation 操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-Spark-算子实战–action"><span class="toc-text">7. Spark 算子实战–action</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-count"><span class="toc-text">7.1 count</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-take"><span class="toc-text">7.2 take</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-top"><span class="toc-text">7.3 top</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-4-countByValue"><span class="toc-text">7.4 countByValue</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-5-reduce"><span class="toc-text">7.5 reduce</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-6-collect"><span class="toc-text">7.6 collect</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-7-foreach-（无返回值）"><span class="toc-text">7.7 foreach （无返回值）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-8-foreachParitition（无返回值）"><span class="toc-text">7.8 foreachParitition（无返回值）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-9-作业："><span class="toc-text">7.9 作业：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-Spark-RDD-分区实战"><span class="toc-text">8. Spark RDD 分区实战</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-RDD-partition-概念"><span class="toc-text">8.1 RDD partition 概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-RDD-partition-的相关属性"><span class="toc-text">8.2 RDD partition 的相关属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-3-查看-RDD-partition-信息"><span class="toc-text">8.3 查看 RDD partition 信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-4-RDD-的初始分区"><span class="toc-text">8.4 RDD 的初始分区</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-5-Transformation-操作对分区的影响"><span class="toc-text">8.5 Transformation 操作对分区的影响</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-1-普通-RDD-操作"><span class="toc-text">8.5.1 普通 RDD 操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-2-Key-value-RDD-操作"><span class="toc-text">8.5.2 Key-value RDD 操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-6-有多少分区是合适的（重点！！）"><span class="toc-text">8.6 有多少分区是合适的（重点！！）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-6-1-分区太少的缺点"><span class="toc-text">8.6.1 分区太少的缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-6-2-分区太多的缺点"><span class="toc-text">8.6.2 分区太多的缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-7-Spark-中的分区器"><span class="toc-text">8.7 Spark 中的分区器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-7-1-HashPartitioner"><span class="toc-text">8.7.1 HashPartitioner</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-7-2-RangePartitioner"><span class="toc-text">8.7.2 RangePartitioner</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-7-3-自定义分区器"><span class="toc-text">8.7.3 自定义分区器</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-Spark-RDD-数据保存实战"><span class="toc-text">9. Spark RDD 数据保存实战</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#9-1-保存数据到-HDFS"><span class="toc-text">9.1 保存数据到 HDFS</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-2-保存数据到-mysql-数据库"><span class="toc-text">9.2 保存数据到 mysql 数据库</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-1-读"><span class="toc-text">9.2.1 读</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-2-写"><span class="toc-text">9.2.2 写</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-3-保存数据到-kafka"><span class="toc-text">9.3 保存数据到 kafka</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-Spark-RDD-缓存实战"><span class="toc-text">10. Spark RDD 缓存实战</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#10-1-前言"><span class="toc-text">10.1 前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-2-要点"><span class="toc-text">10.2 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-3-RDD-持久化存储级别如何选择"><span class="toc-text">10.3 RDD 持久化存储级别如何选择</span></a></li></ol></li></ol>
  </div>



<div class="content content-post CENTER">
   <article id="post-Spark-编程核心抽象——RDD" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">Spark 编程核心抽象——RDD</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2019.07.24</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>Miracle</span>
        </span>
      

      
  <span class="article-category">
    <i class="icon-list"></i>
    <a class="article-category-link" href="/categories/笔记/">笔记</a>
  </span>



      

      
      <i class="fa fa-eye"></i> 
        <span id="busuanzi_container_page_pv">
           &nbsp热度 <span id="busuanzi_value_page_pv">
           <i class="fa fa-spinner fa-spin"></i></span>℃
        </span>
      
      
    </div>
  </header>

  <div class="article-content">
    
      <h1 id="1-什么是RDD？"><a href="#1-什么是RDD？" class="headerlink" title="1. 什么是RDD？"></a>1. 什么是RDD？</h1><p>RDD 是 Resilient Distributed Dataset（<strong>弹性分布式数据集</strong>） 的简称。它是 Apache Spark 的基本数据结构。它是一个不可变的对象集合，在集群的不同节点上进行计算。</p>
<ul>
<li>Resilient: 即在 RDD lineage(DAG) 的帮助下具有容错能力，能够重新计算由于节点故障而丢失或损坏的数据分区。</li>
<li>Distributed: 数据分布在多个节点上。</li>
<li>Dataset: 表示所操作的数据集。用户可以通过 JDBC 从外部加载数据集，数据集可以是 JSON 文件，CSV 文件，文本文件或数据库。<br>
<br>
<a id="more"></a>

</li>
</ul>
<h1 id="2-RDD-的特点"><a href="#2-RDD-的特点" class="headerlink" title="2. RDD 的特点"></a>2. RDD 的特点</h1><ol>
<li><p><strong>内存计算</strong>：它将中间计算结果存储在分布式内存（RAM）中，而不是磁盘中。</p>
</li>
<li><p><strong>延迟计算</strong>：Apache Spark 中的所有 transformation 都是惰性的，因为它们不会立即计算结果，它们会记住应用于数据集的那些 transformation。直到 action 出现时，才会真正开始计算。</p>
</li>
<li><p><strong>容错性</strong>：Spark RDDs 能够容错，因为它们跟踪数据<strong>沿袭</strong>（lineage）信息，以便在故障时自动重建丢失的数据。</p>
</li>
<li><p><strong>不可变性</strong>：跨进程共享数据是安全的。它也可以在任何时候创建或检索，这使得缓存、共享和复制变得容易。因此，它是一种在计算中达到一致性的方法。</p>
</li>
<li><p><strong>分区性</strong>：partition 是 Spark RDD 中并行性的基本单元，每个分区都是数据的逻辑分区。Partition—task 一 一对应。</p>
</li>
<li><p><strong>持久化</strong>：用户可以声明他们将重用哪些 RDDs，并为它们选择存储策略。</p>
</li>
<li><p><strong>数据本地性</strong>：RDDs 能够定义计算分区的位置首选项。位置首选项是关于 RDD 位置的信息。</p>
</li>
</ol>
<h1 id="3-Spark-RDD-的操作类型"><a href="#3-Spark-RDD-的操作类型" class="headerlink" title="3. Spark RDD 的操作类型"></a>3. Spark RDD 的操作类型</h1><p>Apache Spark 中的 RDD 支持两种操作：</p>
<ul>
<li><strong>Transformation</strong></li>
<li><strong>Action</strong></li>
</ul>
<h2 id="3-1-Transformation-操作："><a href="#3-1-Transformation-操作：" class="headerlink" title="3.1 Transformation 操作："></a>3.1 Transformation 操作：</h2><p>Spark RDD transformation 操作是一个从现有的 RDD 生成新 RDD 的函数（方法、算子）。如：map(), filter(), reduceByKey()。</p>
<p>Transformation 操作都是<strong>延迟计算</strong>的操作。</p>
<p>有两种类型：<strong>窄变换、宽变换</strong>（窄依赖、宽依赖）。</p>
<ol>
<li><strong>窄依赖</strong>：它是map、filter 这样数据来自一个单独分区的操作。即输出 RDD 分区中的数据，来自父 RDD 中的单个分区。不需要 shuffle 操作就能解决。<br><img src="/images/2019/07/24/75185340-ad78-11e9-bb89-83337f9b9a34.png" alt="窄依赖.png"></li>
</ol>
<p><strong>窄依赖算子</strong>：map(), flatMap(), mapPartition(), filter(), sample(), union()</p>
<ol start="2">
<li><strong>宽依赖</strong>：在子 RDD 单个分区中计算结果所需的数据可能存在于父 RDD 的多个分区中。类似 groupByKey() 和 reduceBykey() 这样的 transformation。宽依赖也称为 shuffle transformation。<br><img src="/images/2019/07/24/7bda84f0-ad78-11e9-bb89-83337f9b9a34.png" alt="宽依赖.png"></li>
</ol>
<p><strong>宽依赖算子</strong>：intersection(), distinct(), reduceByKey(), groupByKey(), join(), cartesian(), repartition(), coalesce()。</p>
<h2 id="3-2-action-操作"><a href="#3-2-action-操作" class="headerlink" title="3.2 action 操作"></a>3.2 action 操作</h2><p>Spark 中的 action 操作 ，返回 RDD 计算 的最终结果，<strong>其结果是一个值，而不是一个 RDD</strong>。</p>
<p>Action 触发血缘关系中 RDD 上的 transformation  操作的真正计算，计算结果返回 Driver端或者写入数据库。</p>
<p>这种设计使 Spark 运行更加高效。例如：map操作返回的数据集用于 reduce 操作，返回到 driver 端的只是 reduce 的结果值，而不是 map操作的数据集。</p>
<p><strong>常见的 Action</strong>：first(), take(), reduce(), collect(), the count()。</p>
<h1 id="4-创建-RDD"><a href="#4-创建-RDD" class="headerlink" title="4. 创建 RDD"></a>4. 创建 RDD</h1><p>三种创建 RDD 的方法：</p>
<ol>
<li>使用集合创建 RDD（<strong>parallelize</strong>）</li>
<li>使用已有 RDD 创建 RDD（<strong>父生子</strong>）</li>
<li>从外部数据源创建 RDD（<strong>textFile</strong>）</li>
</ol>
<p>在我们学习 Spark 的初始阶段 ，RDD 通常由集合创建的，即在 Driver 程序中创建集合并将其传递给 SparkContext 的 paralize() 方法。这种方法很少在 正式环境中使用，因为这种方法的整个数据集位于一台主机上。</p>
<p>首先实例化 SparkContext 对象：</p>
<p><strong>Scala</strong>：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"sc_wordcount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br></pre></td></tr></table></figure>

<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf().setAppName(appName).setMaster(master);</span><br><span class="line">JavaSparkContext jsc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br></pre></td></tr></table></figure>

<p>其中 appName 用于显示在 Spark 集群的 webUI上面。master 是一个 spark、YARN、mesos 集群 URL，或者是一个 local 字符串。<strong>实际项目中，在集群上运行时，不会对 master 进行硬编码。而是用 spark-submit 启动应用程序，并传递 master 给应用程序。但是，对于本地测试和单元测试，可以使用 local 运行 Spark</strong>。<br><img src="/images/2019/07/24/938a83c0-ad78-11e9-bb89-83337f9b9a34.png" alt="sparksubmit示例.png"></p>
<h2 id="4-1-使用集合创建-RDD"><a href="#4-1-使用集合创建-RDD" class="headerlink" title="4.1 使用集合创建 RDD"></a>4.1 使用集合创建 RDD</h2><p><strong>Scala</strong>:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> distData = sc.paralize(data)</span><br></pre></td></tr></table></figure>

<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; data = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; distData = sc.paralize(data);</span><br></pre></td></tr></table></figure>

<h2 id="4-2-从外部数据源创建-RDD"><a href="#4-2-从外部数据源创建-RDD" class="headerlink" title="4.2 从外部数据源创建 RDD"></a>4.2 从外部数据源创建 RDD</h2><p>Spark 可以从 Hadoop 支持的任何存储源创建分布式数据集，包括本地文件系统、HDFS、Cassandra、HBase、Amazon S3等。<br>Spark 支持文本文件、SequenceFiles 和任何其他 Hadoop InputFormat。</p>
<h3 id="4-2-1-读取本地文件"><a href="#4-2-1-读取本地文件" class="headerlink" title="4.2.1 读取本地文件"></a>4.2.1 读取本地文件</h3><p>文本文件 RDDs 可以使用 SparkContext 的 textFile 方法创建。此方法接受文件的 URI（机器上的本地文件路径、hdfs://、s3a://等URI），并将其作为行集合读取。下面是一个示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> distFile = sc.textFile(<span class="string">"data.txt"</span>)</span><br></pre></td></tr></table></figure>

<p>一旦创建，就可以对 distFile 进行相应操作。例如，我们 可以将所有行的长度相加，使用 map 和 reduce 操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distFile.map(line=&gt;line.length).reduce(_+_)</span><br></pre></td></tr></table></figure>

<p><strong>关于用 Spark 读取文件的一些注意事项</strong>：</p>
<ol>
<li><p>如果使用本地文件系统上的路径，则必须在 worker 节点上的同一路径上，此文件可访问。要么将文件复制到所有 worker 上，要么使用一个挂载网络的共享文件系统。</p>
</li>
<li><p>Spark 所有基于文件的输入方法（textFile 等），支持在目录、压缩文件和通配符上运行，例如：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">textFile(<span class="string">"/my/directory"</span>), textFile(<span class="string">"/my/directory/*.txt"</span>), textFile(<span class="string">"/my/directory/*.gz))</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>textFile 方法还接受一个可选的第二个参数，用于控制文件的分区数量。默认情况下，<strong>Spark 为文件的每个块创建一个分区（HDFS 中的块默认 是 128MB）</strong>，但是 您也可以通过传递更大的值来要求更高数量的分区。注意，分区数不能少于块数。</p>
</li>
</ol>
<p><strong>除了文本文件，Spark 的 Scala API 还支持其他几种数据格式</strong>：</p>
<ol>
<li><p><strong>SparkContext.wholeTextFile</strong> 允许您读取包含多个小文本文件的目录，并将它们作为（filename, content）的键值对返回。这与 textFile 不同，textFile 将在每个文件中每行返回一条 记录。分区由数据本地性决定，在某些情况下，数据本地性可能导致分区太少。对于这些情况，wholeTextFile 提供了控制最小分区数量的第二个可选 参数。</p>
</li>
<li><p>对于** SequenceFiles**，使用 SparkContext 的 sequenceFile[K, V]方法，其中K和V是文件中的键和值的类型。这些应该是 Hadoop Writable 接口的子类，比如 IntWritable 和 Text。</p>
</li>
<li><p>对于其他 Hadoop inputformat，您可以使用  SparkContext.hadoopRDD方法，它接受任意的 JobConf 和 输入格式类、键类和值类。将这些设置为与使用输入源 Hadoop 作业相同的方式。还可以使用 SparkContext。基于“new”MapReduce API（org.apache.hadoop.mapreduce）的 inputformat 的 newAPIHadoopRDD。</p>
</li>
</ol>
<h3 id="4-2-2-读取-HDFS-上的数据"><a href="#4-2-2-读取-HDFS-上的数据" class="headerlink" title="4.2.2 读取 HDFS 上的数据"></a>4.2.2 读取 HDFS 上的数据</h3><ol>
<li>启动 HDFS</li>
<li>读取 HDFS 上的数据<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> textFileRDD = sc.textFile(<span class="string">"hdfs://bigdata01:9000/textdata/order.txt"</span>)</span><br><span class="line"><span class="keyword">val</span> count = textFileRDD.count()</span><br><span class="line">println(<span class="string">"count:"</span> + count)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="4-2-3-提交应用程序到-Spark-集群"><a href="#4-2-3-提交应用程序到-Spark-集群" class="headerlink" title="4.2.3 提交应用程序到 Spark 集群"></a>4.2.3 提交应用程序到 Spark 集群</h3><ol>
<li><p>打包应用程序</p>
</li>
<li><p>上传 jar 包到服务器</p>
</li>
<li><p>运行 spark-submit 命令<br>./spark-submit –class sparkcore.learnTextFile –deploy-mode client /opt/sparkapp/learnTextFile.jar</p>
</li>
</ol>
<h3 id="4-2-4-配置并启动-Spark-History-Server"><a href="#4-2-4-配置并启动-Spark-History-Server" class="headerlink" title="4.2.4 配置并启动 Spark History Server"></a>4.2.4 配置并启动 Spark History Server</h3><ol>
<li><p>重命名 conf/spark-deaults.conf.template 为 conf/spark-defaults.conf</p>
</li>
<li><p>修改 spark-defaults.conf 配置文件，并同步到其他节点。<br>修改前：<br><img src="/images/2019/07/24/323ce350-ad79-11e9-bb89-83337f9b9a34.png" alt="spark historyserver 配置1.png"><br>修改后（注意：hdfs 目录要先创建）：<br><img src="/images/2019/07/24/36b43aa0-ad79-11e9-bb89-83337f9b9a34.png" alt="spark historyserver 配置2.png"></p>
</li>
<li><p>启动 ./start-history-server.sh<br><img src="/images/2019/07/24/3b44e650-ad79-11e9-bb89-83337f9b9a34.png" alt="spark historyserver 配置3.png"></p>
</li>
<li><p>访问 <a href="http://bigdata01:18080/" target="_blank" rel="noopener">http://bigdata01:18080/</a> webUI </p>
</li>
</ol>
<h1 id="5-向-Spark-算子传递函数"><a href="#5-向-Spark-算子传递函数" class="headerlink" title="5. 向 Spark 算子传递函数"></a>5. 向 Spark 算子传递函数</h1><p>Spark API 严重依赖于将 dirver 程序中的函数传递到集群上运行</p>
<p><strong>Scala</strong>:<br>推荐使用如下两种方式实现函数的传递：</p>
<ol>
<li>匿名函数语法，可用于短代码段。</li>
<li>全局单例对象中的静态方法。例如，您可以定义对象 MyFunctions，然后传递 MyFunctions.func1，如下所示：<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyFunctions</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func1</span></span>(s:<span class="type">String</span>): <span class="type">String</span> = &#123;...&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">myRDD.map(<span class="type">MyFunctions</span>.func1)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p><strong>Java</strong>:<br>在 Java 中，函数由实现 org.apacche.spark.api.java.function 包中的接口的类表示。<br><img src="/images/2019/07/24/a6005070-ad78-11e9-bb89-83337f9b9a34.png" alt="Java 函数编程1.png"></p>
<p><img src="/images/2019/07/24/ab566f00-ad78-11e9-bb89-83337f9b9a34.png" alt="Java 函数编程2.png"></p>
<p>有两种方法可以创建这样的函数：</p>
<ol>
<li>可以是匿名内部类</li>
<li>也可以是创建 类实现相应接口，并将其实例传递给 Spark</li>
</ol>
<p>使用<strong>lambda 表达式</strong>简洁的定义实现。<br>例如，可以这样编写代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(<span class="string">"data.txt"</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; lineLengths = lines.map(<span class="keyword">new</span> Function&lt;String, Integer&gt;()&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(String s)</span> </span>&#123;<span class="keyword">return</span> s.length();&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> totalLength = lineLengths.reduce(<span class="keyword">new</span> Function2&lt;Integer,  Integer, Integer&gt;()&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer a, Integer b)</span></span>&#123;<span class="keyword">return</span> a + b;&#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<p>或者 可以这样：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GetLength</span> <span class="keyword">implements</span> <span class="title">Function</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;</span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(String s)</span></span>&#123; <span class="keyword">return</span> s.length();&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sum</span> <span class="keyword">implements</span> <span class="title">Function2</span>&lt;<span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Integer</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer a, Integer b)</span> </span>&#123; <span class="keyword">return</span> a + b; &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(<span class="string">"data.txt"</span>);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; lineLengths = lines.map(<span class="keyword">new</span> GetLength());</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> totalLength = lineLengths.reduce(<span class="keyword">new</span> Sum());</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：Java 中的匿名内部类也可以访问闭包作用域中的变量，只要它们被标记为 final。 Spark 将把这些变量的副本发送到每个 worker 节点上。</p>
<h1 id="6-Spark-算子实战——transformation"><a href="#6-Spark-算子实战——transformation" class="headerlink" title="6. Spark 算子实战——transformation"></a>6. Spark 算子实战——transformation</h1><h2 id="6-1-map-和-flatMap-算子"><a href="#6-1-map-和-flatMap-算子" class="headerlink" title="6.1 map 和 flatMap 算子"></a>6.1 map 和 flatMap 算子</h2><p><strong>map()</strong>: 将传入的函数应用于RDD 中的每一条记录，返回由函数结果组成的新 RDD。函数的结果值是一个对象，不是一个集合。</p>
<p><strong>flatMap()</strong>: 与map() 操作类似。但是传入 flatMap() 的函数可以返回 0个、1个或多个结果值。即函数结果值是一个集合，而不是一个对象。</p>
<p><strong>map操作 Scala版本</strong>：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> textFileRDD = sc.textFile(<span class="string">"in/README.md"</span>)</span><br><span class="line"><span class="keyword">val</span> uppercaseRDD = textFileRDD.map(line=&gt;line.toUpperCase)</span><br><span class="line"><span class="keyword">for</span> ( elem &lt;- uppercaseRDDD.take(<span class="number">3</span>))&#123;</span><br><span class="line">    println(elem)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>map操作 Java版本</strong>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; javaRDD = jsc.textFile(<span class="string">"in/README.md"</span>);</span><br><span class="line">JavaRDD&lt;String&gt; map = javaRDD.map(line -&gt; line.toUpperCase());</span><br><span class="line"><span class="keyword">for</span> (String line : map.take(<span class="number">3</span>)) &#123;</span><br><span class="line">    System.out.println(line);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>flatMap操作 Scala版本</strong>：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> flatMapRDD = textFileRDD.flatMap(line =&gt; line.split(<span class="string">" "</span>))</span><br><span class="line">flatMapRDD.take(<span class="number">3</span>).foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>flatMap操作 Java版本</strong>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; javaRDD = jsc.textFile(<span class="string">"in/README.md"</span>);</span><br><span class="line">JavaRDD&lt;String&gt; wordsRDD = javaRDD.flatMap(line -&gt; (Arrays.asList(line.split(<span class="string">" "</span>)).iterator()));</span><br><span class="line">wordsRDD.take(<span class="number">3</span>).forEach(word-&gt; System.out.println(word));</span><br></pre></td></tr></table></figure>

<p><img src="/images/2019/07/24/b9d781e0-ad78-11e9-bb89-83337f9b9a34.png" alt="flatMap执行过程.png"><br>从Spark map() 和 flatMap() 的比较中可以看出，Spark map函数表达的是一对一的转换。它将集合的每个数据元素转换为结果集合的一个数据元素。而Spark flatMap 函数表示一对多的转换。它将每个元素转换为 0 或更多的元素。</p>
<h2 id="6-2-filter-算子"><a href="#6-2-filter-算子" class="headerlink" title="6.2 filter 算子"></a>6.2 filter 算子</h2><p>Spark RDD filter() 函数返回一个新的 RDD，<strong>只包含满足过滤条件的元素</strong>。这是一个<strong>窄依赖</strong>的操作，不会将数据从一个分区转移到其他分区。——不会发生shuffle。<br>例如，假设 RDD 包含5个整数（1, 2, 3, 4, 5），过滤条件是 判断是否偶数。过滤后得到的 RDD将只包含偶数，即 2 和 4。</p>
<p><strong>Scala</strong>:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> filterRDD = fileRDD.filter(line =&gt; line.contains(<span class="string">"Spark"</span>))</span><br><span class="line">filterRDD.foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; filterRDD = javaRDD.filter(line -&gt; line.contains(<span class="string">"Spark"</span>));</span><br><span class="line">filterRDD.foreach(line -&gt; System.out.println(line));</span><br></pre></td></tr></table></figure>

<h2 id="6-3-distinct-算子"><a href="#6-3-distinct-算子" class="headerlink" title="6.3 distinct 算子"></a>6.3 distinct 算子</h2><p>返回 RDD 中的非重复记录。注意：此操作是昂贵的，因为它需要对数据进行 shuffle。</p>
<p><strong>Scala</strong>:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> distintRDD = sc.parallelize(<span class="type">Seq</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)).distinct()</span><br><span class="line">distintRDD.foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; numsRDD = jsc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>));</span><br><span class="line">JavaRDD&lt;Integer&gt; distinctRDD = numsRDD.distinct();</span><br><span class="line">distinctRDD.foreach(ele -&gt; System.out.println(ele));</span><br></pre></td></tr></table></figure>

<h2 id="6-4-mapPartitions"><a href="#6-4-mapPartitions" class="headerlink" title="6.4 mapPartitions"></a>6.4 mapPartitions</h2><p>在 mapPartition() 函数中，map() 函数同时应用于每个 partition 分区。对比学习 foreachPartition() 函数，foreachPartition() 是一个action 算子，操作方式与 mapPartition相同。</p>
<p><strong>Scala</strong>:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> mapPartitionRDD = fileRDD.mapPartitions(partition =&gt; &#123;</span><br><span class="line">  <span class="comment">// map 每一个分区，然后再 map 分区中的每一个元素</span></span><br><span class="line">  partition.map(line =&gt; line.toUpperCase())</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// foreach 是一个没有返回值的 action</span></span><br><span class="line">mapPartitionRDD.foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; mapPartitionsRDD = javaRDD.mapPartitions(stringIterator -&gt; &#123;</span><br><span class="line">    List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    <span class="keyword">while</span> (stringIterator.hasNext()) &#123;</span><br><span class="line">        list.add(stringIterator.next().toUpperCase());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> list.iterator();</span><br><span class="line">&#125;);</span><br><span class="line">mapPartitionsRDD.foreach(line-&gt; System.out.println(line));</span><br></pre></td></tr></table></figure>

<h2 id="6-5-mapPartitionWithIndex"><a href="#6-5-mapPartitionWithIndex" class="headerlink" title="6.5 mapPartitionWithIndex()"></a>6.5 mapPartitionWithIndex()</h2><p>就像 mapPartition，除了 mapPartition外，它还为传入的函数提供了一个整数值，表示<strong>分区的索引</strong>，map()在分区索引上依次应用。</p>
<p><strong>Scala</strong>:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> mapPartitionRDD = fileRDD.mapPartitionsWithIndex((index, partition) =&gt; &#123;</span><br><span class="line">  partition.map(line =&gt; index + line.toUpperCase())</span><br><span class="line">&#125;)</span><br><span class="line">mapPartitionRDD.foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; mapPartitionsRDD = javaRDD.mapPartitionsWithIndex((index, stringIterator) -&gt; &#123;</span><br><span class="line">    List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    <span class="keyword">while</span> (stringIterator.hasNext()) &#123;</span><br><span class="line">        list.add(index + stringIterator.next().toUpperCase());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> list.iterator();</span><br><span class="line">&#125;, <span class="keyword">false</span>);</span><br><span class="line">mapPartitionsRDD.foreach(line -&gt; System.out.println(line));</span><br></pre></td></tr></table></figure>

<h2 id="6-6-union-并集"><a href="#6-6-union-并集" class="headerlink" title="6.6 union 并集"></a>6.6 union 并集</h2><p>使用 union() 函数，我们可以在新的 RDD 中获得两个 RDD 的元素。<strong>这个函数的关键规则是两个RDDs 属于同一类型</strong>。例如，RDD1 的元素是（Spark, Spark, Hadoop, Flink），而 RDD2 的元素是（Big data, Spark, Flink），所以结果 union(rdd1.union) 有元素（Spark, Spark, Spark, Hadoop, Flink, Flikn, Big data）</p>
<p><strong>Scala</strong>:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">RDD1</span> = sc.parallelize(<span class="type">Seq</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> <span class="type">RDD2</span> = sc.parallelize(<span class="type">Seq</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> unionRDD = <span class="type">RDD1</span>.union(<span class="type">RDD2</span>)</span><br><span class="line">unionRDD.foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; RDD1 = jsc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>));</span><br><span class="line">JavaRDD&lt;Integer&gt; RDD2 = jsc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>));</span><br><span class="line">JavaRDD&lt;Integer&gt; unionRDD = RDD1.union(RDD2);</span><br><span class="line">unionRDD.foreach(num -&gt; System.out.println(num));</span><br></pre></td></tr></table></figure>

<h2 id="6-7-intersection（交集）"><a href="#6-7-intersection（交集）" class="headerlink" title="6.7 intersection（交集）"></a>6.7 intersection（交集）</h2><p>使用 intersection() 函数，我们只得到新 RDD 中的两个 RDD 的公共元素。<strong>这个函数的关键规则是这两个 RDDs 应该是同一类型的</strong>。</p>
<p>举个例子，RDD1 的元素是（Spark, Spark, Hadoop,  Flink)，RDD2的元素是（Big data, Spark, Flink) 交集（RDD1.intersection(RDD2))将包含元素（Spark）。</p>
<p><strong>Scala</strong>:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">RDD1</span> = sc.parallelize(<span class="type">Seq</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> <span class="type">RDD2</span> = sc.parallelize(<span class="type">Seq</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> intersectionRDD = <span class="type">RDD1</span>.intersection(<span class="type">RDD2</span>)</span><br><span class="line">intersectionRDD.foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; RDD1 = jsc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>));</span><br><span class="line">JavaRDD&lt;Integer&gt; RDD2 = jsc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>));</span><br><span class="line">JavaRDD&lt;Integer&gt; intersectionRDD = RDD1.intersection(RDD2);</span><br><span class="line">intersectionRDD.foreach(num -&gt; System.out.println(num));</span><br></pre></td></tr></table></figure>

<h2 id="6-8-PairRDD"><a href="#6-8-PairRDD" class="headerlink" title="6.8 PairRDD"></a>6.8 PairRDD</h2><p>现实生活中的许多数据集通常是<strong>键值对</strong>形式的。例如：包含课程名称和选修课程的学生名单的数据集。<br>这种数据集的典型模式是每一行都是一个key映射到一个或多个value。为此，Spark提供了一个名为 PairRDD 的数据结构，而不是常规的 RDD。这使得处理此类数据更加简单和高效。</p>
<p>PairRDD 是一种特殊类型的 RDD，可以存储 键-值对</p>
<p><strong>创建 PairRDD</strong>:</p>
<ol>
<li>通过键值数据结构列表构建 Pair RDD。键值数据结构称为 tuple2 元组。（Java 语言没有内置的 tuple类型，所以Spark 的 Java API 允许用户使用 scala.Tuple2 类创建元组）。</li>
</ol>
<p><strong>Scala</strong>:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tuple = <span class="type">List</span>((<span class="string">"张三"</span>, <span class="string">"语文"</span>), (<span class="string">"李四"</span>, <span class="string">"数学"</span>), (<span class="string">"王五"</span>, <span class="string">"英语"</span>))</span><br><span class="line"><span class="keyword">val</span> pairRDD = sc.parallelize(tuple)pairRDD.foreach(t =&gt; println(t._1 + <span class="string">": "</span> + t._2))</span><br></pre></td></tr></table></figure>

<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Tuple2&lt;String, String&gt;&gt; tuple2s = Arrays.asList(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"张三"</span>, <span class="string">"语文"</span>),</span><br><span class="line">        <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"李四"</span>, <span class="string">"数学"</span>),</span><br><span class="line">        <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"王五"</span>, <span class="string">"英语"</span>));</span><br><span class="line">JavaPairRDD&lt;String, String&gt; pairRDD = jsc.parallelizePairs(tuple2s);</span><br><span class="line">pairRDD.foreach(t -&gt; System.out.println(t._1 + <span class="string">": "</span> + t._2));</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>将一个常规的 RDD 转换为 PairRDD</li>
</ol>
<p><strong>Scala</strong>：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> regularRDD = sc.parallelize(<span class="type">List</span>(<span class="string">"张三 语文"</span>, <span class="string">"李四 数学"</span>, <span class="string">"王五 英语"</span>))</span><br><span class="line"><span class="keyword">val</span> pairRDD = regularRDD.map(item =&gt; (item.split(<span class="string">" "</span>)(<span class="number">0</span>), item.split(<span class="string">" "</span>)(<span class="number">1</span>)))</span><br><span class="line">pairRDD.foreach(item =&gt; println(item._1 + <span class="string">": "</span> + item._2))</span><br></pre></td></tr></table></figure>

<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; parallelizeRDD = jsc.parallelize(Arrays.asList(<span class="string">"张三 语文"</span>, <span class="string">"李四 数学"</span>, <span class="string">"王五 英语"</span>));</span><br><span class="line">JavaPairRDD&lt;String, String&gt; pairRDD = parallelizeRDD.mapToPair(item -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(item.split(<span class="string">" "</span>)[<span class="number">0</span>], item.split(<span class="string">" "</span>)[<span class="number">1</span>]));</span><br><span class="line">pairRDD.foreach(t -&gt; System.out.println(t._1 + <span class="string">": "</span> + t._2));</span><br></pre></td></tr></table></figure>

<h3 id="6-8-1-PairRDD-上的-transformation-操作"><a href="#6-8-1-PairRDD-上的-transformation-操作" class="headerlink" title="6.8.1 PairRDD 上的 transformation 操作"></a>6.8.1 PairRDD 上的 transformation 操作</h3><p>PairRDDs 允许使用常规 RDDs 可用的所有转换，支持与常规 RDDs 相同功能。<br>由于 PairRDDs 包含元组，所以我们 需要传递操作元组而不是 单个元素的函数给 Spark。</p>
<p><strong>1.  filter</strong><br>在 pairRDD 上使用 filter transformation:</p>
<p><strong>Scala</strong>:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> regularRDD = sc.parallelize(<span class="type">List</span>(<span class="string">"张三 语文"</span>, <span class="string">"李四 数学"</span>, <span class="string">"王五 英语"</span>))</span><br><span class="line"><span class="keyword">val</span> pairRDD = regularRDD.map(item =&gt; (item.split(<span class="string">" "</span>)(<span class="number">0</span>), item.split(<span class="string">" "</span>)(<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> filterPairRDD = pairRDD.filter(t =&gt; t._2.equals(<span class="string">"语文"</span>))</span><br><span class="line">filterPairRDD.foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; parallelizeRDD = jsc.parallelize(Arrays.asList(<span class="string">"张三 语文"</span>, <span class="string">"李四 数学"</span>, <span class="string">"王五 英语"</span>));</span><br><span class="line">JavaPairRDD&lt;String, String&gt; pairRDD = parallelizeRDD.mapToPair(item -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(item.split(<span class="string">" "</span>)[<span class="number">0</span>], item.split(<span class="string">" "</span>)[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, String&gt; filterRDD = pairRDD.filter(t -&gt; t._2.equals(<span class="string">"数学"</span>));</span><br><span class="line">filterRDD.foreach(t -&gt; System.out.println(t));</span><br></pre></td></tr></table></figure>

<p><strong>2. reduceByKey—另一个版本的 wordcount</strong></p>
<p><strong>Scala</strong>:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fileRDD = sc.textFile(<span class="string">"in/README.md"</span>)</span><br><span class="line">  .flatMap(line =&gt; line.split(<span class="string">" "</span>))</span><br><span class="line">  .map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">  .reduceByKey(_ + _)</span><br><span class="line">  .sortBy(_._2, <span class="literal">false</span>)</span><br><span class="line">  .collect()</span><br><span class="line">  .foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; javaRDD = jsc.textFile(<span class="string">"in/README.md"</span>);</span><br><span class="line">JavaRDD&lt;String&gt; wordsRDD = javaRDD.flatMap(line -&gt; Arrays.asList(line.split(<span class="string">" "</span>)).iterator());</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD = wordsRDD.mapToPair(word -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(word, <span class="number">1</span>));</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; wordCounts = javaPairRDD.reduceByKey((a, b) -&gt; (a + b));</span><br><span class="line">JavaRDD&lt;Tuple2&lt;Integer, String&gt;&gt; tuple2JavaRDD = wordCounts.map(t -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(t._2, t._1));</span><br><span class="line">JavaRDD&lt;Tuple2&lt;Integer, String&gt;&gt; tuple2JavaRDD1 = tuple2JavaRDD.sortBy(t -&gt; t._1, <span class="keyword">false</span>, <span class="number">1</span>);</span><br><span class="line">JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; sortedRDD = tuple2JavaRDD1.map(t -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(t._2, t._1));</span><br><span class="line">sortedRDD.foreach(t-&gt; System.out.println(t));</span><br></pre></td></tr></table></figure>

<p><strong>3. combineByKey</strong><br>combineByKey 是 Spark 中一个核心的高级函数，其他一些 键值对函数底层都是用它实现的。如 groupByKey, reduceByKey 等。</p>
<p>例：计算平均分数（Scala）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">learnCombineBeKey</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ScoreDetail</span>(<span class="params">studentName: <span class="type">String</span>, subject: <span class="type">String</span>, score: <span class="type">Float</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">def</span> <span class="title">main</span>(<span class="params">args: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span>: <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .appName(<span class="string">"learnCombineBeKey"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> sc = sparkSession.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * https://www.edureka.co/blog/apache-spark-combinebykey-explained</span></span><br><span class="line"><span class="comment">      *</span></span><br><span class="line"><span class="comment">      * combineByKey transformation</span></span><br><span class="line"><span class="comment">      * combineByKey API 有三个函数</span></span><br><span class="line"><span class="comment">      * Create combiner function: x</span></span><br><span class="line"><span class="comment">      * Merge value function: y</span></span><br><span class="line"><span class="comment">      * Merger combiners function: z</span></span><br><span class="line"><span class="comment">      *</span></span><br><span class="line"><span class="comment">      * API 格式为 combineByKey(x, y, z)</span></span><br><span class="line"><span class="comment">      * 让我们看一个例子（Scala语言）：本例的目标是找到每个学生的平均分数</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> scores = <span class="type">List</span>(</span><br><span class="line">      <span class="type">ScoreDetail</span>(<span class="string">"A"</span>, <span class="string">"Math"</span>, <span class="number">98</span>),</span><br><span class="line">      <span class="type">ScoreDetail</span>(<span class="string">"A"</span>, <span class="string">"English"</span>, <span class="number">66</span>),</span><br><span class="line">      <span class="type">ScoreDetail</span>(<span class="string">"B"</span>, <span class="string">"Math"</span>, <span class="number">74</span>),</span><br><span class="line">      <span class="type">ScoreDetail</span>(<span class="string">"B"</span>, <span class="string">"English"</span>, <span class="number">80</span>),</span><br><span class="line">      <span class="type">ScoreDetail</span>(<span class="string">"C"</span>, <span class="string">"Math"</span>, <span class="number">98</span>),</span><br><span class="line">      <span class="type">ScoreDetail</span>(<span class="string">"C"</span>, <span class="string">"English"</span>, <span class="number">96</span>),</span><br><span class="line">      <span class="type">ScoreDetail</span>(<span class="string">"D"</span>, <span class="string">"Math"</span>, <span class="number">100</span>),</span><br><span class="line">      <span class="type">ScoreDetail</span>(<span class="string">"D"</span>, <span class="string">"English"</span>, <span class="number">95</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 将测试数据转换为键值对形式--键key为学生名称Student Name，值为ScoreDetail 实例对象</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="keyword">val</span> scoreWithKey = <span class="keyword">for</span> (i &lt;- scores) <span class="keyword">yield</span> (i.studentName, i)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 创建一个 pairRDD</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="keyword">val</span> scoreWithKeyRDD = sc.parallelize(scoreWithKey)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 计算 平均分数</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="keyword">val</span> avgScoresRDD = scoreWithKeyRDD.combineByKey(</span><br><span class="line">      (x: <span class="type">ScoreDetail</span>) =&gt; (x.score, <span class="number">1</span>),</span><br><span class="line">      (acc: (<span class="type">Float</span>, <span class="type">Int</span>), x: <span class="type">ScoreDetail</span>) =&gt; (acc._1 + x.score, acc._2 + <span class="number">1</span>),</span><br><span class="line">      (acc1: (<span class="type">Float</span>, <span class="type">Int</span>), acc2: (<span class="type">Float</span>, <span class="type">Int</span>)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2)</span><br><span class="line">    ).map(&#123;</span><br><span class="line">      <span class="keyword">case</span> (key, value) =&gt; (key, value._1 / value._2)</span><br><span class="line">    &#125;)</span><br><span class="line">    avgScoresRDD.foreach(println)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>4. sortByKey</strong><br>当我们在（K, V）数据集中应用 sortByKey() 函数时，数据是根据 RDD 中的键 K 排序的。</p>
<p><strong>Scala</strong>:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data =sc.parallelize(<span class="type">Seq</span>((<span class="string">"maths"</span>,<span class="number">52</span>), (<span class="string">"english"</span>,<span class="number">75</span>), (<span class="string">"science"</span>,<span class="number">82</span>), (<span class="string">"computer"</span>,<span class="number">65</span>), (<span class="string">"maths"</span>,<span class="number">85</span>)))</span><br><span class="line"><span class="keyword">val</span> sorted = data.sortByKey()</span><br><span class="line">sorted.collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>注</strong>：在上面的代码中，sortByKey() 将数据 RDD的 key(String) 按升序排序。</p>
<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2s = Arrays.asList(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"maths"</span>, <span class="number">52</span>),</span><br><span class="line">        <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"english"</span>, <span class="number">75</span>),</span><br><span class="line">        <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"science"</span>, <span class="number">82</span>),</span><br><span class="line">        <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"computer"</span>, <span class="number">65</span>),</span><br><span class="line">        <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"maths"</span>, <span class="number">85</span>));</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD = jsc.parallelizePairs(tuple2s);</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD1 = javaPairRDD.sortByKey();</span><br><span class="line">javaPairRDD1.collect().forEach(t -&gt; System.out.println(t));</span><br></pre></td></tr></table></figure>

<p><strong>5. join</strong><br>join 是数据库术语。它使用公共值组合两个表中的字段。Spark 中的 join() 操作是在 pairRDD 上定义的。pairRDD 每个元素都以 tuple 的形式出现。tuple 第一个元素是 key，第二个元素是 value。join() 操作根据 key 组合两个数据集。</p>
<p><strong>Scala</strong>:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data1 = sc.parallelize(<span class="type">Array</span>(('<span class="type">A</span>', <span class="number">1</span>), ('<span class="type">B</span>', <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">val</span> data2 = sc.parallelize(<span class="type">Array</span>(('<span class="type">A</span>', <span class="number">4</span>), ('<span class="type">A</span>', <span class="number">6</span>), ('b', <span class="number">7</span>), ('c', <span class="number">3</span>), ('c', <span class="number">8</span>)))</span><br><span class="line"><span class="keyword">val</span> result = data1.join(data2)</span><br><span class="line">println(result.collect().mkString(<span class="string">","</span>)) <span class="comment">// (A,(1,4)),(A,(1,6))</span></span><br></pre></td></tr></table></figure>

<h1 id="7-Spark-算子实战–action"><a href="#7-Spark-算子实战–action" class="headerlink" title="7. Spark 算子实战–action"></a>7. Spark 算子实战–action</h1><h2 id="7-1-count"><a href="#7-1-count" class="headerlink" title="7.1 count"></a>7.1 count</h2><p>count() 返回 RDD 中的元素数量。</p>
<h2 id="7-2-take"><a href="#7-2-take" class="headerlink" title="7.2 take"></a>7.2 take</h2><p>从 RDD 返回 n 个元素。它试图减少它访问的分区数量，不能使用此方法来控制访问元素的顺序。</p>
<h2 id="7-3-top"><a href="#7-3-top" class="headerlink" title="7.3 top"></a>7.3 top</h2><p>如果 RDD 中元素有序，那么可以使用 top() 从 RDD 中提取前几个元素。</p>
<p><strong>Scala</strong>：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fileRDD = sc.textFile(<span class="string">"in/README.md"</span>)</span><br><span class="line"><span class="keyword">val</span> lengthRDD = fileRDD.map(line =&gt; (line,line.length))</span><br><span class="line">lengthRDD.top(<span class="number">3</span>).foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; javaRDD = jsc.textFile(<span class="string">"in/README.md"</span>);</span><br><span class="line">javaRDD.top(<span class="number">3</span>).forEach(item -&gt; System.out.println(item));</span><br></pre></td></tr></table></figure>

<h2 id="7-4-countByValue"><a href="#7-4-countByValue" class="headerlink" title="7.4 countByValue"></a>7.4 countByValue</h2><p>countByValue()  返回，每个元素都出现在 RDD 中的次数。例如：<br>RDD 中的元素{1, 2, 2, 3, 4, 5, 5, 6}，“rdd.countByValue()” -&gt; {(1,1), (2,2), (3,1), (4,1), (5,2), (6,1)}，返回一个 HashMap(K, Int) ，包括每个 key 的计数。</p>
<p><strong>Scala</strong>: wordcount 另一种实现</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fileRDD = sc.textFile(<span class="string">"in/README.md"</span>)</span><br><span class="line">fileRDD.flatMap(line =&gt; line.split(<span class="string">" "</span>)).countByValue().foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; javaRDD = jsc.textFile(<span class="string">"in/README.md"</span>);</span><br><span class="line">JavaRDD&lt;String&gt; javaRDD1 = javaRDD.flatMap(line -&gt; Arrays.asList(line.split(<span class="string">" </span></span><br><span class="line"><span class="string">"</span>)).iterator());javaRDD1.countByValue().forEach((key, value) -&gt; System.out.println(key + <span class="string">","</span> + </span><br><span class="line">value));</span><br></pre></td></tr></table></figure>

<h2 id="7-5-reduce"><a href="#7-5-reduce" class="headerlink" title="7.5 reduce"></a>7.5 reduce</h2><p>reduce() 函数将 RDD 的两个元素作为输入，然后生成与输入元素相同类型的输出。这种函数的简单形式就是一个加法。</p>
<p><strong>Scala</strong>:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>(<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">43</span>,<span class="number">53</span>,<span class="number">65</span>,<span class="number">34</span>))</span><br><span class="line"><span class="keyword">val</span> sum = rdd1.reduce(_+_)</span><br><span class="line">println(sum)</span><br></pre></td></tr></table></figure>

<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; parallelizeRDD = jsc.parallelize(Arrays.asList(<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">43</span>, <span class="number">53</span>, <span class="number">65</span>, <span class="number">34</span>));</span><br><span class="line"><span class="keyword">int</span> sum = parallelizeRDD.reduce((a, b) -&gt; a + b);</span><br><span class="line">System.out.println(sum);</span><br></pre></td></tr></table></figure>

<h2 id="7-6-collect"><a href="#7-6-collect" class="headerlink" title="7.6 collect"></a>7.6 collect</h2><p>collect() 是将整个 RDDs 内容返回给 driver 程序的常见且最简单的操作。collect() 的应用是<strong>单元测试</strong>，在单元测试中，期望整个 RDD 能够装入内存。如果使用了 collect 方法，但是 driver 内存不够，则会内存溢出。</p>
<p><strong>Scala</strong>:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data1 = sc.parallelize(<span class="type">Array</span>(('<span class="type">A</span>', <span class="number">1</span>), ('<span class="type">B</span>', <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">val</span> data2 = sc.parallelize(<span class="type">Array</span>(('<span class="type">A</span>', <span class="number">4</span>), ('<span class="type">A</span>', <span class="number">6</span>), ('b', <span class="number">7</span>), ('c', <span class="number">3</span>), ('c', <span class="number">8</span>)))</span><br><span class="line"><span class="keyword">val</span> result = data1.join(data2)</span><br><span class="line">println(result.collect().mkString(<span class="string">","</span>)) <span class="comment">// (A,(1,4)),(A,(1,6))</span></span><br></pre></td></tr></table></figure>

<h2 id="7-7-foreach-（无返回值）"><a href="#7-7-foreach-（无返回值）" class="headerlink" title="7.7 foreach （无返回值）"></a>7.7 foreach （无返回值）</h2><p>当我们希望对 RDD 的每个元素应用操作，但它不应该返回值给 driver 程序时。在这种情况下，foreach() 函数是非常合适的。例如，向输入库插入一条记录。</p>
<h2 id="7-8-foreachParitition（无返回值）"><a href="#7-8-foreachParitition（无返回值）" class="headerlink" title="7.8 foreachParitition（无返回值）"></a>7.8 foreachParitition（无返回值）</h2><p>类似 mapPartitions, 区别在于：1、foreachPartition 是 action 操作 2、foreachPartition 函数没有返回值（返回值是unit）。</p>
<h2 id="7-9-作业："><a href="#7-9-作业：" class="headerlink" title="7.9 作业："></a>7.9 作业：</h2><ol>
<li>航班数最多的航空公司，算出前 6 名。</li>
<li>北京飞往重庆的航空公司，有多少个？</li>
</ol>
<p><strong>数据格式</strong>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">阿克苏,41.188341,80.293842,北京,39.92998578,116.395645,3049,CA1276,中国国航,JET,15:40,21:40,阿克苏机场,41.26940127,80.30091874,首都机场,40.06248537,116.5992671,63%,42分钟,1,0,1,0,1,0,1</span><br></pre></td></tr></table></figure>

<ol>
<li>航班数最多的航空公司，算出前 6 名。</li>
</ol>
<p><strong>Scala</strong>:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> textFileRDD = sc.textFile(<span class="string">"in/Flight.csv"</span>)</span><br><span class="line"><span class="keyword">val</span> airlinesRDD = textFileRDD.map(line =&gt; (line.split(<span class="string">","</span>)(<span class="number">8</span>), <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> resRDD = airlinesRDD.reduceByKey(_ + _)</span><br><span class="line"><span class="keyword">val</span> list1 = resRDD.sortBy(_._2, <span class="literal">false</span>)</span><br><span class="line">  .collect()</span><br><span class="line"><span class="comment">// 取前6名</span></span><br><span class="line"><span class="keyword">for</span> (i &lt;- <span class="number">0</span> to <span class="number">5</span>) &#123;</span><br><span class="line">  println(list1(i))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; textFileRDD = jsc.textFile(<span class="string">"in/Flight.csv"</span>);</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD = textFileRDD.mapToPair(line -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(line.split(<span class="string">","</span>)[<span class="number">8</span>], <span class="number">1</span>));</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD1 = javaPairRDD.reduceByKey((a, b) -&gt; (a + b));</span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; integerStringJavaPairRDD = javaPairRDD1.mapToPair(t -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(t._2, t._1));</span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; integerStringJavaPairRDD1 = integerStringJavaPairRDD.sortByKey(<span class="keyword">false</span>);</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; javaPairRDD2 = integerStringJavaPairRDD1.mapToPair(t -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(t._2, t._1));</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;String, Integer&gt;&gt; collect = javaPairRDD2.collect();</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">    System.out.println(collect.get(i));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>北京飞往重庆的航空公司，有多少个？</li>
</ol>
<p><strong>Scala</strong>:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">BJ_CQ_RDD</span> = textFileRDD.filter(line =&gt; (line.split(<span class="string">","</span>)(<span class="number">0</span>).equals(<span class="string">"北京"</span>) &amp;&amp; line.split(<span class="string">","</span>)(<span class="number">3</span>).equals(<span class="string">"重庆"</span>)))</span><br><span class="line"><span class="keyword">val</span> count = <span class="type">BJ_CQ_RDD</span>.map(line =&gt; (line.split(<span class="string">","</span>)(<span class="number">8</span>), <span class="number">1</span>)).countByKey().size</span><br><span class="line">println(count)</span><br></pre></td></tr></table></figure>

<p><strong>Java</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; filterRDD = textFileRDD.filter(line -&gt; (line.split(<span class="string">","</span>)[<span class="number">0</span>].equals(<span class="string">"北京"</span>) &amp;&amp; line.split(<span class="string">","</span>)[<span class="number">3</span>].equals(<span class="string">"重庆"</span>)));</span><br><span class="line"><span class="keyword">int</span> size = filterRDD.mapToPair(line -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(line.split(<span class="string">","</span>)[<span class="number">8</span>], <span class="number">1</span>)).countByKey().size();</span><br><span class="line">System.out.println(size);</span><br></pre></td></tr></table></figure>

<h1 id="8-Spark-RDD-分区实战"><a href="#8-Spark-RDD-分区实战" class="headerlink" title="8. Spark RDD 分区实战"></a>8. Spark RDD 分区实战</h1><h2 id="8-1-RDD-partition-概念"><a href="#8-1-RDD-partition-概念" class="headerlink" title="8.1 RDD partition 概念"></a>8.1 RDD partition 概念</h2><p>我们处理大数据时，由于数据量太大，以至于单个节点无法完全存储、计算。所以这些数据需要分割成多个数据块 block，以利用多个集群节点的存储、计算资源。Spark 自动对 RDDs 中的大量数据元素进行分区，并在 worker 节点之间分配分区，计算。分区是逻辑上。</p>
<h2 id="8-2-RDD-partition-的相关属性"><a href="#8-2-RDD-partition-的相关属性" class="headerlink" title="8.2 RDD partition 的相关属性"></a>8.2 RDD partition 的相关属性</h2><table>
<thead>
<tr>
<th>属性</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>partitions</td>
<td>返回包含 RDD 所有分区引用 的一个数组</td>
</tr>
<tr>
<td>partitions.size</td>
<td>返回 RDD 的分区数量</td>
</tr>
<tr>
<td>partitioner</td>
<td>返回下列分区器之一：<br>NONE<br>HashPartitioner<br>RangePartitioner<br>自定义分区器</td>
</tr>
</tbody></table>
<p>Spark 使用 partitioner 属性来确定分区算法，以此来确定哪些 worker 需要存储特定的 RDD记录。如果 partitoner 的值为 NONE，意思是分区不是基于数据的特性 ，但是分布是随机的，并且保证在节点之间是均匀地。</p>
<h2 id="8-3-查看-RDD-partition-信息"><a href="#8-3-查看-RDD-partition-信息" class="headerlink" title="8.3 查看 RDD partition 信息"></a>8.3 查看 RDD partition 信息</h2><p><strong>textfile 方法的 partition size 查看</strong>：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">println(textFileRDD.partitions.size)</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：使用 textFile 方法读取数据，可以设置 partition 大小：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> textFileRDD = sc.textFile(<span class="string">"in/Flight.csv"</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>在集群环境中，读取本地文件、HDFS数据，由数据的 block 个数决定，最小为2。</p>
<p><strong>特殊情况</strong>：如果 local 模式，单线程运行，默认 partitions.size 为1。（项目中不会使用此情况）</p>
<p><strong>注意</strong>：每个 partition 会运行一个task 来处理其中的数据元素。</p>
<h2 id="8-4-RDD-的初始分区"><a href="#8-4-RDD-的初始分区" class="headerlink" title="8.4 RDD 的初始分区"></a>8.4 RDD 的初始分区</h2><blockquote>
<p>conf.set(“spark.default.parallelism”,”3”)   // 设置默认的并行度</p>
</blockquote>
<p>local: 一个线程———–sc.defaultParallelism 值为1<br>local[*]: 服务器core 数量——–sc. defaultParallelism 的值为8<br>local[4]: 4个线程———–sc.defaultParallelism 的值为4</p>
<p><strong>spark.default.parallelism 参数值的说明</strong>：<br>如果 spark-default.conf 或 SparkConf 中设置了 spark.default.parallelism 参数值，那么 spark.default.parallelism = 设置值；<br>如果 spark-default.conf 或 SparkConf 中没有设置，那么：</p>
<p><strong>local 模式</strong>：<br>local: spark.default.parallelism =1<br>local[4]: spark.default.parallelism = 4</p>
<p><strong>yarn 和 standalone 模式</strong>：<br>spark.default.parallelism = max(所有 executor 使用的core 总数, 2)</p>
<p><strong>由上述规则，确定 spark.default.parallelism 的默认值</strong><br>当Spark 程序执行时，会生成个 SparkContext 对象，同时会生成以下两个参数值：<br>sc.defaultParallelism = spark.default.parallelism<br>sc.defaultMinPartitions = min(spark.default.parallelism, 2)</p>
<p>当sc.defaultParallelism 和 sc.defaultMinPartitions 确认了，就可以推算出RDD 的分区数了。</p>
<p><strong>有三种产生 RDD 的方式</strong>：</p>
<ol>
<li>通过集合创建<blockquote>
<p>val rdd = sc.parallelize(1 to 100)</p>
</blockquote>
</li>
</ol>
<p>没有指定分区数，则rdd的分区数 = sc.defaultParallelism</p>
<ol start="2">
<li>通过外部存储创建<blockquote>
<p>val rdd = sc.textFile(filePath)</p>
</blockquote>
</li>
</ol>
<p>2.1 从本地 文件生成 RDD，没有指定分区数，则默认分区规则为: rdd 的分区数 = max(本地 file 的分片数, sc.defaultMinPartitions)<br>2.2 从 HDFS 读取数据生成 RDD，没有指定分区数，则默认 分区规则为：rdd 的分区数 = max(HDFS文件的 block 数, sc.defaultMinPartitions)</p>
<ol start="3">
<li>通过已有 RDD 产生新的 RDD，新 RDD的分区数遵循<strong>遗传</strong>特性。见下节。</li>
</ol>
<p><strong>注</strong>：项目中，在 spark-default.conf 文件中，spark.default.parallelism 属性值设置为 executor-cores * executors 个数 * 3</p>
<h2 id="8-5-Transformation-操作对分区的影响"><a href="#8-5-Transformation-操作对分区的影响" class="headerlink" title="8.5 Transformation 操作对分区的影响"></a>8.5 Transformation 操作对分区的影响</h2><h3 id="8-5-1-普通-RDD-操作"><a href="#8-5-1-普通-RDD-操作" class="headerlink" title="8.5.1 普通 RDD 操作"></a>8.5.1 普通 RDD 操作</h3><table>
<thead>
<tr>
<th>API调用</th>
<th>RDD 分区属性值<br>partition.size</th>
<th>RDD 分区属性值<br>partitioner</th>
</tr>
</thead>
<tbody><tr>
<td>map(),  flatMap(), distinct()</td>
<td>与父RDD相同</td>
<td>NONE</td>
</tr>
<tr>
<td>filter()</td>
<td>与父RDD相同</td>
<td>与父RDD相同</td>
</tr>
<tr>
<td>rdd.union(otherRDD)</td>
<td>rdd.partitions.size + otherRDD.partitions.size</td>
<td>NONE</td>
</tr>
<tr>
<td>rdd.intersection(otherRDD)</td>
<td>max(rdd.partitions.size, otherRDD.partitions.size)</td>
<td>NONE</td>
</tr>
<tr>
<td>rdd.subtract(otherRDD)</td>
<td>rdd.partitions.size</td>
<td>NONE</td>
</tr>
<tr>
<td>rdd.cartesian(otherRDD)</td>
<td>rdd.partitions.size * otherRDD.partitions.size</td>
<td>NONE</td>
</tr>
</tbody></table>
<h3 id="8-5-2-Key-value-RDD-操作"><a href="#8-5-2-Key-value-RDD-操作" class="headerlink" title="8.5.2 Key-value RDD 操作"></a>8.5.2 Key-value RDD 操作</h3><table>
<thead>
<tr>
<th>API调用</th>
<th>RDD 分区属性值<br>partition.size</th>
<th>RDD 分区属性值<br>partitioner</th>
</tr>
</thead>
<tbody><tr>
<td>reduceByKey(),foldByKey(),combineByKey(),groupByKey()</td>
<td>与父RDD相同</td>
<td>HashPartitioner</td>
</tr>
<tr>
<td>sortByKey</td>
<td>与父RDD相同</td>
<td>RangePartitioner</td>
</tr>
<tr>
<td>mapValues(), flatMapValues()</td>
<td>与父RDD相同</td>
<td>与父RDD相同</td>
</tr>
<tr>
<td>cogroup(),join(),leftOuterJoin(), rightOuterJoin()</td>
<td>取决于所涉及的两个 RDDs 的某些输入属性</td>
<td>HashPartitioner</td>
</tr>
</tbody></table>
<h2 id="8-6-有多少分区是合适的（重点！！）"><a href="#8-6-有多少分区是合适的（重点！！）" class="headerlink" title="8.6 有多少分区是合适的（重点！！）"></a>8.6 有多少分区是合适的（重点！！）</h2><p>分区数量太少、太多都有一定的优点和缺点。因此，建议根据集群配置和需求进行明智的分区。<br>Core-partition-task</p>
<h3 id="8-6-1-分区太少的缺点"><a href="#8-6-1-分区太少的缺点" class="headerlink" title="8.6.1 分区太少的缺点"></a>8.6.1 分区太少的缺点</h3><p>减少并发性——您没有使用并行性的优点。可能存在空闲的 wroker 节点。<br>数据倾斜和不恰当的资源利用——数据可能在一个分区上倾斜，因此一个 worker 可能比其他 worker 做的更多，因此可能会出现资源问题。</p>
<h3 id="8-6-2-分区太多的缺点"><a href="#8-6-2-分区太多的缺点" class="headerlink" title="8.6.2 分区太多的缺点"></a>8.6.2 分区太多的缺点</h3><p>任务调度可能比实际执行时间花费更多的时间。</p>
<p><strong>因此，在分区的数量之间存在权衡。推荐如下</strong>：</p>
<ol>
<li><p>可用 core 数量的2-3 倍。Spark 只为 RDD 的每个分区运行一个并发任务，最多可以同时运行集群中的核心数量个 task，分区数量至少与可用 core 数量相等。可以通过调用 sc.defaultParallelism 获得可用 core 值。单个分区的数据量大小最终取决于执行程序的可用内存。</p>
</li>
<li><p>WebUI 上查看任务执行，至少需要 100+ ms 时间。如果所用 时间少于 100ms，那么应用程序可能会花更多的时间来调度任务。此时就要减少 partition 的数量。</p>
</li>
</ol>
<h2 id="8-7-Spark-中的分区器"><a href="#8-7-Spark-中的分区器" class="headerlink" title="8.7 Spark 中的分区器"></a>8.7 Spark 中的分区器</h2><p>要使用分区器，首先要<strong>创建 PairRDD类型的 RDD</strong>。<br>Spark 有两种类型的分区器。一个是 HashPartitioner，另一个是 RangePartitioner。</p>
<h3 id="8-7-1-HashPartitioner"><a href="#8-7-1-HashPartitioner" class="headerlink" title="8.7.1 HashPartitioner"></a>8.7.1 HashPartitioner</h3><p>HashPartitioner 基于 Java 的 Object.hashcode() 方法进行分区。</p>
<h3 id="8-7-2-RangePartitioner"><a href="#8-7-2-RangePartitioner" class="headerlink" title="8.7.2 RangePartitioner"></a>8.7.2 RangePartitioner</h3><p>如果有可排序的记录，那么范围分区将几乎在相同的范围内划分记录。范围 Range 是 通过采样传入 RDD的数据内容来确定的。首先，RangePartitioner 将根据 key 对记录进行排序，然后根据给定的值将记录划分为 若干个分区。</p>
<h3 id="8-7-3-自定义分区器"><a href="#8-7-3-自定义分区器" class="headerlink" title="8.7.3 自定义分区器"></a>8.7.3 自定义分区器</h3><p>还可以通过扩展 Spark 中的默认分区器类来定制 需要的分区数量和应该存储在这些分区中的内容。</p>
<p><strong>代码示例</strong>：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> listRDD = sc.parallelize((<span class="number">1</span> to <span class="number">10</span>).toList)</span><br><span class="line"><span class="keyword">val</span> pairRDD = listRDD.map(num =&gt; (num, num))</span><br><span class="line">println(<span class="string">"NumPartitions: "</span> + pairRDD.getNumPartitions) <span class="comment">// NumPartitions: 8</span></span><br><span class="line">println(<span class="string">"Partitioner: "</span> + pairRDD.partitioner)  <span class="comment">// Partitioner: None</span></span><br><span class="line">pairRDD.saveAsTextFile(<span class="string">"out/None"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用 HashPartitioner 并 设置分区个数</span></span><br><span class="line"><span class="keyword">val</span> hashPartitionerRDD = pairRDD.partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">4</span>))</span><br><span class="line">hashPartitionerRDD.saveAsTextFile(<span class="string">"out/hashPartition4"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// coalesce 方法只能用来减少 分区数量，不能用来增加分区数量</span></span><br><span class="line"><span class="comment">// partitionBy 方法可以减少，也可以增加</span></span><br><span class="line">hashPartitionerRDD.coalesce(<span class="number">2</span>).saveAsTextFile(<span class="string">"out/hashPartition2"</span>)</span><br><span class="line">println(hashPartitionerRDD.partitioner) <span class="comment">// Some(org.apache.spark.HashPartitioner@4)</span></span><br></pre></td></tr></table></figure>

<h1 id="9-Spark-RDD-数据保存实战"><a href="#9-Spark-RDD-数据保存实战" class="headerlink" title="9. Spark RDD 数据保存实战"></a>9. Spark RDD 数据保存实战</h1><h2 id="9-1-保存数据到-HDFS"><a href="#9-1-保存数据到-HDFS" class="headerlink" title="9.1 保存数据到 HDFS"></a>9.1 保存数据到 HDFS</h2><p><strong>代码示例</strong>：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> textFileRDD = sc.textFile(<span class="string">"hdfs://master01:8020/in/README.txt"</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> wordsRDD = textFileRDD.flatMap(line =&gt; line.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> wordcountPairRDD = wordsRDD.map(w =&gt; (w, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> wordcountRDD = wordcountPairRDD.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">wordcountRDD.saveAsTextFile(<span class="string">"hdfs://master01:8020/out/wordcount"</span>)</span><br></pre></td></tr></table></figure>

<h2 id="9-2-保存数据到-mysql-数据库"><a href="#9-2-保存数据到-mysql-数据库" class="headerlink" title="9.2 保存数据到 mysql 数据库"></a>9.2 保存数据到 mysql 数据库</h2><h3 id="9-2-1-读"><a href="#9-2-1-读" class="headerlink" title="9.2.1 读"></a>9.2.1 读</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//mysql 读</span></span><br><span class="line"><span class="keyword">val</span> jdbcDF = sparkSession.read</span><br><span class="line">  .format(<span class="string">"jdbc"</span>)</span><br><span class="line">  .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://master01:3306/test"</span>)</span><br><span class="line">  .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">  .option(<span class="string">"password"</span>, <span class="string">"Mysql123!"</span>)</span><br><span class="line">  .option(<span class="string">"dbtable"</span>, <span class="string">"flight"</span>)</span><br><span class="line">  .load()</span><br><span class="line">jdbcDF.printSchema()</span><br></pre></td></tr></table></figure>

<h3 id="9-2-2-写"><a href="#9-2-2-写" class="headerlink" title="9.2.2 写"></a>9.2.2 写</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(<span class="type">List</span>(</span><br><span class="line">  <span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, nullable = <span class="literal">false</span>),</span><br><span class="line">  <span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">IntegerType</span>, nullable = <span class="literal">false</span>),</span><br><span class="line">  <span class="type">StructField</span>(<span class="string">"gender"</span>, <span class="type">StringType</span>, nullable = <span class="literal">false</span>)</span><br><span class="line">))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rowRDD = sc.parallelize(<span class="type">Seq</span>(</span><br><span class="line">  <span class="type">Row</span>(<span class="string">"张1"</span>, <span class="number">18</span>, <span class="string">"男"</span>),</span><br><span class="line">  <span class="type">Row</span>(<span class="string">"张2"</span>, <span class="number">19</span>, <span class="string">"女"</span>),</span><br><span class="line">  <span class="type">Row</span>(<span class="string">"张3"</span>, <span class="number">10</span>, <span class="string">"男"</span>),</span><br><span class="line">  <span class="type">Row</span>(<span class="string">"张4"</span>, <span class="number">48</span>, <span class="string">"女"</span>),</span><br><span class="line">  <span class="type">Row</span>(<span class="string">"张5"</span>, <span class="number">68</span>, <span class="string">"男"</span>),</span><br><span class="line">  <span class="type">Row</span>(<span class="string">"张6"</span>, <span class="number">16</span>, <span class="string">"男"</span>)</span><br><span class="line">))</span><br><span class="line"><span class="keyword">val</span> df = sparkSession.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line"><span class="comment">// mysql 写</span></span><br><span class="line">df.write</span><br><span class="line">  .format(<span class="string">"jdbc"</span>)</span><br><span class="line">  .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://master01:3306/test"</span>)</span><br><span class="line">  .option(<span class="string">"dbtable"</span>, <span class="string">"user"</span>)</span><br><span class="line">  .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">  .option(<span class="string">"password"</span>, <span class="string">"Mysql123!"</span>)</span><br><span class="line">  .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">  .save()</span><br></pre></td></tr></table></figure>

<h2 id="9-3-保存数据到-kafka"><a href="#9-3-保存数据到-kafka" class="headerlink" title="9.3 保存数据到 kafka"></a>9.3 保存数据到 kafka</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// producer 配置</span></span><br><span class="line"><span class="keyword">val</span> props = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">props.setProperty(<span class="type">ProducerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span>, <span class="string">"master01:9092"</span>)</span><br><span class="line">props.setProperty(<span class="type">ProducerConfig</span>.<span class="type">VALUE_SERIALIZER_CLASS_CONFIG</span>, classOf[<span class="type">StringSerializer</span>].getName)</span><br><span class="line">props.setProperty(<span class="type">ProducerConfig</span>.<span class="type">KEY_SERIALIZER_CLASS_CONFIG</span>, classOf[<span class="type">StringSerializer</span>].getName)</span><br><span class="line"></span><br><span class="line"><span class="comment">// producer 发送 RDD 数据</span></span><br><span class="line"><span class="keyword">val</span> textFileRDD = sc.textFile(<span class="string">"in/Flight1.csv"</span>)</span><br><span class="line">textFileRDD.foreach(line =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> producer = <span class="keyword">new</span> <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>](props)</span><br><span class="line">  <span class="keyword">val</span> message = <span class="keyword">new</span> <span class="type">ProducerRecord</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="string">"myTopic"</span>, line)</span><br><span class="line">  println(message)</span><br><span class="line">  producer.send(message)</span><br><span class="line">  <span class="type">Thread</span>.sleep(<span class="number">3000</span>)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<h1 id="10-Spark-RDD-缓存实战"><a href="#10-Spark-RDD-缓存实战" class="headerlink" title="10. Spark RDD 缓存实战"></a>10. Spark RDD 缓存实战</h1><h2 id="10-1-前言"><a href="#10-1-前言" class="headerlink" title="10.1 前言"></a>10.1 前言</h2><p>一个 action 会启动一个 job， 一个 job 里面有一个或多个 stage，一个 stage 里面有一个或者多个 task。</p>
<p>Repartiton 引起 shuffle 操作，shuffle 操作发生的时候，stage 会一分为二。</p>
<p>窄依赖， 宽依赖<br>一种性能调优的方式。</p>
<h2 id="10-2-要点"><a href="#10-2-要点" class="headerlink" title="10.2 要点"></a>10.2 要点</h2><ol>
<li><p><strong>缓存</strong> 和 <strong>持久化</strong>是 Spark 计算过程中的调优技术。缓存和持久化可以保存中间计算结果，以便在后续的 stage 中重用，而不需要再次从头计算。这些中间结果以 RDD 的形式保存在内存（默认）中，或者磁盘中。</p>
</li>
<li><p>StorageLevel 描述了 RDD 是如何被<strong>持久化</strong>（persist）的，可以提供如下相关信息：</p>
</li>
</ol>
<ul>
<li>RDD 持久化磁盘存储还是内存存储</li>
<li>RDD 持久化是否使用了 off-heap</li>
<li>RDD 是否需要被序列化</li>
<li>缓存的副本是多少（默认是 1）</li>
</ul>
<ol start="3">
<li><strong>StorageLevel</strong> 的值包括：</li>
</ol>
<ul>
<li>NONE（默认）</li>
<li>DISK_ONLY: RDD 只是存储在磁盘，内存消耗低，CPU 密集型。</li>
<li>DISK_ONLY_2</li>
<li>MEMORY_ONLY（cache 操作）：RDD 以非序列化的 Java 对象存储在 JVM中。如果 RDD 的大小超过了内存大小，那么某些 partition 将会不缓存，下次使用时重新计算。这种存储级别比较耗内存，但是不耗 CPU。数据只存储在内存，不存储在磁盘。</li>
<li>MEMORY_ONLY_2</li>
<li>MEMORY_ONLY_SER: RDD 以序列化 Java 对象（每个 partition 一个字节数组）的形式存储。在这个级别，内存空间 使用很低，CPU计算时间高。</li>
<li>MEMORY_ONLY_SER_2</li>
<li>MEMORY_AND_DISK: RDD 以非序列化的 Java 对象存储在 JVM 中。当 RDD 的大小超过了内存 大小，多出的 partition  会缓存在磁盘上，后续计算如果用到这些多出的 partiton，会从磁盘获取。这种存储级别比较耗内存，CPU消耗一般。</li>
<li>MEMORY_AND_DISK_2</li>
<li>MEMORY_AND_DISK_SER: 与 MEMORY_ONLY_SER 类似，只是将大于内存的partition 数据序列化到磁盘，而不是重新计算。内存消耗低，CPU密集型。</li>
<li>MEMORY_AND_DISK_SER_2</li>
<li>OFF_HEAP</li>
</ul>
<p>可以使用 <strong>getStorageLevel</strong> 方法查看 RDD 的 StorageLevel:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> textFileRDD = sc.textFile(<span class="string">"\in\README.txt"</span>)</span><br><span class="line">println(textFileRDD.getStorageLevel)</span><br><span class="line">println(textFileRDD.getStorageLevel)</span><br></pre></td></tr></table></figure>

<p><strong>输出</strong>：</p>
<blockquote>
<p>StorageLevel(1 replicas)</p>
</blockquote>
<ol start="4">
<li><p>RDD 可以被缓存（cache）到内存，使用 cache() 方法，也可以被持久化（persist），使用 persist() 方法。</p>
</li>
<li><p>cache() 和 persist() 方法的<strong>区别</strong>在于：cache() 等价于 persist(MEMEORY_ONLY)，即 cache() 仅仅是 persist() 使用默认存储级别 MEMORY_ONLY 的一种情况。使用 persist() 方法可以设置不同的 StorageLevel值。</p>
</li>
<li><p>对于<strong>迭代算法</strong>，缓存和持久化是一个重要的工具。因为，当我们在一个节点上缓存了 RDD 的某个 partiton 到内存中，其就可以在下面的计算中重复使用，而不需要从头计算，可以使计算性能提高 <strong>10</strong>倍。如果缓存中的某个 partiton 丢失或者不可用，根据 Spark RDD 的容错特性，Spark 会从头计算这个 partition。</p>
</li>
<li><p>什么时候需要对 RDD 进行持久化？在 Spark 中，我们可以多次使用同一个 RDD，如：使用 RDD 计算 count()、max()、min()等 action 操作。而且这些操作可能<strong>很耗内存</strong>，尤其是迭代算法（机器学习）。为了解决<strong>频繁重复计算</strong>的问题，此时就需要对 RDD 进行持久化。</p>
</li>
<li><p>Spark 自动监控每个节点的<strong>缓存</strong>和以 <strong>LRU</strong>（最近最少使用）方式删除旧数据分区。LRU算法，保证了最常用的数据被缓存。我们 也可以使用 <strong>RDD.unpersist()</strong> 方法手动删除缓存。</p>
</li>
<li><p>Spark 会在 <strong>shuffle</strong> 操作中<strong>自动持久化</strong>一些中间数据（例如 redueByKey），即使没有调用 persist 方法。这样做是为了避免在 shuffle 期间节点故障时重新计算整个输入。如果用户准备重用生成的 RDD，推荐显式调用持久化。</p>
</li>
</ol>
<h2 id="10-3-RDD-持久化存储级别如何选择"><a href="#10-3-RDD-持久化存储级别如何选择" class="headerlink" title="10.3 RDD 持久化存储级别如何选择"></a>10.3 RDD 持久化存储级别如何选择</h2><p>Spark 的存储级别是为了在<strong>内存使用</strong>和 <strong>CPU 效率</strong>之间 提供不同的权衡，具体选择哪个存储级别，可以从以下方面考虑：</p>
<ul>
<li>如果 RDDs 数据适合默认存储级别（MEMORY_ONLY），那么就使用默认。此时，RDD 的运算速度最快。</li>
<li>如果没有，请尝试使用 MEMORY_ONLY_SER 并选择一个快速序列化库，以使对象更节省空间，但访问速度仍然相当快。</li>
<li>不要持久化到磁盘，除非计算 数据集的函数很耗时，或者 过滤了大量 数据。因为，从磁盘读取分区，可能没有重新计算快。</li>
<li>如果需要快速的故障恢复，则使用副本存储级别。</li>
</ul>

    
  </div>

</article>


   
  <div class="text-center donation">
    <div class="inner-donation">
      <span class="btn-donation">支持一下</span>
      <div class="donation-body">
        <div class="tip text-center">扫一扫，支持forsigner</div>
        <ul>
        
          <li class="item">
            
              <span>微信扫一扫</span>
            
            <img src="/images/qr-wechat.png" alt="">
          </li>
        
          <li class="item">
            
              <span>支付宝扫一扫</span>
            
            <img src="/images/qr-alipay.png" alt="">
          </li>
        
        </ul>
      </div>
    </div>
  </div>


   
  <div class="box-prev-next clearfix">
    <a class="show pull-left" href="/2019/07/23/Hadoop-集群-HA-架构配置/">
        <i class="icon icon-angle-left"></i>
    </a>
    <a class="show pull-right" href="/2019/07/25/SparkCore-知识点/">
        <i class="icon icon-angle-right"></i>
    </a>
  </div>




</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">关闭</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              rel="noopener noreferrer"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/category/"
              rel="noopener noreferrer"
              target="_self"
              >
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              rel="noopener noreferrer"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              rel="noopener noreferrer"
              target="_self"
              >
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/atom.xml"
              rel="noopener noreferrer"
              target="_blank"
              >
              RSS
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    

    

    
    

    

    
    

    

<!-- Gitalk评论插件通用代码 -->
<div id="gitalk-container"></div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="https://cdn.bootcss.com/blueimp-md5/2.11.1/js/md5.js"></script>
<script>
const gitalk = new Gitalk({
  clientID: 'cffabda338955fb33e72',
  clientSecret: '27685d32607acc9c76041016860f5434fa1d65d0',
  repo: 'gitalk_comment',
  owner: 'Miracle-Xing',
  // 在这里设置一下截取前50个字符串, 这是因为 github 对 label 的长度有了要求, 如果超过
  // 50个字符串则会报错.
  //id: location.pathname.split('/').pop().substring(0, 49),
   id: md5(location.pathname),
  // id: title,
  admin: ['Miracle-Xing'],
  // facebook-like distraction free mode
  distractionFreeMode: false
})
gitalk.render('gitalk-container')
</script>
<!-- Gitalk代码结束 -->



  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

</body>
</html>

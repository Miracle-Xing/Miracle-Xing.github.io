<!DOCTYPE html>


  <html class="light page-post">


<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>Spark Streaming 入门 | 邢大强的blog</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="Spark,Spark Streaming,">
  

  <meta name="description" content="1. 诞生背景传统 MapReduce 等批处理框架已经满足不了人们对实时性的需求，出现了 Storm，Flink 等一批实时计算框架。 Spark Streaming 是在 Spark 批处理基础上构建的流式框架。 2. Duration 时间窗口（Batch Duration，Slide Duration，Window Duration）2.1 Batch Duration批处理间隔，它是指">
<meta name="keywords" content="Spark,Spark Streaming">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark Streaming 入门">
<meta property="og:url" content="https://miracle-xing.github.io/2019/09/04/Spark-Streaming-入门/index.html">
<meta property="og:site_name" content="邢大强的blog">
<meta property="og:description" content="1. 诞生背景传统 MapReduce 等批处理框架已经满足不了人们对实时性的需求，出现了 Storm，Flink 等一批实时计算框架。 Spark Streaming 是在 Spark 批处理基础上构建的流式框架。 2. Duration 时间窗口（Batch Duration，Slide Duration，Window Duration）2.1 Batch Duration批处理间隔，它是指">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/09/04/f6f3b370-cef0-11e9-ba98-ab6252b342d8.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/09/04/994c1090-cef1-11e9-ba98-ab6252b342d8.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/09/04/c4e912c0-cef1-11e9-ba98-ab6252b342d8.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/09/04/f2d0a130-cef1-11e9-ba98-ab6252b342d8.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/09/04/ff966f80-cef1-11e9-ba98-ab6252b342d8.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/09/04/076f6860-cef2-11e9-ba98-ab6252b342d8.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/09/04/193d8360-cef2-11e9-ba98-ab6252b342d8.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/09/07/9d534fb0-d15c-11e9-835c-d1aed722d3e1.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/09/07/29c634d0-d15d-11e9-835c-d1aed722d3e1.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/09/08/6541ad30-d1f0-11e9-835c-d1aed722d3e1.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/09/08/be308d80-d1f0-11e9-835c-d1aed722d3e1.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/09/08/d09f0730-d1f0-11e9-835c-d1aed722d3e1.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/09/08/f1217a10-d1f0-11e9-835c-d1aed722d3e1.png">
<meta property="og:image" content="https://miracle-xing.github.io/images/2019/09/08/1a3b7ae0-d1f1-11e9-835c-d1aed722d3e1.png">
<meta property="og:updated_time" content="2019-09-08T06:44:02.953Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark Streaming 入门">
<meta name="twitter:description" content="1. 诞生背景传统 MapReduce 等批处理框架已经满足不了人们对实时性的需求，出现了 Storm，Flink 等一批实时计算框架。 Spark Streaming 是在 Spark 批处理基础上构建的流式框架。 2. Duration 时间窗口（Batch Duration，Slide Duration，Window Duration）2.1 Batch Duration批处理间隔，它是指">
<meta name="twitter:image" content="https://miracle-xing.github.io/images/2019/09/04/f6f3b370-cef0-11e9-ba98-ab6252b342d8.png">

  

  
    <link rel="icon" href="/assets/img/m.png">
  

  <link href="/css/styles.css?v=c114cbeddx" rel="stylesheet">


  
    <link rel="stylesheet" href="/css/personal-style.css">
  

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-38189205-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?57e94d016e201fba3603a8a2b0263af0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  
  <script type="text/javascript">
	(function(){
	    var bp = document.createElement('script');
	    var curProtocol = window.location.protocol.split(':')[0];
	    if (curProtocol === 'https') {
	        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
	    }
	    else {
	        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
	    }
	    var s = document.getElementsByTagName("script")[0];
	    s.parentNode.insertBefore(bp, s);
	})();
  </script>



  
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">
  

</head>
</html>
<body>


  
    <span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/archives/"
            rel="noopener noreferrer"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/category/"
            rel="noopener noreferrer"
            target="_self"
            >
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tag/"
            rel="noopener noreferrer"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/about/"
            rel="noopener noreferrer"
            target="_self"
            >
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/atom.xml"
            rel="noopener noreferrer"
            target="_blank"
            >
            RSS
          </a>
        </li>
      
    </ul>
  </div>


</div>


  <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-诞生背景"><span class="toc-text">1. 诞生背景</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Duration-时间窗口（Batch-Duration，Slide-Duration，Window-Duration）"><span class="toc-text">2. Duration 时间窗口（Batch Duration，Slide Duration，Window Duration）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Batch-Duration"><span class="toc-text">2.1 Batch Duration</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-Slide-Duration（处理数据的时间间隔）和-Window-Duration（处理的数据量间隔）"><span class="toc-text">2.2 Slide Duration（处理数据的时间间隔）和 Window Duration（处理的数据量间隔）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-DStream"><span class="toc-text">3. DStream</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-DStream-定义"><span class="toc-text">3.1 DStream 定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-DStream-输入源"><span class="toc-text">3.2 DStream 输入源</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-DStream-操作"><span class="toc-text">3.3 DStream 操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-1-转换操作"><span class="toc-text">3.3.1 转换操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-2-输出操作"><span class="toc-text">3.3.2 输出操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-例一"><span class="toc-text">4. 例一</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-需求"><span class="toc-text">4.1 需求</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-代码"><span class="toc-text">4.2 代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-执行"><span class="toc-text">4.3 执行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-说明"><span class="toc-text">4.4 说明</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-5-Streaming-作业和-Spark-作业之间的关系"><span class="toc-text">4.5 Streaming 作业和 Spark 作业之间的关系</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-Spark-Streaming-架构"><span class="toc-text">5. Spark Streaming 架构</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-Spark-栈"><span class="toc-text">6. Spark 栈</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-例二"><span class="toc-text">7. 例二</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-需求"><span class="toc-text">7.1 需求</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-代码"><span class="toc-text">7.2 代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-执行"><span class="toc-text">7.3 执行</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-例三"><span class="toc-text">8. 例三</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-需求"><span class="toc-text">8.1 需求</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-代码"><span class="toc-text">8.2 代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-3-执行"><span class="toc-text">8.3 执行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-4-说明"><span class="toc-text">8.4 说明</span></a></li></ol></li></ol>
  </div>



<div class="content content-post CENTER">
   <article id="post-Spark-Streaming-入门" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">Spark Streaming 入门</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2019.09.04</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>Miracle</span>
        </span>
      

      
  <span class="article-category">
    <i class="icon-list"></i>
    <a class="article-category-link" href="/categories/笔记/">笔记</a>
  </span>



      

      
      <i class="fa fa-eye"></i> 
        <span id="busuanzi_container_page_pv">
           &nbsp热度 <span id="busuanzi_value_page_pv">
           <i class="fa fa-spinner fa-spin"></i></span>℃
        </span>
      
      
    </div>
  </header>

  <div class="article-content">
    
      <h1 id="1-诞生背景"><a href="#1-诞生背景" class="headerlink" title="1. 诞生背景"></a>1. 诞生背景</h1><p>传统 MapReduce 等批处理框架已经满足不了人们对实时性的需求，出现了 Storm，Flink 等一批实时计算框架。</p>
<p>Spark Streaming 是在 Spark 批处理基础上构建的流式框架。</p>
<h1 id="2-Duration-时间窗口（Batch-Duration，Slide-Duration，Window-Duration）"><a href="#2-Duration-时间窗口（Batch-Duration，Slide-Duration，Window-Duration）" class="headerlink" title="2. Duration 时间窗口（Batch Duration，Slide Duration，Window Duration）"></a>2. Duration 时间窗口（Batch Duration，Slide Duration，Window Duration）</h1><h2 id="2-1-Batch-Duration"><a href="#2-1-Batch-Duration" class="headerlink" title="2.1 Batch Duration"></a>2.1 Batch Duration</h2><p>批处理间隔，它是指 Spark Streaming 以多少时间间隔为单位来提交任务逻辑。比如 1min，30s。这一参数将会伴随整个 <strong>StreamingContext</strong> 的生命周期且<strong>无法重新设置</strong>。</p>
<p>Spark Streaming 处理数据的单位是一批，Spark Streaming 系统需要设置间隔使得数据汇总到一定的量后再一并进行批处理，这个间隔就是 Batch Duration。此参数决定提交作业的频率和数据处理的延迟。</p>
<h2 id="2-2-Slide-Duration（处理数据的时间间隔）和-Window-Duration（处理的数据量间隔）"><a href="#2-2-Slide-Duration（处理数据的时间间隔）和-Window-Duration（处理的数据量间隔）" class="headerlink" title="2.2 Slide Duration（处理数据的时间间隔）和 Window Duration（处理的数据量间隔）"></a>2.2 Slide Duration（处理数据的时间间隔）和 Window Duration（处理的数据量间隔）</h2><p>默认值都是和 Batch Duration相同，也可自行设置，但必须为 Batch Duration 的整数倍。<br><img src="/images/2019/09/04/f6f3b370-cef0-11e9-ba98-ab6252b342d8.png" alt="图片2.png"></p>
<p>Spark Streaming 针对 Slide Duration 和 Window Duration 的保证：</p>
<ul>
<li>因为每个 Batch 内的数据可能被后几个窗口间隔所处理，所以数据会保存在 Spark Streaming 系统中，不会立即清理。</li>
<li>窗口的重叠会带来重复计算，Spark Streaming 框架会进行优化，保证计算过的数据不会被重复计算。</li>
</ul>
<p><strong>注意</strong>：<br>对于初始的几个窗口，有可能数据是没有撑满的，随着时间的推进，窗口会被最终撑满。</p>
<h1 id="3-DStream"><a href="#3-DStream" class="headerlink" title="3. DStream"></a>3. DStream</h1><h2 id="3-1-DStream-定义"><a href="#3-1-DStream-定义" class="headerlink" title="3.1 DStream 定义"></a>3.1 DStream 定义</h2><p>Spark Streaming 抽象了离散数据流（DStream，Discretized Stream）这个概念，它包含了一组连续的 RDD，这一组连续的 RDD 代表了连续的流式数据。</p>
<p>DStream 是一组时间序列上连续的 RDD 来表示的，每个 RDD 都包含特定时间间隔内的数据流。我们对 DStream 上的各种操作最终都会映射到内部的 RDD 中。<br><img src="/images/2019/09/04/994c1090-cef1-11e9-ba98-ab6252b342d8.png" alt="图片1.png"></p>
<h2 id="3-2-DStream-输入源"><a href="#3-2-DStream-输入源" class="headerlink" title="3.2 DStream 输入源"></a>3.2 DStream 输入源</h2><p><img src="/images/2019/09/04/c4e912c0-cef1-11e9-ba98-ab6252b342d8.png" alt="图片3.png"></p>
<h2 id="3-3-DStream-操作"><a href="#3-3-DStream-操作" class="headerlink" title="3.3 DStream 操作"></a>3.3 DStream 操作</h2><h3 id="3-3-1-转换操作"><a href="#3-3-1-转换操作" class="headerlink" title="3.3.1 转换操作"></a>3.3.1 转换操作</h3><p>转换操作不会产生和提交作业，只构成 DStream 的操作链。<br><img src="/images/2019/09/04/f2d0a130-cef1-11e9-ba98-ab6252b342d8.png" alt="图片4.png"><br><img src="/images/2019/09/04/ff966f80-cef1-11e9-ba98-ab6252b342d8.png" alt="图片5.png"><br><img src="/images/2019/09/04/076f6860-cef2-11e9-ba98-ab6252b342d8.png" alt="图片6.png"></p>
<h3 id="3-3-2-输出操作"><a href="#3-3-2-输出操作" class="headerlink" title="3.3.2 输出操作"></a>3.3.2 输出操作</h3><p><img src="/images/2019/09/04/193d8360-cef2-11e9-ba98-ab6252b342d8.png" alt="图片7.png"></p>
<h1 id="4-例一"><a href="#4-例一" class="headerlink" title="4. 例一"></a>4. 例一</h1><h2 id="4-1-需求"><a href="#4-1-需求" class="headerlink" title="4.1 需求"></a>4.1 需求</h2><p>使用 Spark Streaming + Socket 流实时进行 wordcount</p>
<h2 id="4-2-代码"><a href="#4-2-代码" class="headerlink" title="4.2 代码"></a>4.2 代码</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.miracle.spark;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@program</span>: sparkapp</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>: SparkStreaming 实现 wordcount</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: Miracle</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2019-09-04 15:54</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Ss_wordcount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">            System.err.println(<span class="string">"usage: Spark Streaming App &lt;host&gt; &lt;port&gt;"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        String host = args[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> port = Integer.valueOf(args[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">        SparkConf sparkConf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"ss_wordcount"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Durations.seconds(5) 指定 batchDuration</span></span><br><span class="line">        <span class="comment">// JavaStreamingContext 内部包含 SparkContext</span></span><br><span class="line">        JavaStreamingContext jsc = <span class="keyword">new</span> JavaStreamingContext(sparkConf, Durations.seconds(<span class="number">5</span>));</span><br><span class="line">        jsc.sparkContext().setLogLevel(<span class="string">"ERROR"</span>);</span><br><span class="line">        JavaReceiverInputDStream&lt;String&gt; lines = jsc.socketTextStream(host, port);</span><br><span class="line">        lines.flatMap(line -&gt; Arrays.asList(line.split(<span class="string">" "</span>)).iterator())</span><br><span class="line">                .mapToPair(word -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(word, <span class="number">1</span>))</span><br><span class="line">                .reduceByKey((a, b) -&gt; a + b)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 以上流程创建完成</span></span><br><span class="line">        jsc.start();    <span class="comment">// 启动执行计划</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            jsc.awaitTermination(); <span class="comment">// 等待程序停止，执行期间发出的异常都将会抛出</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        jsc.stop();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="4-3-执行"><a href="#4-3-执行" class="headerlink" title="4.3 执行"></a>4.3 执行</h2><ul>
<li><p>Maven install</p>
</li>
<li><p>将 jar 包上传到服务器上</p>
</li>
<li><p>提交 spark 程序</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --master local[2,3] --class com.miracle.spark.Ss_wordcount spark-app-1.0-SNAPSHOT-jar-with-dependencies.jar localhost 9999</span><br></pre></td></tr></table></figure>
</li>
<li><p>同一台机器再打开一个终端，执行 &gt; nc -l 9999</p>
</li>
<li><p>输入字符，程序每 5s 执行一次并输出 wordcount 结果。</p>
</li>
</ul>
<h2 id="4-4-说明"><a href="#4-4-说明" class="headerlink" title="4.4 说明"></a>4.4 说明</h2><p>在 Spark Streaming 中，作业产生后并不会立即被提交，而是需要等到 StreamingContext 启动后才会被依次提交。    //jsc.start()<br>作业的提交间隔是由批处理间隔 Slide Duration（默认和 Batch Duration相同）决定的。</p>
<h2 id="4-5-Streaming-作业和-Spark-作业之间的关系"><a href="#4-5-Streaming-作业和-Spark-作业之间的关系" class="headerlink" title="4.5 Streaming 作业和 Spark 作业之间的关系"></a>4.5 Streaming 作业和 Spark 作业之间的关系</h2><p>Spark Streaming 作业最终会被翻译成 Spark 作业并提交和执行。DStream 在内部维护了相应的 RDD，对于 DStream 的操作，无论是转换操作还是输出操作，最终都会被映射到 RDD 上。</p>
<p>当我们在程序中构建 DStream 操作链，在 Spark Streaming 内部会隐式地构建 RDD 操作链。<br>见 DStream.generateJob 代码部分</p>
<h1 id="5-Spark-Streaming-架构"><a href="#5-Spark-Streaming-架构" class="headerlink" title="5. Spark Streaming 架构"></a>5. Spark Streaming 架构</h1><p><img src="/images/2019/09/07/9d534fb0-d15c-11e9-835c-d1aed722d3e1.png" alt="图片8.png"></p>
<h1 id="6-Spark-栈"><a href="#6-Spark-栈" class="headerlink" title="6. Spark 栈"></a>6. Spark 栈</h1><p>Spark 容纳了一个栈式的库：包括 SQL and DataFrames，用于机器学习的 MLib，GraphX，SparkStreaming，你可以在相同的应用中无缝结合这些库。<br><img src="/images/2019/09/07/29c634d0-d15d-11e9-835c-d1aed722d3e1.png" alt="图片9.png"></p>
<h1 id="7-例二"><a href="#7-例二" class="headerlink" title="7. 例二"></a>7. 例二</h1><h2 id="7-1-需求"><a href="#7-1-需求" class="headerlink" title="7.1 需求"></a>7.1 需求</h2><p>监听 HDFS 目录，对新增内容做 ETL 处理，checkpoint 检查点。</p>
<h2 id="7-2-代码"><a href="#7-2-代码" class="headerlink" title="7.2 代码"></a>7.2 代码</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.miracle.spark;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@program</span>: sparkapp</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>: 实时监听 HDFS 目录</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: Miracle</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2019-09-05 08:55</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Ss_HdfsApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">            System.err.println(<span class="string">"usage: Spark Streaming App &lt;host&gt; &lt;port&gt;"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        String input = args[<span class="number">0</span>];</span><br><span class="line">        String output = args[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 检查点，checkpoint，用于容错，当程序崩溃时，你可以重启程序，并让驱动程序 driver 从检查点恢复</span></span><br><span class="line">        <span class="comment">// 这样 Spark Streaming 就可以读取之前运行的程序处理数据的进度，并从失败的地方开始继续处理</span></span><br><span class="line">        JavaStreamingContext context = JavaStreamingContext.getOrCreate(<span class="string">"/user/work/checkpoint"</span>, () -&gt; &#123;</span><br><span class="line">            SparkConf sparkConf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"Ss_HdfsApp"</span>);</span><br><span class="line">            JavaStreamingContext jsc = <span class="keyword">new</span> JavaStreamingContext(sparkConf, Durations.seconds(<span class="number">10</span>));</span><br><span class="line">            jsc.checkpoint(<span class="string">"/user/work/checkpoint"</span>);</span><br><span class="line">            JavaDStream&lt;String&gt; lineDStream = jsc.textFileStream(input);</span><br><span class="line">            JavaDStream&lt;String&gt; map = lineDStream.map(line -&gt; line + <span class="string">"miracle"</span>);</span><br><span class="line">            JavaDStream&lt;String&gt; transform = map.flatMap(line -&gt; Arrays.asList(line.split(<span class="string">"="</span>)).iterator());</span><br><span class="line">            transform.repartition(<span class="number">1</span>).dstream().saveAsTextFiles(output, <span class="string">""</span>);</span><br><span class="line">            <span class="keyword">return</span> jsc;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 以上流程创建完成</span></span><br><span class="line">        context.start();    <span class="comment">// 启动执行计划</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            context.awaitTermination(); <span class="comment">// 等待程序停止，执行期间发出的异常都将会抛出</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        context.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="7-3-执行"><a href="#7-3-执行" class="headerlink" title="7.3 执行"></a>7.3 执行</h2><ul>
<li><p>在 <strong>spark-env.sh</strong> 增加配置（这样代码中 hdfs 前缀可以省略）</p>
<blockquote>
<p>HADOOP_CONF_DIR=/opt/modules/hadoop277/etc/hadoop</p>
</blockquote>
</li>
<li><p>执行 <strong>generateData.sh</strong> 不断生成 content* 文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line">num=1</span><br><span class="line">hdfs dfs -rm -r /user/work/content</span><br><span class="line">hdfs dfs -rm -r /user/work/content_copy</span><br><span class="line">hdfs dfs -mkdir /user/work/content</span><br><span class="line">hdfs dfs -mkdir /user/work/content_copy</span><br><span class="line"></span><br><span class="line">rm -f content*</span><br><span class="line"><span class="keyword">while</span> [ <span class="variable">$num</span> -le 10000 ]</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  sleep 2s</span><br><span class="line">  (( num ++ ))</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"content="</span><span class="variable">$num</span> &gt; content<span class="variable">$num</span></span><br><span class="line">  hdfs dfs -put content<span class="variable">$num</span> /user/work/content_copy</span><br><span class="line">  hdfs dfs -mv /user/work/content_copy/content<span class="variable">$num</span> /user/work/content</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>提交任务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --master yarn --deploy-mode cluster --driver-memory 1g --executor-memory 1g --executor-cores 1 --num-executors 3 --class com.miracle.spark.Ss_HdfsApp spark-app-1.0-SNAPSHOT-jar-with-dependencies.jar /user/work/content /user/work/output</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;<img src="/images/2019/09/08/6541ad30-d1f0-11e9-835c-d1aed722d3e1.png" alt="image.png"></p>
<ul>
<li><p>验证 checkpoint 可用性</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn application -kill application_1567915234723_0001</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看最后一个输出结果<br><img src="/images/2019/09/08/be308d80-d1f0-11e9-835c-d1aed722d3e1.png" alt="image.png"><br><img src="/images/2019/09/08/d09f0730-d1f0-11e9-835c-d1aed722d3e1.png" alt="image.png"></p>
</li>
<li><p>重新提交任务</p>
</li>
<li><p>查看输出文件夹<br><img src="/images/2019/09/08/f1217a10-d1f0-11e9-835c-d1aed722d3e1.png" alt="image.png"></p>
</li>
<li><p>查看结果是否中断<br><img src="/images/2019/09/08/1a3b7ae0-d1f1-11e9-835c-d1aed722d3e1.png" alt="image.png"></p>
</li>
</ul>
<h1 id="8-例三"><a href="#8-例三" class="headerlink" title="8. 例三"></a>8. 例三</h1><h2 id="8-1-需求"><a href="#8-1-需求" class="headerlink" title="8.1 需求"></a>8.1 需求</h2><p>滑动窗口实例：热点搜索词滑动统计，每隔 10s，统计最近 60s 的搜索词的搜索频次，并打印出排名最靠前的 3 个搜索词和它出现次数。（Slide Duration，Window Duration）</p>
<h2 id="8-2-代码"><a href="#8-2-代码" class="headerlink" title="8.2 代码"></a>8.2 代码</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.miracle.spark;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@program</span>: sparkapp</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>: 热点搜索词滑动统计</span></span><br><span class="line"><span class="comment"> * 每隔 10s，统计最近 60s 的搜索词的搜索频次，并打印出排名最靠前的 3 个搜索词和它出现次数。（Slide Duration，Window Duration）</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: Miracle</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2019-09-08 12:39</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Ss_WindowApp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">            System.err.println(<span class="string">"usage: Spark Streaming App &lt;host&gt; &lt;port&gt;"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        String host = args[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> port = Integer.valueOf(args[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">        SparkConf sparkConf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"ss_wordcount"</span>);</span><br><span class="line">        JavaStreamingContext jsc = <span class="keyword">new</span> JavaStreamingContext(sparkConf, Durations.seconds(<span class="number">5</span>));</span><br><span class="line">        JavaReceiverInputDStream&lt;String&gt; searchDStream = jsc.socketTextStream(host, port);</span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; searchWordPairDStream = searchDStream.flatMap(line -&gt; Arrays.asList(line.split(<span class="string">" "</span>)).iterator()).mapToPair(word -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(word, <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Window Duration 窗口长度是 60s</span></span><br><span class="line">        <span class="comment">// Slide Duration 窗口长度是 10s</span></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; searchWordCountPairDStream = searchWordPairDStream.reduceByKeyAndWindow((x, y) -&gt; (x + y), Durations.seconds(<span class="number">60</span>), Durations.seconds(<span class="number">10</span>));</span><br><span class="line"></span><br><span class="line">        searchWordCountPairDStream.foreachRDD(p -&gt;</span><br><span class="line">                p.mapToPair(x -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(x._2, x._1))</span><br><span class="line">                        .sortByKey(<span class="keyword">false</span>)</span><br><span class="line">                        .mapToPair(x -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(x._2, x._1))</span><br><span class="line">                        .take(<span class="number">3</span>)</span><br><span class="line">                        .forEach(x -&gt; System.out.println(<span class="string">"---------------top 3 word is: "</span> + x))</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 以上流程创建完成</span></span><br><span class="line">        jsc.start();    <span class="comment">// 启动执行计划</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            jsc.awaitTermination(); <span class="comment">// 等待程序停止，执行期间发出的异常都将会抛出</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        jsc.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="8-3-执行"><a href="#8-3-执行" class="headerlink" title="8.3 执行"></a>8.3 执行</h2><ul>
<li><p>开启 socket 流端口并键入字符</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nc -l 9999</span><br><span class="line">不停输入字符...</span><br></pre></td></tr></table></figure>
</li>
<li><p>提交 spark 程序</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --master local[2,3] --class com.miracle.spark.Ss_WindowApp /opt/sparkAPP/spark-app-1.0-SNAPSHOT-jar-with-dependencies.jar localhost 9999</span><br></pre></td></tr></table></figure>
</li>
<li><p>输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">job1:</span><br><span class="line">---------------top 3 word is: (hello,2)</span><br><span class="line">---------------top 3 word is: (s,2)</span><br><span class="line">---------------top 3 word is: (d,1)</span><br><span class="line"></span><br><span class="line">···</span><br><span class="line"></span><br><span class="line">job2: </span><br><span class="line">---------------top 3 word is: (,11)</span><br><span class="line">---------------top 3 word is: (d,5)</span><br><span class="line">---------------top 3 word is: (s,5)</span><br><span class="line"></span><br><span class="line">···</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="8-4-说明"><a href="#8-4-说明" class="headerlink" title="8.4 说明"></a>8.4 说明</h2><p><strong>Slide Duration</strong> 为 10s，表示每 10s 执行一次 job；<br><strong>Window Duration</strong> 为 60s，表示每次的 job 统计的是最近 60s 的数据。</p>

    
  </div>

</article>


   
  <div class="text-center donation">
    <div class="inner-donation">
      <span class="btn-donation">支持一下</span>
      <div class="donation-body">
        <div class="tip text-center">扫一扫，支持forsigner</div>
        <ul>
        
          <li class="item">
            
              <span>微信扫一扫</span>
            
            <img src="/images/qr-wechat.png" alt="">
          </li>
        
          <li class="item">
            
              <span>支付宝扫一扫</span>
            
            <img src="/images/qr-alipay.png" alt="">
          </li>
        
        </ul>
      </div>
    </div>
  </div>


   
  <div class="box-prev-next clearfix">
    <a class="show pull-left" href="/2019/09/03/SparkSQL-窗口函数/">
        <i class="icon icon-angle-left"></i>
    </a>
    <a class="show pull-right" href="/2019/09/19/Linux-操作系统（一）/">
        <i class="icon icon-angle-right"></i>
    </a>
  </div>




</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">关闭</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              rel="noopener noreferrer"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/category/"
              rel="noopener noreferrer"
              target="_self"
              >
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              rel="noopener noreferrer"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              rel="noopener noreferrer"
              target="_self"
              >
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/atom.xml"
              rel="noopener noreferrer"
              target="_blank"
              >
              RSS
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    

    

    
    

    

    
    

    

<!-- Gitalk评论插件通用代码 -->
<div id="gitalk-container"></div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="https://cdn.bootcss.com/blueimp-md5/2.11.1/js/md5.js"></script>
<script>
const gitalk = new Gitalk({
  clientID: 'cffabda338955fb33e72',
  clientSecret: '27685d32607acc9c76041016860f5434fa1d65d0',
  repo: 'gitalk_comment',
  owner: 'Miracle-Xing',
  // 在这里设置一下截取前50个字符串, 这是因为 github 对 label 的长度有了要求, 如果超过
  // 50个字符串则会报错.
  //id: location.pathname.split('/').pop().substring(0, 49),
   id: md5(location.pathname),
  // id: title,
  admin: ['Miracle-Xing'],
  // facebook-like distraction free mode
  distractionFreeMode: false
})
gitalk.render('gitalk-container')
</script>
<!-- Gitalk代码结束 -->



  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

</body>
</html>
